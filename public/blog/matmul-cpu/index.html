<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>[WIP] Optimizing matmul on CPU | Srishti Gureja</title><meta name=keywords content="Optimization,Parallel Processing"><meta name=description content="Introduction and Setup
This blog starts with a naive implementation of matmul in C and optimizes it one step at a time.
I am using the following machine with a 10 core CPU (4 Performance cores+6 Efficiency cores).
Food for thought: Is matrix multiplication on CPU compute bound or memory bound? Think about it&mldr;
Algorithmic complexity of Matrix Multiplication: Calculating the FLOPs required
Matrix multiplication is ubiquitous in many areas of computer sciene. As the matrices grow in sizes, the amount of FLOPs required to calculate the matmul grow cubically1. Let&rsquo;s see how."><meta name=author content><link rel=canonical href=http://localhost:1313/blog/matmul-cpu/><link crossorigin=anonymous href=/assets/css/stylesheet.b26ba54a60d00ea06fda6b711f3da6382e5fe3a6fae7b4cc58bdc38f1f26bddc.css integrity="sha256-smulSmDQDqBv2mtxHz2mOC5f46b657TMWL3Djx8mvdw=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/blog/matmul-cpu/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="[WIP] Optimizing matmul on CPU"><meta property="og:description" content="Introduction and Setup
This blog starts with a naive implementation of matmul in C and optimizes it one step at a time.
I am using the following machine with a 10 core CPU (4 Performance cores+6 Efficiency cores).
Food for thought: Is matrix multiplication on CPU compute bound or memory bound? Think about it&mldr;
Algorithmic complexity of Matrix Multiplication: Calculating the FLOPs required
Matrix multiplication is ubiquitous in many areas of computer sciene. As the matrices grow in sizes, the amount of FLOPs required to calculate the matmul grow cubically1. Let&rsquo;s see how."><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/blog/matmul-cpu/"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-12-20T00:00:00+00:00"><meta property="article:modified_time" content="2025-12-20T00:00:00+00:00"><meta property="og:site_name" content="Srishti Gureja"><meta name=twitter:card content="summary"><meta name=twitter:title content="[WIP] Optimizing matmul on CPU"><meta name=twitter:description content="Introduction and Setup
This blog starts with a naive implementation of matmul in C and optimizes it one step at a time.
I am using the following machine with a 10 core CPU (4 Performance cores+6 Efficiency cores).
Food for thought: Is matrix multiplication on CPU compute bound or memory bound? Think about it&mldr;
Algorithmic complexity of Matrix Multiplication: Calculating the FLOPs required
Matrix multiplication is ubiquitous in many areas of computer sciene. As the matrices grow in sizes, the amount of FLOPs required to calculate the matmul grow cubically1. Let&rsquo;s see how."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blogs","item":"http://localhost:1313/blog/"},{"@type":"ListItem","position":2,"name":"[WIP] Optimizing matmul on CPU","item":"http://localhost:1313/blog/matmul-cpu/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"[WIP] Optimizing matmul on CPU","name":"[WIP] Optimizing matmul on CPU","description":"Introduction and Setup This blog starts with a naive implementation of matmul in C and optimizes it one step at a time.\nI am using the following machine with a 10 core CPU (4 Performance cores+6 Efficiency cores).\nFood for thought: Is matrix multiplication on CPU compute bound or memory bound? Think about it\u0026hellip;\nAlgorithmic complexity of Matrix Multiplication: Calculating the FLOPs required Matrix multiplication is ubiquitous in many areas of computer sciene. As the matrices grow in sizes, the amount of FLOPs required to calculate the matmul grow cubically1. Let\u0026rsquo;s see how.\n","keywords":["Optimization, Parallel Processing"],"articleBody":"Introduction and Setup This blog starts with a naive implementation of matmul in C and optimizes it one step at a time.\nI am using the following machine with a 10 core CPU (4 Performance cores+6 Efficiency cores).\nFood for thought: Is matrix multiplication on CPU compute bound or memory bound? Think about itâ€¦\nAlgorithmic complexity of Matrix Multiplication: Calculating the FLOPs required Matrix multiplication is ubiquitous in many areas of computer sciene. As the matrices grow in sizes, the amount of FLOPs required to calculate the matmul grow cubically1. Letâ€™s see how.\nConsider two matrices $A (i \\times k)$ and $B (k \\times j)$. The product of $A$ and $B$, $AB$ is a matrix $C$ of shape $(i \\times j)$.\nFor simplicity and without loss of generality, letâ€™s consider the matrices to be square so we have $A (n \\times n)$, $B (n \\times n)$ and their product $C (n \\times n)$. One element $(c_1, c_2)$ of $C$ is defined as:\n$$c_{c1,c2} = \\sum_{x=1}^{n} a_{c1,x} b_{x,c2}$$\nThis is a total of 2n - 1 floating point operations (FLOPs) required to calculate one element of C â€“ $n$ multiplication ops + $(n-1)$ addition ops. A total of $n^2$ elements need to be calculated in $C$ and hence the total FLOPs:\n$$(2n - 1) n^2 = 2n^3 - n^2$$\nAs $n$ grows bigger (asymptomatic, if youâ€™re feeling fancy), $n^2$ becomes pretty negligible in comparison to $2n^3$ and hence can be ignored so the total FLOPs required is roughly $2n^3$. Therefore, the computational complexity is $O(n^3)$.\nFor the purpose of this blog, I am considering two matrix sizes $N=4096$ and $N=8192$ which translates to roughly 137 and 1099 GFLOPs respectively. I initially started with $N=4096$ but because some optimization didnâ€™t yield any significant benefit for that size, I also decided to include $N=8192$. Anyway, those numbers seem like a lot of FLOPs, but theyâ€™re rather pretty small if compared with the peak FLOPs offered by any standard modern processor.\nTo benchmark, I calculated the time required to multiply numpy arrays using np.matmul and here are the results:\nTechnique 4096 8192 Baseline (using np) 0.10s 0.78s Naive implementation The matmul loop is this:\nfor (int i = 0; i \u003c N; i++) { for (int j = 0; j \u003c N; j++) { for (int k = 0; k \u003c N; k++) { C[i][j] += A[i][k] * B[k][j]; } } } When complied with the -O3 flag2 which is the maximum level with safe optimizations, the latencies are as follows:\nTechnique 4096 8192 Speedup Baseline (np) 0.10 s 0.78 s â€“ Naive implementation 203 s 46 min - The speedup column for all further tables is calculated with respect to the row (optimization technique) just above so it simply gives an idea of the amount of improvement we get with a new intervention as compared to what we had just before it.\nFull compilation command below:\ngcc -Wall -O3 sgemm-cpu/matmuls/naive.c -o sgemm-cpu/matmuls/naive All the further optimizations use the same flags to compile the code.\nA minor optimization: Avoiding unnecessary memory accesses A very small thing we could do with the naive implementation is avoiding multiple reads and writes of intermediate partial sums from and to the memory/cache, like so:\nfor (int i = 0; i \u003c N; i++) { for (int j = 0; j \u003c N; j++) { float running_sum = 0.0; for (int k = 0; k \u003c N; k++) { running_sum += A[i][k] * B[k][j]; } C[i][j] = running_sum; } } The table below shows that helps very little for $N=4096$ but quite a lot for $N=8192$ 3:\nTechnique 4096 8192 Speedup ($N=4096$) Speedup ($N=4096$) Baseline (np) 0.10s 0.74s â€“ â€“ Naive implementation 203s 46min - - Naive w register accumulation 199s 27min 1.02x 1.7x This optimization is simply causing the compiler to keep the partial sum in the registers only and write back to the memory only once when the full sum is done. Meaning in this case the compiler isnâ€™t issuing separate instructions to store the partial sum back to the memory after every loop iteration and to load it back on the next iteration; and that reduces some latency 4.\nLoop reordering The naive implementation follows the most natural mathy way to calculate a matmul $C = AB$ â€” element $C[0][0]$ is fully calculated first via a scalar product of the first row of $A$ with the first column of $B$, element $C[0][1]$ is fully calculated next via the scalar product of the first row of $A$ with the second column of $B$, and so on.\nThere are two key insights over here:\nLanguages like C store matrices in the memory in a row major format like this: If we now follow the calculation of $C[0][0]$ in code, itâ€™s equivalent to completing the inner-most $k$ loop for $i=0, j=0$.\nThe first part of the figure below uses 3 shades of red to color the values from $A, B, C$ that are accessed in the code during the calculation of $C[0][0]$. Focus on the location of these values in the memory by following the color.\nThe figure quickly makes it clear that the accessed values for $A, C$ are close to each other in the memory while theyâ€™re further apart for $B$. Now, remember that data is fetched from the memory in the caches in the granularity of cache lines. One cache line on my machine is of size 128 bytes which is equivalent to 32 single precision values.\nSo on the very first loop iteration $i=0, j=0, k=0$, when $A[0][0], B[0][0], C[0][0]$ are fetched from the memory, the cache looks something like:\nThis is simply because a cache line loads contiguous values from the memory where the matrices are stored in a row major format. On the second iteration $i=0, j=0, k=1$, we need $A[0][1], B[1][0], C[0][0]$ and while $A[0][1]$ and $C[0][0]$ are found in the cache, we have a miss for $B[1][0]$ that we need to fetch from the memory. And this holds for each iteration of the k-loop for $i=0, j=0$. Easy to figure out why â€“ our cache line is only 128 bytes (32 floats) while each subsequent loop iteration accesses a value from B thatâ€™s 4096 values apart from the value accessed in the last iteration. And this high cache miss rate explains the high latency we saw w the naive implementation.\nWhat about the subsequent iterations for different values of j and i? Say $i=0, j=1, k=0$ when we need $B[0][1]$ that was a part of the cache line we loaded on $i=0, j=0, k=0$ â€” that cache line most probably would get evicted from the cache till now. Hence, the cache miss rate still remains high.\nðŸ‘‰ ðŸ‘‰ ðŸ‘‰ This is actually a central recurring point of discussion: Some data that we load once gets evicted by the time we need it again making things go slower. In a few upcoming sections, weâ€™ll be revolving around this point making optimizations that aim to reuse the data in subsequent iterations by using it as much as needed (and possible) before its eviction.\nThe second insight is not too difficult to understand â€“ if we just change the loop orders (eg. $jik$ or $jki$ etc. instead of the most natural $ijk$), weâ€™ll still get the correct matmul. Itâ€™s also why I was italicizing fully above. Thing is that with different loop orders weâ€™re not fully calculating each element of C in one full iteration of the innermost loop but that doesnâ€™t hurt the correctness of the matmul and thatâ€™s easy to realise. Alright. Given that, we note that some loop orders have a better overall cache hit rate as compared to the naive $ijk$ order (btw some orders also have a worse rate than $ijk$!). And hence just by changing the orders, weâ€™ll be able to reduce the latency by not having to make as many high latency accesses to the memory. Experimenting w different orders, the lowest latency corresponds to order ikj as follows: Technique 4096 8192 Speedup ($N=4096$) Speedup ($N=4096$) Baseline (np) 0.10s 0.74s â€“ â€“ Naive implementation 203s 46min - - Naive w register accumulation 199s 27min 1.02x 1.7x Loop reordering (ikj) 4.31s 34.28s 46x 47x for (int i = 0; i \u003c N; i++) { for (int k = 0; k \u003c N; k++) { float a_ik = A[i][k]; for (int j = 0; j \u003c N; j++) { C[i][j] += a_ik * B[k][j]; } } } What was the bottleneck? A simple loop reordering of our naive implementation provided a 45 fold improvement. Thatâ€™s a lot for such a simple change. What does this tell us? Obviously, we still performed total 137 GFLOPs so that part didnâ€™t change. Turns out, in the naive implementation our bottleneck was the memory bandwidth. Meaning the CPU execution units (ALUs) were bottlenecked by the latency of data transfer between the memory and the CPU, implying we were overall memory bound in the naive implementation. Of course, we cannot change the memory bandwidth towards a faster memory. So what did we do? We simply wrote the code such that it allows for better cache reuse in subsequent loop iters!\nWhat about the computation aspect? // vectorization enabled by loop ordering - figured out by the compiler\nWhat about the minor optimization from above? Because with the reordered loops, weâ€™re not calculating the full result $C[i][j]$ in one iteration of the inner most loop, we canâ€™t avoid the loading and storing of partial sums anymore!\nTiling The long story short about tiling: itâ€™s all about enabling a better cache reuse. Nothing different from the goal of loop reordering.\nLetâ€™s try to understand tiling by doing something boring. For our best loop order $ikj$, we start by looking at the values that are accessed by the inner $k, j$ loops for different values of $i$.\n$(i=0, k=0-4095, j=0-4095)$\n$A[0][0], A[0][1], â€¦ A[0][4095]$ â€“\u003e Aâ€™s 1st row $C[0][0], C[0][1], â€¦ C[0][4095]$ â€“\u003e Câ€™s 1st row $B[0][0], B[0][1], â€¦ B[0][4095]$ â€“\u003e Bâ€™s 1st row $B[1][0], B[1][1], â€¦ B[1][4095]$ â€“\u003e Bâ€™s 2nd row â€¦ $B[4095][0], B[4095][1], â€¦ B[4095][4095]$ â€“\u003e Bâ€™s 4095th row $(i=1, k=0-4095, j=0-4095)$\n$A[1][0], A[1][1], â€¦ A[1][4095]$ â€“\u003e Aâ€™s 2nd row $C[1][0], C[1][1], â€¦ C[1][4095]$ â€“\u003e Câ€™s 2nd row $B[0][0], B[0][1], â€¦ B[0][4095]$ â€“\u003e Bâ€™s 1st row $B[1][0], B[1][1], â€¦ B[1][4095]$ â€“\u003e Bâ€™s 2nd row â€¦\n$B[4095][0], B[4095][1], â€¦ B[4095][4095]$ â€“\u003e Bâ€™s 4095th row What do we infer?\nðŸ‘‰ At every iteration of the $i$ loop, the code accesses a different row of A and C but all the rows of B are accessed at each iteration.\nSo far so good. Letâ€™s now take an example with smaller 8x8 matrices and see what caches actually look like with our best loop order. For the example, assume a fully associative cache following an LRU (least recently used) policy with a cache line of 32 bytes = 8 floats, and a cache size of 160 bytes = 5 cache lines.\nThe (terrible) figure below shows the values accessed in sequence for $i=0$, and the state of the cache at each step. Red denotes a cache miss requiring a cache update and blue denotes a cache hit with no cache update required. Note that by the time we end $k=2$ for, our cache is full.\nThe figure below shows the program resumed from $k=3$. Also shown is the cache state at the end of $i=0$ after multiple evictions. Note how the first 5 rows of B had to be evicted even before reaching $i=1$.\nNow what? Start $i=1$. The same story gets repeated and below are the various cache states showing how we need to load the entire B matrix from the memory to be able to finish $i=1$! Not even a single row found in cache when needed despite the fact that all of them were loaded in the previous iteration $i=0$.\nOf course, this was true for our particular example that 1 cache line = 1 row, and no single row of B was found in the cache. In practice, the number of caches, their sizes, eviction policies, cache line sizes all matter in determining what rows/values are found but the general gist stated below remains the same.\nThe gist: At each new value of i, weâ€™re having to load all or some rows (/partial rows depending on the cache line) of B from the high latency memory despite having them loaded in the previous iteration of i. We should definitely do better!\nk-tiling Think about this: how about there was no eviction and hence no need to reload values of B at each new iteration of i? Itâ€™d have been great except that caches are limited and we cannot increase their size. How do we make peace with eviction? By using the data as much as we want before itâ€™s evicted. Meaning we load a few rows of B in the cache and use them for all the values of i so that we never require them again? Then load the next few and again use them for all the values of i. The same until weâ€™re done with all the rows of B. This is where the word tile comes from â€“ weâ€™re loading and utilizing one tile fully before evicting it.\nWhat Iâ€™ve described is tiling on the k-loop only. In code:\nfor (int k_tile=0; k_tile\u003cN; k_tile+=TILE_SIZE) { for (int i=0; i\u003cN; i++) { int kend = (k_tile + TILE_SIZE \u003e N) ? N : k_tile + TILE_SIZE; // check if this is expensive than an if for (int k=k_tile; k\u003ckend; k++) { float a_ik = A[i][k]; for (int j=0; j\u003cN; j++) { C[i][j] += a_ik * B[k][j]; } } } } After doing a good amount of search over different values of TILE_SIZE, I found k-tiling didnâ€™t really yield any benefit on my machine both for $N=4096$ and $N=8192$. This is probably because of the already large cache sizes of my machime.\nijk tiling Letâ€™s now separately understand the purpose of tiling rest of the two loops - i and j.\nTiling on the j-loop : With k-tiling rather than loading all the rows of $B$ in the cache for each iteration of $i$, weâ€™re covering a particular number of rows at a time that fit in the cache. But for these rows, weâ€™re still loading all the columns of $B$ (all values of $j$)! In the oversimplified example above, 1 row = 1 cache line but that obviously isnâ€™t the case with bigger matrices and hence, itâ€™d further benefit for the cache hit rate to also tile on the $j$ loop.\nTiling on the i-loop : Further with both $k$ and $j$ tiled, weâ€™d still be needing all the rows of C (all values of i, but ofc not full rows due to j-tiling) in the cache multiple times for different values of k_tile and j_tile. And that might again lead to cache misses for certain values of $C$. And hence, it also benefits to tile on the $i$ loop which, combined with $j$ tiling, effectively translates to taking a sub-matrix of C and finishing all the calculations for it (covering all values of k) before proceeding to another sub-matrix. Of course, we cover all the k values in a tiled manner only.\nIn code, this looks like:\nfor (int i_tile = 0; i_tile \u003c N; i_tile += TILE_I) { int iend = (i_tile + TILE_I \u003c N) ? i_tile + TILE_I : N; for (int j_tile = 0; j_tile \u003c N; j_tile += TILE_J) { int jend = (j_tile + TILE_J \u003c N) ? j_tile + TILE_J : N; /* for a certain tile of C (i_tile:iend, j_tile, jend) we now cover all values of k in our already discussed k-tiled manner */ for (int k_tile = 0; k_tile \u003c N; k_tile += TILE_K) { int kend = (k_tile + TILE_K \u003c N) ? k_tile + TILE_K : N; for (int i = i_tile; i \u003c iend; i++) { for (int k = k_tile; k \u003c kend; k++) { float a_ik = A[i][k]; for (int j = j_tile; j \u003c jend; j++) { C[i][j] += a_ik * B[k][j]; } } } } } } With ijk tiling, the results are as follows:\nTechnique 4096 8192 Speedup ($N=4096$) Speedup ($N=4096$) Baseline (np) 0.10s 0.74s â€“ â€“ Naive implementation 203s 46min - - Naive w register accumulation 199s 27min 1.02x 1.7x Loop reordering (ikj) 4.31s 34.28s 46x 47x ijk tiling (best tile sizes) 3.16s 26.20s 1.36 1.3x This is for the standard algorithm. Thereâ€™s other algos like Strassenâ€™s with better theoretical complexity.Â â†©ï¸Ž\nExplore all optimization levels here.Â â†©ï¸Ž\nBut this optimization wasnâ€™t the reason why I decided to also run numbers for $N=8192$; the reason was ijk-tiling discussed later in the blog.Â â†©ï¸Ž\nSometimes, the compiler might also consider it safe to skip the extra store/reload steps when we directly update $C[i][j]$ in every loop iteration. But other times it may not. So, weâ€™re just being explicit here and writing code such that the compiler would always consider it safe to not issue the extra store/reload instructions for the partial sums.Â â†©ï¸Ž\n","wordCount":"2846","inLanguage":"en","datePublished":"2025-12-20T00:00:00Z","dateModified":"2025-12-20T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/blog/matmul-cpu/"},"publisher":{"@type":"Organization","name":"Srishti Gureja","logo":{"@type":"ImageObject","url":"http://localhost:1313/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Srishti Gureja (Alt + H)">Srishti Gureja</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/code/ title=code><span>code</span></a></li><li><a href=http://localhost:1313/blog/ title=blog><span>blog</span></a></li><li><a href=http://localhost:1313/talks/ title=talks><span>talks</span></a></li><li><a href=http://localhost:1313/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;Â»&nbsp;<a href=http://localhost:1313/blog/>Blogs</a></div><h1 class="post-title entry-hint-parent">[WIP] Optimizing matmul on CPU</h1><div class=post-meta><span title='2025-12-20 00:00:00 +0000 UTC'>December 20, 2025</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#introduction-and-setup>Introduction and Setup</a></li><li><a href=#algorithmic-complexity-of-matrix-multiplication-calculating-the-flops-required>Algorithmic complexity of Matrix Multiplication: Calculating the FLOPs required</a></li><li><a href=#naive-implementation><a href=https://github.com/srishti-git1110/SGEMM-cpu/blob/main/sgemm-cpu/matmuls/naive.c>Naive implementation</a></a><ul><li><a href=#a-minor-optimization-avoiding-unnecessary-memory-accesses><a href=https://github.com/srishti-git1110/SGEMM-cpu/blob/main/sgemm-cpu/matmuls/naive_register_accumulation.c>A minor optimization: Avoiding unnecessary memory accesses</a></a></li></ul></li><li><a href=#loop-reordering><a href=https://github.com/srishti-git1110/SGEMM-cpu/blob/main/sgemm-cpu/matmuls/cache_aware.c>Loop reordering</a></a><ul><li><a href=#what-was-the-bottleneck>What was the bottleneck?</a></li><li><a href=#what-about-the-computation-aspect>What about the computation aspect?</a></li><li><a href=#what-about-the-minor-optimization-from-above>What about the minor optimization from <a href=https://srishti-git1110.github.io/blog/matmul-cpu/#a-minor-optimization-avoiding-unnecessary-memory-accesses>above</a>?</a></li></ul></li><li><a href=#tiling>Tiling</a><ul><li><a href=#k-tiling>k-tiling</a></li><li><a href=#ijk-tiling>ijk tiling</a></li></ul></li></ul></nav></div></details></div><div class=post-content><h2 id=introduction-and-setup>Introduction and Setup<a hidden class=anchor aria-hidden=true href=#introduction-and-setup>#</a></h2><p>This blog starts with a naive implementation of matmul in C and optimizes it one step at a time.</p><p>I am using <a href=https://www.apple.com/in/shop/buy-mac/macbook-pro/14-inch-space-black-standard-display-apple-m5-chip-with-10-core-cpu-and-10-core-gpu-16gb-memory-512gb>the following machine</a> with a 10 core CPU (4 Performance cores+6 Efficiency cores).</p><p>Food for thought: Is matrix multiplication on CPU compute bound or memory bound? Think about it&mldr;</p><h2 id=algorithmic-complexity-of-matrix-multiplication-calculating-the-flops-required>Algorithmic complexity of Matrix Multiplication: Calculating the FLOPs required<a hidden class=anchor aria-hidden=true href=#algorithmic-complexity-of-matrix-multiplication-calculating-the-flops-required>#</a></h2><p>Matrix multiplication is ubiquitous in many areas of computer sciene. As the matrices grow in sizes, the amount of FLOPs required to calculate the matmul grow cubically<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>. Let&rsquo;s see how.</p><p>Consider two matrices $A (i \times k)$ and $B (k \times j)$. The product of $A$ and $B$, $AB$ is a matrix $C$ of shape $(i \times j)$.</p><p>For simplicity and without loss of generality, let&rsquo;s consider the matrices to be square so we have $A (n \times n)$, $B (n \times n)$ and their product $C (n \times n)$. One element $(c_1, c_2)$ of $C$ is defined as:</p><p>$$c_{c1,c2} = \sum_{x=1}^{n} a_{c1,x} b_{x,c2}$$</p><p>This is a total of 2n - 1 floating point operations (FLOPs) required to calculate one element of C &ndash; $n$ multiplication ops + $(n-1)$ addition ops. A total of $n^2$ elements need to be calculated in $C$ and hence the total FLOPs:</p><p>$$(2n - 1) n^2 = 2n^3 - n^2$$</p><p>As $n$ grows bigger (asymptomatic, if you&rsquo;re feeling fancy), $n^2$ becomes pretty negligible in comparison to $2n^3$ and hence can be ignored so the total FLOPs required is roughly $2n^3$. Therefore, the computational complexity is $O(n^3)$.</p><p>For the purpose of this blog, I am considering two matrix sizes $N=4096$ and $N=8192$ which translates to roughly 137 and 1099 GFLOPs respectively. I initially started with $N=4096$ but because some optimization didn&rsquo;t yield any significant benefit for that size, I also decided to include $N=8192$. Anyway, those numbers seem like a lot of FLOPs, but they&rsquo;re rather pretty small if compared with the peak FLOPs offered by any standard modern processor.</p><p>To benchmark, I calculated the time required to multiply numpy arrays using <code>np.matmul</code> and here are the results:</p><table><thead><tr><th>Technique</th><th>4096</th><th>8192</th></tr></thead><tbody><tr><td>Baseline (using np)</td><td>0.10s</td><td>0.78s</td></tr></tbody></table><h2 id=naive-implementation><a href=https://github.com/srishti-git1110/SGEMM-cpu/blob/main/sgemm-cpu/matmuls/naive.c>Naive implementation</a><a hidden class=anchor aria-hidden=true href=#naive-implementation>#</a></h2><p>The matmul loop is this:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-C data-lang=C><span class=line><span class=cl><span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>N</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>j</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>j</span> <span class=o>&lt;</span> <span class=n>N</span><span class=p>;</span> <span class=n>j</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>k</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>k</span> <span class=o>&lt;</span> <span class=n>N</span><span class=p>;</span> <span class=n>k</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=n>C</span><span class=p>[</span><span class=n>i</span><span class=p>][</span><span class=n>j</span><span class=p>]</span> <span class=o>+=</span> <span class=n>A</span><span class=p>[</span><span class=n>i</span><span class=p>][</span><span class=n>k</span><span class=p>]</span> <span class=o>*</span> <span class=n>B</span><span class=p>[</span><span class=n>k</span><span class=p>][</span><span class=n>j</span><span class=p>];</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>When complied with the <code>-O3</code> flag<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup> which is the maximum level with safe optimizations, the latencies are as follows:</p><table><thead><tr><th>Technique</th><th>4096</th><th>8192</th><th>Speedup</th></tr></thead><tbody><tr><td><code>Baseline (np)</code></td><td><code>0.10 s</code></td><td><code>0.78 s</code></td><td><code>â€“</code></td></tr><tr><td><code>Naive implementation</code></td><td><code>203 s</code></td><td><code>46 min</code></td><td><code>-</code></td></tr></tbody></table><p>The speedup column for all further tables is calculated with respect to the row (optimization technique) just above so it simply gives an idea of the amount of improvement we get with a new intervention as compared to what we had just before it.</p><p>Full compilation command below:</p><pre tabindex=0><code>gcc -Wall -O3 sgemm-cpu/matmuls/naive.c -o sgemm-cpu/matmuls/naive
</code></pre><p>All the further optimizations use the same flags to compile the code.</p><h3 id=a-minor-optimization-avoiding-unnecessary-memory-accesses><a href=https://github.com/srishti-git1110/SGEMM-cpu/blob/main/sgemm-cpu/matmuls/naive_register_accumulation.c>A minor optimization: Avoiding unnecessary memory accesses</a><a hidden class=anchor aria-hidden=true href=#a-minor-optimization-avoiding-unnecessary-memory-accesses>#</a></h3><p>A very small thing we could do with the naive implementation is avoiding multiple reads and writes of intermediate partial sums from and to the memory/cache, like so:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-C data-lang=C><span class=line><span class=cl><span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>N</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>j</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>j</span> <span class=o>&lt;</span> <span class=n>N</span><span class=p>;</span> <span class=n>j</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=kt>float</span> <span class=n>running_sum</span> <span class=o>=</span> <span class=mf>0.0</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>k</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>k</span> <span class=o>&lt;</span> <span class=n>N</span><span class=p>;</span> <span class=n>k</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=n>running_sum</span> <span class=o>+=</span> <span class=n>A</span><span class=p>[</span><span class=n>i</span><span class=p>][</span><span class=n>k</span><span class=p>]</span> <span class=o>*</span> <span class=n>B</span><span class=p>[</span><span class=n>k</span><span class=p>][</span><span class=n>j</span><span class=p>];</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>        <span class=n>C</span><span class=p>[</span><span class=n>i</span><span class=p>][</span><span class=n>j</span><span class=p>]</span> <span class=o>=</span> <span class=n>running_sum</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>The table below shows that helps very little for $N=4096$ but quite a lot for $N=8192$ <sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>:</p><table><thead><tr><th>Technique</th><th>4096</th><th>8192</th><th>Speedup ($N=4096$)</th><th>Speedup ($N=4096$)</th></tr></thead><tbody><tr><td><code>Baseline (np)</code></td><td><code>0.10s</code></td><td><code>0.74s</code></td><td><code>â€“</code></td><td><code>â€“</code></td></tr><tr><td><code>Naive implementation</code></td><td><code>203s</code></td><td><code>46min</code></td><td><code>-</code></td><td><code>-</code></td></tr><tr><td><code>Naive w register accumulation</code></td><td><code>199s</code></td><td><code>27min</code></td><td><code>1.02x</code></td><td><code>1.7x</code></td></tr></tbody></table><p>This optimization is simply causing the compiler to keep the partial sum in the registers only and write back to the memory only once when the full sum is done. Meaning in this case the compiler isn&rsquo;t issuing separate instructions to store the partial sum back to the memory after every loop iteration and to load it back on the next iteration; and that reduces some latency <sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup>.</p><h2 id=loop-reordering><a href=https://github.com/srishti-git1110/SGEMM-cpu/blob/main/sgemm-cpu/matmuls/cache_aware.c>Loop reordering</a><a hidden class=anchor aria-hidden=true href=#loop-reordering>#</a></h2><p>The naive implementation follows the most natural mathy way to calculate a matmul $C = AB$ &mdash; element $C[0][0]$ is <em>fully</em> calculated first via a scalar product of the first row of $A$ with the first column of $B$, element $C[0][1]$ is <em>fully</em> calculated next via the scalar product of the first row of $A$ with the second column of $B$, and so on.</p><p>There are two key insights over here:</p><ol><li>Languages like C store matrices in the memory in a row major format like this:</li></ol><p><img loading=lazy src=row-major.jpeg alt="row major order"></p><p>If we now follow the calculation of $C[0][0]$ in code, it&rsquo;s equivalent to completing the inner-most $k$ loop for $i=0, j=0$.</p><p>The first part of the figure below uses 3 shades of red to color the values from $A, B, C$ that are accessed in the code during the calculation of $C[0][0]$. Focus on the location of these values in the memory by following the color.</p><p><img loading=lazy src=matmul-values-accessed.png alt="values accessed"></p><p>The figure quickly makes it clear that the accessed values for $A, C$ are close to each other in the memory while they&rsquo;re further apart for $B$. Now, remember that data is fetched from the memory in the caches in the granularity of cache lines. One cache line on my machine is of size 128 bytes which is equivalent to 32 single precision values.</p><p>So on the very first loop iteration $i=0, j=0, k=0$, when $A[0][0], B[0][0], C[0][0]$ are fetched from the memory, the cache looks something like:</p><p><img loading=lazy src=cache.png alt="cache after first iter"></p><p>This is simply because a cache line loads contiguous values from the memory where the matrices are stored in a row major format.
On the second iteration $i=0, j=0, k=1$, we need $A[0][1], B[1][0], C[0][0]$ and while $A[0][1]$ and $C[0][0]$ are found in the cache, we have a miss for $B[1][0]$ that we need to fetch from the memory. And this holds for each iteration of the k-loop for $i=0, j=0$. Easy to figure out why &ndash; our cache line is only 128 bytes (32 floats) while each subsequent loop iteration accesses a value from B that&rsquo;s 4096 values apart from the value accessed in the last iteration. And this high cache miss rate explains the high latency we saw w the naive implementation.</p><p>What about the subsequent iterations for different values of j and i? Say $i=0, j=1, k=0$ when we need $B[0][1]$ that was a part of the cache line we loaded on $i=0, j=0, k=0$ &mdash; that cache line most probably would get evicted from the cache till now. Hence, the cache miss rate still remains high.</p><p>ðŸ‘‰ ðŸ‘‰ ðŸ‘‰ <em>This is actually a central recurring point of discussion: Some data that we load once gets evicted by the time we need it again making things go slower. In a few upcoming sections, we&rsquo;ll be revolving around this point making optimizations that aim to reuse the data in subsequent iterations by using it as much as needed (and possible) before its eviction.</em></p><ol start=2><li>The second insight is not too difficult to understand &ndash; if we just change the loop orders (eg. $jik$ or $jki$ etc. instead of the most natural $ijk$), we&rsquo;ll still get the correct matmul. It&rsquo;s also why I was italicizing <em>fully</em> above. Thing is that with different loop orders we&rsquo;re not fully calculating each element of C in one full iteration of the innermost loop but that doesn&rsquo;t hurt the correctness of the matmul and that&rsquo;s easy to realise. Alright. Given that, we note that some loop orders have a better overall cache hit rate as compared to the naive $ijk$ order (btw some orders also have a worse rate than $ijk$!). And hence just by changing the orders, we&rsquo;ll be able to reduce the latency by not having to make as many high latency accesses to the memory. Experimenting w different orders, the lowest latency corresponds to order ikj as follows:</li></ol><table><thead><tr><th>Technique</th><th>4096</th><th>8192</th><th>Speedup ($N=4096$)</th><th>Speedup ($N=4096$)</th></tr></thead><tbody><tr><td><code>Baseline (np)</code></td><td><code>0.10s</code></td><td><code>0.74s</code></td><td><code>â€“</code></td><td><code>â€“</code></td></tr><tr><td><code>Naive implementation</code></td><td><code>203s</code></td><td><code>46min</code></td><td><code>-</code></td><td><code>-</code></td></tr><tr><td><code>Naive w register accumulation</code></td><td><code>199s</code></td><td><code>27min</code></td><td><code>1.02x</code></td><td><code>1.7x</code></td></tr><tr><td><code>Loop reordering (ikj)</code></td><td><code>4.31s</code></td><td><code>34.28s</code></td><td><code>46x</code></td><td><code>47x</code></td></tr></tbody></table><div class=highlight><pre tabindex=0 class=chroma><code class=language-C data-lang=C><span class=line><span class=cl><span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>N</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>k</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>k</span> <span class=o>&lt;</span> <span class=n>N</span><span class=p>;</span> <span class=n>k</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=kt>float</span> <span class=n>a_ik</span> <span class=o>=</span> <span class=n>A</span><span class=p>[</span><span class=n>i</span><span class=p>][</span><span class=n>k</span><span class=p>];</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>j</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>j</span> <span class=o>&lt;</span> <span class=n>N</span><span class=p>;</span> <span class=n>j</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=n>C</span><span class=p>[</span><span class=n>i</span><span class=p>][</span><span class=n>j</span><span class=p>]</span> <span class=o>+=</span> <span class=n>a_ik</span> <span class=o>*</span> <span class=n>B</span><span class=p>[</span><span class=n>k</span><span class=p>][</span><span class=n>j</span><span class=p>];</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><h3 id=what-was-the-bottleneck>What was the bottleneck?<a hidden class=anchor aria-hidden=true href=#what-was-the-bottleneck>#</a></h3><p>A simple loop reordering of our naive implementation provided a 45 fold improvement. That&rsquo;s a lot for such a simple change. What does this tell us? Obviously, we still performed total 137 GFLOPs so that part didn&rsquo;t change. Turns out, in the naive implementation our bottleneck was the memory bandwidth. Meaning the CPU execution units (ALUs) were bottlenecked by the latency of data transfer between the memory and the CPU, implying we were overall <em>memory bound in the naive implementation</em>. Of course, we cannot change the memory bandwidth towards a faster memory. So what did we do? We simply wrote the code such that it allows for better cache reuse in subsequent loop iters!</p><h3 id=what-about-the-computation-aspect>What about the computation aspect?<a hidden class=anchor aria-hidden=true href=#what-about-the-computation-aspect>#</a></h3><p>// vectorization enabled by loop ordering - figured out by the compiler</p><h3 id=what-about-the-minor-optimization-from-above>What about the minor optimization from <a href=https://srishti-git1110.github.io/blog/matmul-cpu/#a-minor-optimization-avoiding-unnecessary-memory-accesses>above</a>?<a hidden class=anchor aria-hidden=true href=#what-about-the-minor-optimization-from-above>#</a></h3><p>Because with the reordered loops, we&rsquo;re not calculating the full result $C[i][j]$ in one iteration of the inner most loop, we can&rsquo;t avoid the loading and storing of partial sums anymore!</p><h2 id=tiling>Tiling<a hidden class=anchor aria-hidden=true href=#tiling>#</a></h2><p>The long story short about tiling: it&rsquo;s all about enabling a better cache reuse. Nothing different from the goal of loop reordering.</p><p>Let&rsquo;s try to understand tiling by doing something boring. For our best loop order $ikj$, we start by looking at the values that are accessed by the inner $k, j$ loops for different values of $i$.</p><p>$(i=0, k=0-4095, j=0-4095)$</p><ul><li>$A[0][0], A[0][1], &mldr; A[0][4095]$ &ndash;> A&rsquo;s 1st row</li><li>$C[0][0], C[0][1], &mldr; C[0][4095]$ &ndash;> C&rsquo;s 1st row</li><li>$B[0][0], B[0][1], &mldr; B[0][4095]$ &ndash;> B&rsquo;s 1st row</li><li>$B[1][0], B[1][1], &mldr; B[1][4095]$ &ndash;> B&rsquo;s 2nd row
&mldr;</li><li>$B[4095][0], B[4095][1], &mldr; B[4095][4095]$ &ndash;> B&rsquo;s 4095th row</li></ul><p>$(i=1, k=0-4095, j=0-4095)$</p><ul><li>$A[1][0], A[1][1], &mldr; A[1][4095]$ &ndash;> A&rsquo;s 2nd row</li><li>$C[1][0], C[1][1], &mldr; C[1][4095]$ &ndash;> C&rsquo;s 2nd row</li><li>$B[0][0], B[0][1], &mldr; B[0][4095]$ &ndash;> B&rsquo;s 1st row</li><li>$B[1][0], B[1][1], &mldr; B[1][4095]$ &ndash;> B&rsquo;s 2nd row</li></ul><p>&mldr;</p><ul><li>$B[4095][0], B[4095][1], &mldr; B[4095][4095]$ &ndash;> B&rsquo;s 4095th row</li></ul><p>What do we infer?</p><p>ðŸ‘‰ At every iteration of the $i$ loop, the code accesses a different row of A and C but all the rows of B are accessed at each iteration.</p><p>So far so good. Let&rsquo;s now take an example with smaller 8x8 matrices and see what caches actually look like with our best loop order. For the example, assume a fully associative cache following an LRU (least recently used) policy with a cache line of 32 bytes = 8 floats, and a cache size of 160 bytes = 5 cache lines.</p><p>The (terrible) figure below shows the values accessed in sequence for $i=0$, and the state of the cache at each step. Red denotes a cache miss requiring a cache update and blue denotes a cache hit with no cache update required. Note that by the time we end $k=2$ for, our cache is full.</p><p><img loading=lazy src=first_cache_full.jpeg alt="i=0, k=2"></p><p>The figure below shows the program resumed from $k=3$. Also shown is the cache state at the end of $i=0$ after multiple evictions. Note how the first 5 rows of B had to be evicted even before reaching $i=1$.</p><p><img loading=lazy src=cache_i0.jpeg alt="i=0 done"></p><p>Now what? Start $i=1$. The same story gets repeated and below are the various cache states showing how we need to load the <em>entire B matrix from the memory</em> to be able to finish $i=1$! Not even a single row found in cache when needed despite the fact that all of them were loaded in the previous iteration $i=0$.</p><p><img loading=lazy src=cache_i1.jpeg alt="i=1"></p><p>Of course, this was true for our particular example that 1 cache line = 1 row, and no single row of B was found in the cache. In practice, the number of caches, their sizes, eviction policies, cache line sizes all matter in determining what rows/values are found but the general gist stated below remains the same.</p><p>The gist: At each new value of i, we&rsquo;re having to load all or some rows (/partial rows depending on the cache line) of B from the high latency memory despite having them loaded in the previous iteration of i. We should definitely do better!</p><h3 id=k-tiling>k-tiling<a hidden class=anchor aria-hidden=true href=#k-tiling>#</a></h3><p>Think about this: how about there was no eviction and hence no need to reload values of B at each new iteration of i? It&rsquo;d have been great except that caches are limited and we cannot increase their size. How do we make peace with eviction? By using the data as much as we want before it&rsquo;s evicted. Meaning we load a few rows of B in the cache and use them for all the values of i so that we never require them again? Then load the next few and again use them for all the values of i. The same until we&rsquo;re done with all the rows of B. This is where the word tile comes from &ndash; we&rsquo;re loading and utilizing one tile fully before evicting it.</p><p>What I&rsquo;ve described is tiling on the k-loop only. In code:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-C data-lang=C><span class=line><span class=cl><span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>k_tile</span><span class=o>=</span><span class=mi>0</span><span class=p>;</span> <span class=n>k_tile</span><span class=o>&lt;</span><span class=n>N</span><span class=p>;</span> <span class=n>k_tile</span><span class=o>+=</span><span class=n>TILE_SIZE</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span><span class=o>=</span><span class=mi>0</span><span class=p>;</span> <span class=n>i</span><span class=o>&lt;</span><span class=n>N</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=kt>int</span> <span class=n>kend</span> <span class=o>=</span> <span class=p>(</span><span class=n>k_tile</span> <span class=o>+</span> <span class=n>TILE_SIZE</span> <span class=o>&gt;</span> <span class=n>N</span><span class=p>)</span> <span class=o>?</span> <span class=nl>N</span> <span class=p>:</span> <span class=n>k_tile</span> <span class=o>+</span> <span class=n>TILE_SIZE</span><span class=p>;</span> <span class=c1>// check if this is expensive than an if
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>k</span><span class=o>=</span><span class=n>k_tile</span><span class=p>;</span> <span class=n>k</span><span class=o>&lt;</span><span class=n>kend</span><span class=p>;</span> <span class=n>k</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=kt>float</span> <span class=n>a_ik</span> <span class=o>=</span> <span class=n>A</span><span class=p>[</span><span class=n>i</span><span class=p>][</span><span class=n>k</span><span class=p>];</span>
</span></span><span class=line><span class=cl>            <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>j</span><span class=o>=</span><span class=mi>0</span><span class=p>;</span> <span class=n>j</span><span class=o>&lt;</span><span class=n>N</span><span class=p>;</span> <span class=n>j</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>                <span class=n>C</span><span class=p>[</span><span class=n>i</span><span class=p>][</span><span class=n>j</span><span class=p>]</span> <span class=o>+=</span> <span class=n>a_ik</span> <span class=o>*</span> <span class=n>B</span><span class=p>[</span><span class=n>k</span><span class=p>][</span><span class=n>j</span><span class=p>];</span>
</span></span><span class=line><span class=cl>            <span class=p>}</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>After doing a good amount of search over different values of TILE_SIZE, I found k-tiling didn&rsquo;t really yield any benefit on my machine both for $N=4096$ and $N=8192$. This is probably because of the already large cache sizes of my machime.</p><h3 id=ijk-tiling>ijk tiling<a hidden class=anchor aria-hidden=true href=#ijk-tiling>#</a></h3><p>Let&rsquo;s now separately understand the purpose of tiling rest of the two loops - i and j.</p><p><u>Tiling on the j-loop </u>: With k-tiling rather than loading all the rows of $B$ in the cache for each iteration of $i$, we&rsquo;re covering a particular number of rows at a time that fit in the cache. But for these rows, we&rsquo;re still loading all the columns of $B$ (all values of $j$)! In the oversimplified example above, 1 row = 1 cache line but that obviously isn&rsquo;t the case with bigger matrices and hence, it&rsquo;d further benefit for the cache hit rate to also tile on the $j$ loop.</p><p><u>Tiling on the i-loop </u>: Further with both $k$ and $j$ tiled, we&rsquo;d still be needing all the rows of C (all values of i, but ofc not full rows due to j-tiling) in the cache multiple times for different values of <code>k_tile</code> and <code>j_tile</code>. And that might again lead to cache misses for certain values of $C$. And hence, it also benefits to tile on the $i$ loop which, combined with $j$ tiling, effectively translates to taking a sub-matrix of C and finishing all the calculations for it (covering all values of k) before proceeding to another sub-matrix. Of course, we cover all the k values in a tiled manner only.</p><p>In code, this looks like:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-C data-lang=C><span class=line><span class=cl><span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i_tile</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i_tile</span> <span class=o>&lt;</span> <span class=n>N</span><span class=p>;</span> <span class=n>i_tile</span> <span class=o>+=</span> <span class=n>TILE_I</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=kt>int</span> <span class=n>iend</span> <span class=o>=</span> <span class=p>(</span><span class=n>i_tile</span> <span class=o>+</span> <span class=n>TILE_I</span> <span class=o>&lt;</span> <span class=n>N</span><span class=p>)</span> <span class=o>?</span> <span class=n>i_tile</span> <span class=o>+</span> <span class=nl>TILE_I</span> <span class=p>:</span> <span class=n>N</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>j_tile</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>j_tile</span> <span class=o>&lt;</span> <span class=n>N</span><span class=p>;</span> <span class=n>j_tile</span> <span class=o>+=</span> <span class=n>TILE_J</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=kt>int</span> <span class=n>jend</span> <span class=o>=</span> <span class=p>(</span><span class=n>j_tile</span> <span class=o>+</span> <span class=n>TILE_J</span> <span class=o>&lt;</span> <span class=n>N</span><span class=p>)</span> <span class=o>?</span> <span class=n>j_tile</span> <span class=o>+</span> <span class=nl>TILE_J</span> <span class=p>:</span> <span class=n>N</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=cm>/* for a certain tile of C (i_tile:iend, j_tile, jend) we now cover all values of k in our already
</span></span></span><span class=line><span class=cl><span class=cm>            discussed k-tiled manner */</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>k_tile</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>k_tile</span> <span class=o>&lt;</span> <span class=n>N</span><span class=p>;</span> <span class=n>k_tile</span> <span class=o>+=</span> <span class=n>TILE_K</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>                <span class=kt>int</span> <span class=n>kend</span> <span class=o>=</span> <span class=p>(</span><span class=n>k_tile</span> <span class=o>+</span> <span class=n>TILE_K</span> <span class=o>&lt;</span> <span class=n>N</span><span class=p>)</span> <span class=o>?</span> <span class=n>k_tile</span> <span class=o>+</span> <span class=nl>TILE_K</span> <span class=p>:</span> <span class=n>N</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>                <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=n>i_tile</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>iend</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>                    <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>k</span> <span class=o>=</span> <span class=n>k_tile</span><span class=p>;</span> <span class=n>k</span> <span class=o>&lt;</span> <span class=n>kend</span><span class=p>;</span> <span class=n>k</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>                        <span class=kt>float</span> <span class=n>a_ik</span> <span class=o>=</span> <span class=n>A</span><span class=p>[</span><span class=n>i</span><span class=p>][</span><span class=n>k</span><span class=p>];</span>
</span></span><span class=line><span class=cl>                        <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>j</span> <span class=o>=</span> <span class=n>j_tile</span><span class=p>;</span> <span class=n>j</span> <span class=o>&lt;</span> <span class=n>jend</span><span class=p>;</span> <span class=n>j</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>                            <span class=n>C</span><span class=p>[</span><span class=n>i</span><span class=p>][</span><span class=n>j</span><span class=p>]</span> <span class=o>+=</span> <span class=n>a_ik</span> <span class=o>*</span> <span class=n>B</span><span class=p>[</span><span class=n>k</span><span class=p>][</span><span class=n>j</span><span class=p>];</span>
</span></span><span class=line><span class=cl>                        <span class=p>}</span>
</span></span><span class=line><span class=cl>                    <span class=p>}</span>
</span></span><span class=line><span class=cl>                <span class=p>}</span>
</span></span><span class=line><span class=cl>            <span class=p>}</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span></code></pre></div><p>With ijk tiling, the results are as follows:</p><table><thead><tr><th>Technique</th><th>4096</th><th>8192</th><th>Speedup ($N=4096$)</th><th>Speedup ($N=4096$)</th></tr></thead><tbody><tr><td><code>Baseline (np)</code></td><td><code>0.10s</code></td><td><code>0.74s</code></td><td><code>â€“</code></td><td><code>â€“</code></td></tr><tr><td><code>Naive implementation</code></td><td><code>203s</code></td><td><code>46min</code></td><td><code>-</code></td><td><code>-</code></td></tr><tr><td><code>Naive w register accumulation</code></td><td><code>199s</code></td><td><code>27min</code></td><td><code>1.02x</code></td><td><code>1.7x</code></td></tr><tr><td><code>Loop reordering (ikj)</code></td><td><code>4.31s</code></td><td><code>34.28s</code></td><td><code>46x</code></td><td><code>47x</code></td></tr><tr><td><code>ijk tiling (best tile sizes)</code></td><td><code>3.16s</code></td><td><code>26.20s</code></td><td><code>1.36</code></td><td><code>1.3x</code></td></tr></tbody></table><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>This is for the standard algorithm. There&rsquo;s other algos like <a href=https://en.wikipedia.org/wiki/Strassen_algorithm>Strassen&rsquo;s</a> with better theoretical complexity.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>Explore all optimization levels <a href=https://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html>here</a>.&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>But this optimization wasn&rsquo;t the reason why I decided to also run numbers for $N=8192$; the reason was ijk-tiling discussed later in the blog.&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p>Sometimes, the compiler might also consider it safe to skip the extra store/reload steps when we directly update $C[i][j]$ in every loop iteration. But other times it may not. So, we&rsquo;re just being explicit here and writing code such that the compiler would <em>always</em> consider it safe to not issue the extra store/reload instructions for the partial sums.&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/optimization-parallel-processing/>Optimization, Parallel Processing</a></li></ul><nav class=paginav><a class=next href=http://localhost:1313/blog/hardware-primer/><span class=title>Next Â»</span><br><span>[WIP] CPU, GPU and stuff!</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2026 <a href=http://localhost:1313/>Srishti Gureja</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}}</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js defer></script></body></html>