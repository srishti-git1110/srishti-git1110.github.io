<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>[WIP] Optimizing matmul on CPU | Srishti Gureja</title><meta name=keywords content="Optimization,Parallel Processing"><meta name=description content="Introduction and Setup
This blog starts with a naive implementation of matmul in C and optimizes it one step at a time.
I am using the following machine with a 10 core CPU (4 Performance cores+6 Efficiency cores).
Food for thought: Is matrix multiplication on CPU compute bound or memory bound? Think about it&mldr;
Algorithmic complexity of Matrix Multiplication: Calculating the FLOPs required
Matrix multiplication is ubiquitous in many areas of computer sciene. As the matrices grow in sizes, the amount of FLOPs required to calculate the matmul grow cubically1. Let&rsquo;s see how."><meta name=author content><link rel=canonical href=http://localhost:1313/blog/matmul-cpu/><link crossorigin=anonymous href=/assets/css/stylesheet.b26ba54a60d00ea06fda6b711f3da6382e5fe3a6fae7b4cc58bdc38f1f26bddc.css integrity="sha256-smulSmDQDqBv2mtxHz2mOC5f46b657TMWL3Djx8mvdw=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/blog/matmul-cpu/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="[WIP] Optimizing matmul on CPU"><meta property="og:description" content="Introduction and Setup
This blog starts with a naive implementation of matmul in C and optimizes it one step at a time.
I am using the following machine with a 10 core CPU (4 Performance cores+6 Efficiency cores).
Food for thought: Is matrix multiplication on CPU compute bound or memory bound? Think about it&mldr;
Algorithmic complexity of Matrix Multiplication: Calculating the FLOPs required
Matrix multiplication is ubiquitous in many areas of computer sciene. As the matrices grow in sizes, the amount of FLOPs required to calculate the matmul grow cubically1. Let&rsquo;s see how."><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/blog/matmul-cpu/"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-12-20T00:00:00+00:00"><meta property="article:modified_time" content="2025-12-20T00:00:00+00:00"><meta property="og:site_name" content="Srishti Gureja"><meta name=twitter:card content="summary"><meta name=twitter:title content="[WIP] Optimizing matmul on CPU"><meta name=twitter:description content="Introduction and Setup
This blog starts with a naive implementation of matmul in C and optimizes it one step at a time.
I am using the following machine with a 10 core CPU (4 Performance cores+6 Efficiency cores).
Food for thought: Is matrix multiplication on CPU compute bound or memory bound? Think about it&mldr;
Algorithmic complexity of Matrix Multiplication: Calculating the FLOPs required
Matrix multiplication is ubiquitous in many areas of computer sciene. As the matrices grow in sizes, the amount of FLOPs required to calculate the matmul grow cubically1. Let&rsquo;s see how."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blogs","item":"http://localhost:1313/blog/"},{"@type":"ListItem","position":2,"name":"[WIP] Optimizing matmul on CPU","item":"http://localhost:1313/blog/matmul-cpu/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"[WIP] Optimizing matmul on CPU","name":"[WIP] Optimizing matmul on CPU","description":"Introduction and Setup This blog starts with a naive implementation of matmul in C and optimizes it one step at a time.\nI am using the following machine with a 10 core CPU (4 Performance cores+6 Efficiency cores).\nFood for thought: Is matrix multiplication on CPU compute bound or memory bound? Think about it\u0026hellip;\nAlgorithmic complexity of Matrix Multiplication: Calculating the FLOPs required Matrix multiplication is ubiquitous in many areas of computer sciene. As the matrices grow in sizes, the amount of FLOPs required to calculate the matmul grow cubically1. Let\u0026rsquo;s see how.\n","keywords":["Optimization, Parallel Processing"],"articleBody":"Introduction and Setup This blog starts with a naive implementation of matmul in C and optimizes it one step at a time.\nI am using the following machine with a 10 core CPU (4 Performance cores+6 Efficiency cores).\nFood for thought: Is matrix multiplication on CPU compute bound or memory bound? Think about it…\nAlgorithmic complexity of Matrix Multiplication: Calculating the FLOPs required Matrix multiplication is ubiquitous in many areas of computer sciene. As the matrices grow in sizes, the amount of FLOPs required to calculate the matmul grow cubically1. Let’s see how.\nConsider two matrices $A (i \\times k)$ and $B (k \\times j)$. The product of $A$ and $B$, $AB$ is a matrix $C$ of shape $(i \\times j)$.\nFor simplicity and without loss of generality, let’s consider the matrices to be square so we have $A (n \\times n)$, $B (n \\times n)$ and their product $C (n \\times n)$. One element $(c_1, c_2)$ of $C$ is defined as:\n$$c_{c1,c2} = \\sum_{x=1}^{n} a_{c1,x} b_{x,c2}$$\nThis is a total of 2n - 1 floating point operations (FLOPs) required to calculate one element of C – $n$ multiplication ops + $(n-1)$ addition ops. A total of $n^2$ elements need to be calculated in $C$ and hence the total FLOPs:\n$$(2n - 1) n^2 = 2n^3 - n^2$$\nAs $n$ grows bigger (asymptomatic, if you’re feeling fancy), $n^2$ becomes pretty negligible in comparison to $2n^3$ and hence can be ignored so the total FLOPs required is roughly $2n^3$. Therefore, the computational complexity is $O(n^3)$.\nFor the purpose of this blog, I am keeping $n=4096$ which translates to roughly 137 GFLOPs. That’s seems like a lot of FLOPs, but is rather pretty small if compared with the peak FLOPs offered by any standard modern processor.\nTo benchmark, I calculate the time required to multiply two $4096 x 4096$ numpy arrays using np.matmul and it comes out to 0.1042s.\nNaive implementation The matmul loop is this:\nfor (int i = 0; i \u003c N; i++) { for (int j = 0; j \u003c N; j++) { for (int k = 0; k \u003c N; k++) { C[i][j] += A[i][k] * B[k][j]; } } } When complied with the -O3 flag2 which is the maximum level with safe optimizations, the latency is 299.289 sec. Full compilation command below:\ngcc -Wall -O3 sgemm-cpu/matmuls/naive.c -o sgemm-cpu/matmuls/naive All the further optimizations use the same flags to compile the code.\nLoop reordering The naive implementation follows the most natural mathy way to calculate a matmul $C = AB$ – element $C[0][0]$ is fully calculated first via a scalar product of the first row of $A$ with the first column of $B$. Element $C[0][1]$ is fully calculated next via the scalar product of the first row of $A$ with the second column of $B$, and so on.\nThere are two key insights over here:\nLanguages like C store matrices in the memory in a row major format like this: If we now follow the calculation of $C[0][0]$ in code, it’s equivalent to completing the inner-most $k$ loop for $i=0, j=0$ and the following values from $A, B, C$ are accessed at each subsequent iteration of this loop (focus on the location of these values in the memory by following the color):\nRemember that data is fetched from the memory in the caches in the granularity of cache lines. One cache line on my machine is of size 128 bytes which is equivalent to 32 single precision values.\nSo on the very first loop iteration $i=0, j=0, k=0$, when $A[0][0], B[0][0], C[0][0]$ are fetched from the memory, the cache looks something like:\nThis is simply because a cache line loads contiguous values from the memory where the matrices are stored in a row major format. On the second iteration $i=0, j=0, k=1$, we need $A[0][1], B[1][0], C[0][0]$ and while A[0][1] and C[0][0] are found in the cache, we have a miss for B[1][0] that we need to fetch from the memory. And this holds for each iteration of the loop. Easy to figure out why – our cache line is only 128 bytes (32 floats) while each subsequent loop iteration accesses a value from B that’s 4096 values apart from the value accessed in the last iteration. And this high cache miss rate explains the high latency we saw w the naive implementation.\nThe second insight is not too difficult to understand – if we just change the loop orders (eg. jik or jki etc. instead of the most natural ijk), we’ll still get the correct matmul. It’s also why I was italicizing fully above. Thing is that with different loop orders we’re not fully calculating each element of C in one full iteration of the innermost loop but that doesn’t hurt the correctness of the matmul and that’s easy to realise. Alright. Given that, we note that some loop orders have a better overall cache hit rate as compared to the naive ijk order (btw some orders also have a worse rate than ijk!). And hence just by changing the orders, we’ll be able to reduce the latency by not having to make as many high latency accesses to the memory. Experimenting w different orders, the lowest latency of 4.49s corresponds to order ikj down from 203.229s with order ijk which is a ~45x improvement already! for (int i = 0; i \u003c N; i++) { for (int k = 0; k \u003c N; k++) { for (int j = 0; j \u003c N; j++) { C[i][j] += A[i][k] * B[k][j]; } } } What was the bottleneck? A simple loop reordering of our naive implementation provided a 45 fold improvement. That’s a lot for such a simple change. What does this tell us? Obviously, we still performed total 137 GFLOPs so that part didn’t change. Turns out, in the naive implementation our bottleneck was the memory bandwidth. Meaning the CPU execution units (ALUs) were bottlenecked by the latency of data transfer between the memory and the CPU, implying we were overall memory bound in the naive implementation. Of course, we cannot change the memory bandwidth towards a faster memory. So what did we do? We simply wrote the code such that it allows for better cache reuse in subsequent loop iters!\nWhat about the computation aspect? Tiling This is for the standard algorithm. There’s other algos like Strassen’s with better complexity. ↩︎\nExplore all optimization levels here. ↩︎\n","wordCount":"1069","inLanguage":"en","datePublished":"2025-12-20T00:00:00Z","dateModified":"2025-12-20T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/blog/matmul-cpu/"},"publisher":{"@type":"Organization","name":"Srishti Gureja","logo":{"@type":"ImageObject","url":"http://localhost:1313/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Srishti Gureja (Alt + H)">Srishti Gureja</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/code/ title=code><span>code</span></a></li><li><a href=http://localhost:1313/blog/ title=blog><span>blog</span></a></li><li><a href=http://localhost:1313/talks/ title=talks><span>talks</span></a></li><li><a href=http://localhost:1313/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/blog/>Blogs</a></div><h1 class="post-title entry-hint-parent">[WIP] Optimizing matmul on CPU</h1><div class=post-meta><span title='2025-12-20 00:00:00 +0000 UTC'>December 20, 2025</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#introduction-and-setup>Introduction and Setup</a></li><li><a href=#algorithmic-complexity-of-matrix-multiplication-calculating-the-flops-required>Algorithmic complexity of Matrix Multiplication: Calculating the FLOPs required</a></li><li><a href=#naive-implementation><a href=https://github.com/srishti-git1110/SGEMM-cpu/blob/main/sgemm-cpu/matmuls/naive.c>Naive implementation</a></a></li><li><a href=#loop-reordering><a href=https://github.com/srishti-git1110/SGEMM-cpu/blob/main/sgemm-cpu/matmuls/cache_aware.c>Loop reordering</a></a><ul><li><a href=#what-was-the-bottleneck>What was the bottleneck?</a></li><li><a href=#what-about-the-computation-aspect>What about the computation aspect?</a></li></ul></li><li><a href=#tiling>Tiling</a></li></ul></nav></div></details></div><div class=post-content><h2 id=introduction-and-setup>Introduction and Setup<a hidden class=anchor aria-hidden=true href=#introduction-and-setup>#</a></h2><p>This blog starts with a naive implementation of matmul in C and optimizes it one step at a time.</p><p>I am using <a href=https://www.apple.com/in/shop/buy-mac/macbook-pro/14-inch-space-black-standard-display-apple-m5-chip-with-10-core-cpu-and-10-core-gpu-16gb-memory-512gb>the following machine</a> with a 10 core CPU (4 Performance cores+6 Efficiency cores).</p><p>Food for thought: Is matrix multiplication on CPU compute bound or memory bound? Think about it&mldr;</p><h2 id=algorithmic-complexity-of-matrix-multiplication-calculating-the-flops-required>Algorithmic complexity of Matrix Multiplication: Calculating the FLOPs required<a hidden class=anchor aria-hidden=true href=#algorithmic-complexity-of-matrix-multiplication-calculating-the-flops-required>#</a></h2><p>Matrix multiplication is ubiquitous in many areas of computer sciene. As the matrices grow in sizes, the amount of FLOPs required to calculate the matmul grow cubically<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>. Let&rsquo;s see how.</p><p>Consider two matrices $A (i \times k)$ and $B (k \times j)$. The product of $A$ and $B$, $AB$ is a matrix $C$ of shape $(i \times j)$.</p><p>For simplicity and without loss of generality, let&rsquo;s consider the matrices to be square so we have $A (n \times n)$, $B (n \times n)$ and their product $C (n \times n)$. One element $(c_1, c_2)$ of $C$ is defined as:</p><p>$$c_{c1,c2} = \sum_{x=1}^{n} a_{c1,x} b_{x,c2}$$</p><p>This is a total of 2n - 1 floating point operations (FLOPs) required to calculate one element of C &ndash; $n$ multiplication ops + $(n-1)$ addition ops. A total of $n^2$ elements need to be calculated in $C$ and hence the total FLOPs:</p><p>$$(2n - 1) n^2 = 2n^3 - n^2$$</p><p>As $n$ grows bigger (asymptomatic, if you&rsquo;re feeling fancy), $n^2$ becomes pretty negligible in comparison to $2n^3$ and hence can be ignored so the total FLOPs required is roughly $2n^3$. Therefore, the computational complexity is $O(n^3)$.</p><p>For the purpose of this blog, I am keeping $n=4096$ which translates to roughly 137 GFLOPs. That&rsquo;s seems like a lot of FLOPs, but is rather pretty small if compared with the peak FLOPs offered by any standard modern processor.</p><p>To benchmark, I calculate the time required to multiply two $4096 x 4096$ numpy arrays using <code>np.matmul</code> and it comes out to 0.1042s.</p><h2 id=naive-implementation><a href=https://github.com/srishti-git1110/SGEMM-cpu/blob/main/sgemm-cpu/matmuls/naive.c>Naive implementation</a><a hidden class=anchor aria-hidden=true href=#naive-implementation>#</a></h2><p>The matmul loop is this:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-C data-lang=C><span class=line><span class=cl><span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>N</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>j</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>j</span> <span class=o>&lt;</span> <span class=n>N</span><span class=p>;</span> <span class=n>j</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>k</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>k</span> <span class=o>&lt;</span> <span class=n>N</span><span class=p>;</span> <span class=n>k</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>                <span class=n>C</span><span class=p>[</span><span class=n>i</span><span class=p>][</span><span class=n>j</span><span class=p>]</span> <span class=o>+=</span> <span class=n>A</span><span class=p>[</span><span class=n>i</span><span class=p>][</span><span class=n>k</span><span class=p>]</span> <span class=o>*</span> <span class=n>B</span><span class=p>[</span><span class=n>k</span><span class=p>][</span><span class=n>j</span><span class=p>];</span>
</span></span><span class=line><span class=cl>            <span class=p>}</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span></code></pre></div><p>When complied with the <code>-O3</code> flag<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup> which is the maximum level with safe optimizations, the latency is 299.289 sec. Full compilation command below:</p><pre tabindex=0><code>gcc -Wall -O3 sgemm-cpu/matmuls/naive.c -o sgemm-cpu/matmuls/naive
</code></pre><p>All the further optimizations use the same flags to compile the code.</p><h2 id=loop-reordering><a href=https://github.com/srishti-git1110/SGEMM-cpu/blob/main/sgemm-cpu/matmuls/cache_aware.c>Loop reordering</a><a hidden class=anchor aria-hidden=true href=#loop-reordering>#</a></h2><p>The naive implementation follows the most natural mathy way to calculate a matmul $C = AB$ &ndash; element $C[0][0]$ is <em>fully</em> calculated first via a scalar product of the first row of $A$ with the first column of $B$. Element $C[0][1]$ is <em>fully</em> calculated next via the scalar product of the first row of $A$ with the second column of $B$, and so on.</p><p>There are two key insights over here:</p><ol><li>Languages like C store matrices in the memory in a row major format like this:</li></ol><p><img loading=lazy src=row-major.jpeg alt="row major order"></p><p>If we now follow the calculation of $C[0][0]$ in code, it&rsquo;s equivalent to completing the inner-most $k$ loop for $i=0, j=0$ and the following values from $A, B, C$ are accessed at each subsequent iteration of this loop (focus on the location of these values in the memory by following the color):</p><p><img loading=lazy src=matmul-values-accessed.png alt="values accessed"></p><p>Remember that data is fetched from the memory in the caches in the granularity of cache lines. One cache line on my machine is of size 128 bytes which is equivalent to 32 single precision values.</p><p>So on the very first loop iteration $i=0, j=0, k=0$, when $A[0][0], B[0][0], C[0][0]$ are fetched from the memory, the cache looks something like:</p><p><img loading=lazy src=cache.png alt="cache after first iter"></p><p>This is simply because a cache line loads contiguous values from the memory where the matrices are stored in a row major format.
On the second iteration $i=0, j=0, k=1$, we need $A[0][1], B[1][0], C[0][0]$ and while A[0][1] and C[0][0] are found in the cache, we have a miss for B[1][0] that we need to fetch from the memory. And this holds for each iteration of the loop. Easy to figure out why &ndash; our cache line is only 128 bytes (32 floats) while each subsequent loop iteration accesses a value from B that&rsquo;s 4096 values apart from the value accessed in the last iteration. And this high cache miss rate explains the high latency we saw w the naive implementation.</p><ol start=2><li>The second insight is not too difficult to understand &ndash; if we just change the loop orders (eg. jik or jki etc. instead of the most natural ijk), we&rsquo;ll still get the correct matmul. It&rsquo;s also why I was italicizing <em>fully</em> above. Thing is that with different loop orders we&rsquo;re not fully calculating each element of C in one full iteration of the innermost loop but that doesn&rsquo;t hurt the correctness of the matmul and that&rsquo;s easy to realise. Alright. Given that, we note that some loop orders have a better overall cache hit rate as compared to the naive ijk order (btw some orders also have a worse rate than ijk!). And hence just by changing the orders, we&rsquo;ll be able to reduce the latency by not having to make as many high latency accesses to the memory. Experimenting w different orders, the lowest latency of 4.49s corresponds to order ikj down from 203.229s with order ijk which is a ~45x improvement already!</li></ol><div class=highlight><pre tabindex=0 class=chroma><code class=language-C data-lang=C><span class=line><span class=cl><span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>N</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>k</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>k</span> <span class=o>&lt;</span> <span class=n>N</span><span class=p>;</span> <span class=n>k</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>j</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>j</span> <span class=o>&lt;</span> <span class=n>N</span><span class=p>;</span> <span class=n>j</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>                <span class=n>C</span><span class=p>[</span><span class=n>i</span><span class=p>][</span><span class=n>j</span><span class=p>]</span> <span class=o>+=</span> <span class=n>A</span><span class=p>[</span><span class=n>i</span><span class=p>][</span><span class=n>k</span><span class=p>]</span> <span class=o>*</span> <span class=n>B</span><span class=p>[</span><span class=n>k</span><span class=p>][</span><span class=n>j</span><span class=p>];</span>
</span></span><span class=line><span class=cl>            <span class=p>}</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span></code></pre></div><h3 id=what-was-the-bottleneck>What was the bottleneck?<a hidden class=anchor aria-hidden=true href=#what-was-the-bottleneck>#</a></h3><p>A simple loop reordering of our naive implementation provided a 45 fold improvement. That&rsquo;s a lot for such a simple change. What does this tell us? Obviously, we still performed total 137 GFLOPs so that part didn&rsquo;t change. Turns out, in the naive implementation our bottleneck was the memory bandwidth. Meaning the CPU execution units (ALUs) were bottlenecked by the latency of data transfer between the memory and the CPU, implying we were overall memory bound in the naive implementation. Of course, we cannot change the memory bandwidth towards a faster memory. So what did we do? We simply wrote the code such that it allows for better cache reuse in subsequent loop iters!</p><h3 id=what-about-the-computation-aspect>What about the computation aspect?<a hidden class=anchor aria-hidden=true href=#what-about-the-computation-aspect>#</a></h3><h2 id=tiling>Tiling<a hidden class=anchor aria-hidden=true href=#tiling>#</a></h2><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>This is for the standard algorithm. There&rsquo;s other algos like <a href=https://en.wikipedia.org/wiki/Strassen_algorithm>Strassen&rsquo;s</a> with better complexity.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>Explore all optimization levels <a href=https://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html>here</a>.&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/optimization-parallel-processing/>Optimization, Parallel Processing</a></li></ul><nav class=paginav><a class=next href=http://localhost:1313/blog/hardware-primer/><span class=title>Next »</span><br><span>[WIP] CPU, GPU and stuff!</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=http://localhost:1313/>Srishti Gureja</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}}</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js defer></script></body></html>