<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>[WIP] Learning CUDA Programming: A Primer | Srishti Gureja</title>
<meta name=keywords content="CUDA,Computer Architecture,Optimization,GPU Programming,Parallel Processing"><meta name=description content="A friendly primer to learning CUDA programming"><meta name=author content><link rel=canonical href=http://localhost:1313/blog/cuda-primer/><link crossorigin=anonymous href=/assets/css/stylesheet.332775c49e0220aac9dda57353b6b3279b1e6c6d75e3da6b878afa883f329282.css integrity="sha256-Myd1xJ4CIKrJ3aVzU7azJ5sebG1149prh4r6iD8ykoI=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/blog/cuda-primer/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="[WIP] Learning CUDA Programming: A Primer"><meta property="og:description" content="A friendly primer to learning CUDA programming"><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/blog/cuda-primer/"><meta property="article:section" content="blog"><meta property="article:published_time" content="2024-03-31T00:00:00+00:00"><meta property="article:modified_time" content="2024-03-31T00:00:00+00:00"><meta property="og:site_name" content="Srishti Gureja"><meta name=twitter:card content="summary"><meta name=twitter:title content="[WIP] Learning CUDA Programming: A Primer"><meta name=twitter:description content="A friendly primer to learning CUDA programming"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blogs","item":"http://localhost:1313/blog/"},{"@type":"ListItem","position":2,"name":"[WIP] Learning CUDA Programming: A Primer","item":"http://localhost:1313/blog/cuda-primer/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"[WIP] Learning CUDA Programming: A Primer","name":"[WIP] Learning CUDA Programming: A Primer","description":"A friendly primer to learning CUDA programming","keywords":["CUDA, Computer Architecture, Optimization, GPU Programming, Parallel Processing"],"articleBody":"I want this post to be an entry point for anyone willing to learn GPU programming without a formal background in compsci/comp arch (I myself belong to this category so sorry in advance for any errors that I might make, and please email me to get them corrected).\nI will cover the architectural details of two of the several processors that empower the modern day computers - the CPUs and the GPUs. By the end of this post, the reader should have a good understanding of the following terms - (in no particular order) chips, processors, microprocessors, cores, latency device, throughput device, clock speed, threads, processes, instructions, memory bandwidth, memory system.\nWhy are we here? A few months back, I used to visit the PyTorch forums almost daily. When I just began reading over there, terms ‚Äúhost memory‚Äù, and ‚Äúdevice memory‚Äù confused me (I haven‚Äôt majored in CS and hence the unfamiliarity with basics).\nA quick lookup did the deal but a similar thing happened as I started reading the PMPP book with a goal to learn parallel programming on GPU ‚Äúdevices‚Äù ;). I was able to get a hold of the code fairly easily (because C++ doesn‚Äôt haunt - I like it more than Python which is what ML Researchers like myself usually use), but the jargons weren‚Äôt entirely clear at once and hence this primer post.\nCore, Microprocessor/Processor, Chip ‚ÄúA chip‚Äù is the physical semiconductor chip; it‚Äôs ‚Äúa physical integrated circuit‚Äù comprised of transistors, resistors, and capacitors.\nA processor (here, think of CPU, the central ‚Äúprocessing‚Äù unit) is a digital circuit that‚Äôs implemented on a single or a few chips. Now, the term micro is appended to the beginning of processors to refer to the fact that it takes a single or a very few chips to implement a microprocessor. But this is more of a definition. In the context/scope of this post, consider 1 microprocessor = 1 chip.\nThe modern day computer is powered by processors of different types - CPUs, GPUs etc. I have also read the term processor chip, the meaning of which should be clear now.\nNow, here‚Äôs the thing: In older days, 1 processor used to mean 1 processing unit (single CPU based microprocessor) which changed around the year 2000 when microprocessors with more than one processing unit/CPU were introduced. Those are what are called as multi-core processors. Hence, a CPU ‚Äúcore‚Äù is basically a single processing unit within a processor chip that is capable of running instructions independently; and hence the modern day microprocessor with several cores is essentially a parallel processor benefitting from software that leverages parallel programming paradigms. Read that again until the terms Core, Microprocessor/Processor, Chip and the distinctions and synonymities between them are clear.\nüëâ If you google intel core i9 processor, the table there has a column # of ‚Äúcores‚Äù.\nClock rate Types of Random Access Memory (RAM) Before proceeding to study the processor architectures, it‚Äôs worth discussing in brief two types of RAM - Static RAM (SRAM) and Dynamic RAM (DRAM).\nSRAM - DRAM - Processor Architectures (Hardware Design) Let us now look into the architectures of the CPU and the GPU, and try to make sense of why the CPU is called a latency device and the GPU a throughput device.\nCPU Let us first look at what a chip with 4 cores looks like:\nImage source A slight but important correction to note here is that while according to the figure above, the DRAM (sometimes simply referred to as RAM or system memory) appears to be located on the chip, it‚Äôs not acutally the case. The DRAM is a separate hardware entity that‚Äôs mounted on the motherboard.\nNext, pay attention to how the chip area is divided among the different components. Note also the multiple levels of cache memories present on the chip (purple and blue) ‚Äì they help to reduce the latency by decreasing the required amounts of high latency memory (DRAM) accesses. Ah, I went too fast here! To further clarify, since\nNow let‚Äôs zoom into a single core:\n// figure\nA few main components are shown in the core above:\nA few very powerful ALUs (Arithmetic Logic Units): A few of them are present on each core and it‚Äôs where the actual computation happens. Each ALU in itself is very powerful and capable of completing a computation in a very few clock cycles; and hence are geared towards the low latency paradigm.\nA Control Unit (CU): A major area is occupied by this component as its two main functions help greatly in reducing the latency - branch prediction and data forwarding. I won‚Äôt elaborate on these two but the takeaway is that each CPU core features a sophisticated CU which again serves the low latency design.\nA big L1 cache: Ofcourse, much smaller than the DRAM, a significant portion on each is dedicated to the L1 cache again to reduce the latency.\nGPU From the same source, here‚Äôs what a GPU chip looks like:\nAs can be seen, the major chip area is now occupied by the green boxes which are the components where the computation takes place (I am refraining from giving a name to the green boxes just yet but yes they are the equivalent of the green ALUs we saw in the CPU). But what‚Äôs also worth noting is that each green box is now much more smaller than 1 single ALU in the CPU core ‚Äì this actually reflects the real scenario that a single of these units on the GPU is much much less powerful than a single ALU and hence has a much longer latency.\nThe L1 caches and the control occupy much lesser chip area.\nInstructions, Threads, Processes Memory Bandwidth Why can‚Äôt CPUs just have a higher memory bandwidth? ","wordCount":"965","inLanguage":"en","datePublished":"2024-03-31T00:00:00Z","dateModified":"2024-03-31T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/blog/cuda-primer/"},"publisher":{"@type":"Organization","name":"Srishti Gureja","logo":{"@type":"ImageObject","url":"http://localhost:1313/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Srishti Gureja (Alt + H)">Srishti Gureja</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/code/ title=code><span>code</span></a></li><li><a href=http://localhost:1313/blog/ title=blog><span>blog</span></a></li><li><a href=http://localhost:1313/talks/ title=talks><span>talks</span></a></li><li><a href=http://localhost:1313/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;¬ª&nbsp;<a href=http://localhost:1313/blog/>Blogs</a></div><h1 class="post-title entry-hint-parent">[WIP] Learning CUDA Programming: A Primer</h1><div class=post-description>A friendly primer to learning CUDA programming</div><div class=post-meta><span title='2024-03-31 00:00:00 +0000 UTC'>March 31, 2024</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#why-are-we-here>Why are we here?</a></li><li><a href=#core-microprocessorprocessor-chip>Core, Microprocessor/Processor, Chip</a></li><li><a href=#clock-rate>Clock rate</a></li><li><a href=#types-of-random-access-memory-ram>Types of Random Access Memory (RAM)</a></li><li><a href=#processor-architectures-hardware-design>Processor Architectures (Hardware Design)</a><ul><li><a href=#cpu>CPU</a></li><li><a href=#gpu>GPU</a></li></ul></li><li><a href=#instructions-threads-processes>Instructions, Threads, Processes</a></li><li><a href=#memory-bandwidth>Memory Bandwidth</a><ul><li><a href=#why-cant-cpus-just-have-a-higher-memory-bandwidth>Why can&rsquo;t CPUs just have a higher memory bandwidth?</a></li></ul></li></ul></nav></div></details></div><div class=post-content><p>I want this post to be an entry point for anyone willing to learn GPU programming without a formal background in compsci/comp arch (I myself belong to this category so sorry in advance for any errors that I might make, and please email me to get them corrected).</p><p>I will cover the architectural details of two of the several processors that empower the modern day computers - the CPUs and the GPUs.
By the end of this post, the reader should have a good understanding of the following terms - (in no particular order) chips, processors, microprocessors, cores, latency device, throughput device, clock speed, threads, processes, instructions, memory bandwidth, memory system.</p><h2 id=why-are-we-here>Why are we here?<a hidden class=anchor aria-hidden=true href=#why-are-we-here>#</a></h2><p>A few months back, I used to visit the PyTorch forums almost daily. When I just began reading over there, terms &ldquo;host memory&rdquo;, and &ldquo;device memory&rdquo; confused me (I haven&rsquo;t majored in CS and hence the unfamiliarity with basics).</p><p>A quick lookup did the deal but a similar thing happened as I started reading the PMPP book with a goal to learn parallel programming on GPU &ldquo;devices&rdquo; ;). I was able to get a hold of the code fairly easily (because C++ doesn&rsquo;t haunt - I like it more than Python which is what ML Researchers like myself usually use), but the jargons weren&rsquo;t entirely clear at once and hence this primer post.</p><h2 id=core-microprocessorprocessor-chip>Core, Microprocessor/Processor, Chip<a hidden class=anchor aria-hidden=true href=#core-microprocessorprocessor-chip>#</a></h2><p>&ldquo;A chip&rdquo; is the physical semiconductor chip; it&rsquo;s &ldquo;a physical integrated circuit&rdquo; comprised of transistors, resistors, and capacitors.</p><p>A processor (here, think of CPU, the central &ldquo;processing&rdquo; unit) is a digital circuit that&rsquo;s implemented on a single or a few chips. Now, the term micro is appended to the beginning of processors to refer to the fact that it takes a single or a very few chips to implement a microprocessor. But this is more of a definition. In the context/scope of this post, consider 1 microprocessor = 1 chip.</p><p>The modern day computer is powered by processors of different types - CPUs, GPUs etc. I have also read the term processor chip, the meaning of which should be clear now.</p><p>Now, here&rsquo;s the thing: In older days, 1 processor used to mean 1 processing unit (single CPU based microprocessor) which changed around the year 2000 when microprocessors with more than one processing unit/CPU were introduced. Those are what are called as multi-core processors. Hence, a CPU &ldquo;core&rdquo; is basically a single processing unit within a processor chip that is capable of running instructions independently; and hence the modern day microprocessor with several cores is essentially a parallel processor benefitting from software that leverages parallel programming paradigms. Read that again until the terms Core, Microprocessor/Processor, Chip and the distinctions and synonymities between them are clear.</p><p>üëâ If you google <a href=https://www.intel.com/content/www/us/en/products/details/processors/core/i9/products.html>intel core i9 processor</a>, the table there has a column # of &ldquo;cores&rdquo;.</p><h2 id=clock-rate>Clock rate<a hidden class=anchor aria-hidden=true href=#clock-rate>#</a></h2><h2 id=types-of-random-access-memory-ram>Types of Random Access Memory (RAM)<a hidden class=anchor aria-hidden=true href=#types-of-random-access-memory-ram>#</a></h2><p>Before proceeding to study the processor architectures, it&rsquo;s worth discussing in brief two types of RAM - Static RAM (SRAM) and Dynamic RAM (DRAM).</p><ol><li><strong>SRAM</strong> -</li><li><strong>DRAM</strong> -</li></ol><h2 id=processor-architectures-hardware-design>Processor Architectures (Hardware Design)<a hidden class=anchor aria-hidden=true href=#processor-architectures-hardware-design>#</a></h2><p>Let us now look into the architectures of the CPU and the GPU, and try to make sense of why the CPU is called a latency device and the GPU a throughput device.</p><h3 id=cpu>CPU<a hidden class=anchor aria-hidden=true href=#cpu>#</a></h3><p>Let us first look at what a chip with 4 cores looks like:</p><p><img loading=lazy src=cpu-chip.png#center alt="A CPU chip">
<em><a href=https://cvw.cac.cornell.edu/gpu-architecture/gpu-characteristics/design>Image source</a></em>
A slight but important correction to note here is that while according to the figure above, the DRAM (sometimes simply referred to as RAM or system memory) appears to be located on the chip, it&rsquo;s not acutally the case. The DRAM is a separate hardware entity that&rsquo;s mounted on the motherboard.</p><p>Next, pay attention to <em>how</em> the chip area is divided among the different components. Note also the multiple levels of cache memories present on the chip (purple and blue) &ndash; they help to reduce the latency by decreasing the required amounts of high latency memory (DRAM) accesses. Ah, I went too fast here! To further clarify, since</p><p>Now let&rsquo;s zoom into a single core:</p><p>// figure</p><p>A few main components are shown in the core above:</p><ol><li><p>A few very powerful ALUs (Arithmetic Logic Units): A few of them are present on each core and it&rsquo;s where the actual computation happens. Each ALU in itself is very powerful and capable of completing a computation in a very few clock cycles; and hence are geared towards the low latency paradigm.</p></li><li><p>A Control Unit (CU): A major area is occupied by this component as its two main functions help greatly in reducing the latency - branch prediction and data forwarding. I won&rsquo;t elaborate on these two but the takeaway is that each CPU core features a sophisticated CU which again serves the low latency design.</p></li><li><p>A big L1 cache: Ofcourse, much smaller than the DRAM, a significant portion on each is dedicated to the L1 cache again to reduce the latency.</p></li></ol><h3 id=gpu>GPU<a hidden class=anchor aria-hidden=true href=#gpu>#</a></h3><p>From the same <a href=https://cvw.cac.cornell.edu/gpu-architecture/gpu-characteristics/design>source</a>, here&rsquo;s what a GPU chip looks like:</p><p><img loading=lazy src=gpu-chip.png#center alt="A GPU chip"></p><p>As can be seen, the major chip area is now occupied by the green boxes which are the components where the computation takes place (I am refraining from giving a name to the green boxes just yet but yes they are the equivalent of the green ALUs we saw in the CPU). But what&rsquo;s also worth noting is that each green box is now much more smaller than 1 single ALU in the CPU core &ndash; this actually reflects the real scenario that a single of these units on the GPU is much much less powerful than a single ALU and hence has a much longer latency.</p><p>The L1 caches and the control occupy much lesser chip area.</p><h2 id=instructions-threads-processes>Instructions, Threads, Processes<a hidden class=anchor aria-hidden=true href=#instructions-threads-processes>#</a></h2><h2 id=memory-bandwidth>Memory Bandwidth<a hidden class=anchor aria-hidden=true href=#memory-bandwidth>#</a></h2><h3 id=why-cant-cpus-just-have-a-higher-memory-bandwidth>Why can&rsquo;t CPUs just have a higher memory bandwidth?<a hidden class=anchor aria-hidden=true href=#why-cant-cpus-just-have-a-higher-memory-bandwidth>#</a></h3></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/cuda-computer-architecture-optimization-gpu-programming-parallel-processing/>CUDA, Computer Architecture, Optimization, GPU Programming, Parallel Processing</a></li></ul><nav class=paginav><a class=prev href=http://localhost:1313/blog/moes/><span class=title>¬´ Prev</span><br><span>Switch Transformer - Sparse Routed Networks/MoEs</span>
</a><a class=next href=http://localhost:1313/blog/dp/><span class=title>Next ¬ª</span><br><span>An Introduction to Differential Privacy</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2024 <a href=http://localhost:1313/>Srishti Gureja</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}}</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js defer></script></body></html>