<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Blogs on Srishti Gureja</title>
    <link>http://localhost:1313/blog/</link>
    <description>Recent content in Blogs on Srishti Gureja</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 10 Jun 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/blog/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Reinforcement Learning</title>
      <link>http://localhost:1313/blog/rl/</link>
      <pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/blog/rl/</guid>
      <description>&lt;h1 id=&#34;prep-formalizing-the-rl-problem&#34;&gt;Prep: Formalizing the RL problem&lt;/h1&gt;
&lt;p&gt;Reinforcement learning is the science of decision making. What does it mean to make decisions? Making decisions requires one to take into account the current circumstances and then take an action(s) with a goal to optimize the long term effects of the decision. RL is no different.&lt;/p&gt;
&lt;p&gt;I think it&amp;rsquo;s already nice to note that we&amp;rsquo;re talking about sequential decision making where the decisions taken today are relevant to the long term outcomes.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Switch Transformer - Sparse Routed Networks/MoEs</title>
      <link>http://localhost:1313/blog/moes/</link>
      <pubDate>Mon, 17 Jun 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/blog/moes/</guid>
      <description>&lt;p&gt;In this short post, I am going to talk about the Switch Transformer paper.&lt;/p&gt;
&lt;h2 id=&#34;background-and-architecture&#34;&gt;Background and Architecture&lt;/h2&gt;
&lt;p&gt;To begin: In Dense Models, each parameter/neuron of the network gets activated during the forward pass.&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;ffnn.png#center&#34; alt=&#34;fully activated dense model&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;Hence, to be able to derive more performance by means of increasing the model size, more computation is required. Implicitly, more model parameters =&amp;gt; more computation performed by a single token.
And tangential to this, work by &lt;a href=&#34;https://arxiv.org/abs/2001.08361&#34;&gt;Kaplan et al. (2020)&lt;/a&gt; becomes super relevant as they discuss training larger models on comparatively smaller amounts of data as the computationally optimal approach.&lt;/p&gt;</description>
    </item>
    <item>
      <title>[WIP] Learning CUDA Programming: A Primer</title>
      <link>http://localhost:1313/blog/cuda-primer/</link>
      <pubDate>Sun, 31 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/blog/cuda-primer/</guid>
      <description>A friendly primer to learning CUDA programming</description>
    </item>
    <item>
      <title>An Introduction to Differential Privacy</title>
      <link>http://localhost:1313/blog/dp/</link>
      <pubDate>Mon, 25 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/blog/dp/</guid>
      <description>A detailed mathematical and intuitive introduction to differential privacy.</description>
    </item>
    <item>
      <title>PyTorch Datasets and Dataloaders</title>
      <link>http://localhost:1313/blog/2023-01-05-pytorch-ds-dl/</link>
      <pubDate>Fri, 05 Aug 2022 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/blog/2023-01-05-pytorch-ds-dl/</guid>
      <description>A beginner&amp;#39;s guide to understanding two important PyTorch classes - Dataset and Dataloader. The new DataPipe functionality is also explored.</description>
    </item>
  </channel>
</rss>
