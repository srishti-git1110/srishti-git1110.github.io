<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>An Introduction to Differential Privacy | Srishti Gureja</title>
<meta name=keywords content="Differential Privacy,PyTorch,Privacy Preserving ML"><meta name=description content="A detailed mathematical and intuitive introduction to differential privacy."><meta name=author content><link rel=canonical href=http://localhost:1313/blog/dp/differential-privacy/><link crossorigin=anonymous href=/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z+V9+cO1A=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/blog/dp/differential-privacy/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="An Introduction to Differential Privacy"><meta property="og:description" content="A detailed mathematical and intuitive introduction to differential privacy."><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/blog/dp/differential-privacy/"><meta property="article:section" content="blog"><meta property="article:published_time" content="2022-08-05T00:00:00+00:00"><meta property="article:modified_time" content="2022-08-05T00:00:00+00:00"><meta property="og:site_name" content="Srishti Gureja"><meta name=twitter:card content="summary"><meta name=twitter:title content="An Introduction to Differential Privacy"><meta name=twitter:description content="A detailed mathematical and intuitive introduction to differential privacy."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blogs","item":"http://localhost:1313/blog/"},{"@type":"ListItem","position":2,"name":"An Introduction to Differential Privacy","item":"http://localhost:1313/blog/dp/differential-privacy/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"An Introduction to Differential Privacy","name":"An Introduction to Differential Privacy","description":"A detailed mathematical and intuitive introduction to differential privacy.","keywords":["Differential Privacy, PyTorch, Privacy Preserving ML"],"articleBody":"I think differential privacy is beautiful!\nWhy are we here?: Protecting the privacy of data is important and not trivial. To help make sense of things here, the Fundamental Law of Information Recovery becomes useful which states: Overly accurate estimates of too many statistics can completely destroy (data) privacy. Another example that provides a good incentive for why privacy is important is the ability of LLMs to memorize data which is an undesirable outcome as it risks the leak of PII.\nBefore discussing DP, I’d like to spend a few words on giving some intuition behind The Privacy vs Utility Tradeoff: Whenever we make some use of the data at hand and hence, learn something useful from it, we lose out on some privacy. Elaboratively, let’s say we begin by learning only one statistic from a collection of datapoints at hand, even then we lose out on some privacy as far as the individual data points are concerned. This privacy loss keeps amplifying as we keep on learning more useful information from the data, and importantly, this is inevitable.\nConversely, to be able to maintain full privacy of the data, we will have to give up on learning “anything” useful from it. Total privacy = No learning.\nDifferential Privacy: A High Level Intuition First of all, what even is privacy in a “non-subjective sense”?\n[Why non-subjective?: To me, getting the data on handedness leaked is not a breach of my privacy. I don’t mind it but someone else might consider it as a privacy violation if their handedness is to get leaked.]\nDP answers this question in a concrete mathematical equation, but let’s first get the intuition right. Consider an attacker who is interested in knowing my handedness and somehow gets access to the following information:\nI, along with 999 other individuals are participating in a survey whose end result shall be percentage statistics of people belonging to both kinds of handedness. The attacker is very strong and somehow was also able to get their hands on the handedness of the other 999 people. Voila! Let the survey results get published and the attacker will be able to learn my handedness with 100% accuracy (I am ambidexterous btw and I don’t mind sharing it but you get the gist :)).\nHow can we prevent this?: Keeping the above example in mind, consider a Mechanism M (the survey) and a dataset D. The Mechanism M processes D and produces some output O. By virtue of M, it’s possible for the attacker to look at O and recover my info using it. What differential privacy does is that it adds some form of noise (or randomness) to M as a result of which the attacker is no more able to make such deductions using the output O that’s a result of the Differentially Private Mechanism M. I mean they are free to make such deductions ofcourse, but those will not be as accurate as we discussed above and it is all by virtue of the noise we add to M.\n[What this noise is, where and how it’s added and how it works to provide mathematical privacy guarantees is something I’ll circle back to in a minute.]\nElaborating on it: If M is differentially private, then the output we will get out of M in case I am right handed and the output we get out of M in case I am left, are “similar”.\nI want to define here concretely what “similar” actually means: It means that we can get the exact same output out of M for both these cases with a “similar probability”. And hence, the attacker can never be 100% sure of my handedness and so the Mechanism M protects my privacy. A key thing to note here is that it is the Mechanism M, and not the output O, that is differentially private.\nThere are two very intuitive guarantees that need some clarification here:\nGoal of the attacker - DP is capable of protecting all kinds of info. Say, if there’s a different setup wherein there are two datasets D1 and D2 of which I am present in only one, and the goal of the attacker is to identify the dataset which I am a part of. DP protects my privacy in this and many other kinds of potential attacks, too. Strength of the Attacker - The attacker that we considered above was quite strong (so much so that the two cases we considered differ only in one data point which is mine), and even then we were able to protect my data using DP. The point here is that DP protects privacy no matter how strong the attacker is, what they know and whatever their capabilities are. I hope this intuition was helpful in building some notion of what DP is and how it defines privacy.\nThe Math I will now write in an equation what we discussed above in words. This equation is actually the standard definition of differential privacy and the goal of the next few paragraphs is to decode the formal definition of DP and understand how that relates to the intuitive understanding of privacy that we got above. If the equation does not make 100% sense at the first read, please hold on and read and re-read more.\nDefinition Consider a mechanism M, and two datasets D1 and D2 that differ only in one single datapoints. The mechanism M is ε-differentially private if for all such datasets D1 and D2, the following holds:\n$$ \\mathbb{P}\\left[M(D_1)=O\\right] \\le e^\\varepsilon\\cdot\\mathbb{P}\\left[M(D_2)=O\\right] , ε\u003e0 $$\nand, it holds for all possible output values of M.\nIt should go without saying that if I swap the places of D1 and D2, the equation will still hold in which case it becomes: \\mathbb{P}\\left[M(D_2)=O\\right] \\le e^\\varepsilon\\cdot\\mathbb{P}\\left[M(D_1)=O\\right]\nWith some easy math on these two equations, here’s what we get:\nNow, let’s reiterate: The mechanism M is ε-differentially private if for all such datasets D1 and D2 that differ in only one data point, the following holds for all possible outputs O:\nThis equation is exactly what we dicussed above: If M is differentially private, then the (ratio of the) output we get out of M in case I am right handed and the output we get out of M in case I am left are “similar”, and hence we are protecting the privacy here. In essence, the probability of obtaining O as the output of M(D1) does not differ too much from the probability of obtaining O as the output of M(D2) (that’s what the bounds above mean) thus making it hard for the attacker to make deductions.\n[BTW, if it isn’t clear why we’re talking terms of probability here: Remember we add some random noise to M rather than returning its true output. This makes it a probabilistic mechanism rather than a deterministic one.]\nAnd ofcourse, it follows by basic math that higher the value of ε, lesser is the privacy guarantee that we get and vice versa. What’s also nice to note here is that now that we have formalized DP by means of the parameter ε, we can easily compare the privacy offered by two mechanisms M1 and M2 and say things like M1 is more private than M2 etc.\nConnect everything We have discussed the mathematical formalization and the intuitive understanding of DP but it is all still very abstract. To get a crystal clear picture, let us circle back to my handedness example.\nIn numbers, say,\nTotal no. of individuals in the survey = 999 + 1 (I) = 1000 individuals The attacker knows the following: 500 are right handed and 499 are left handed The statistic to be released is the number of left handed individuals (for simplicity and without any loss of generality, I am taking the number rather than the proportion here). Also, let D1 = I am left handed and D2 = I am right handed and say, the ground truth is D1 which the attacker obviously does not know and which is what they intend to deduce/know by means of the survey statistic.\nDP in the picture: Now the mechanism M here simply counts the number of left handed and right handed individuals with a goal to release the said statistic. And to be able to protect my privacy, we also want to make M ε-differentially private.\nHow do we do that?: As part of M, we need to add some noise to the counts before releasing them. The mathematical selection of this noise is the key and it should be such that the mechanism M becomes ε-differentially private. Without any hints and long talk, I’ll tell you that we sample this noise from the Double Exponential Distribution with a scale parameter 1/ε and it does what we are looking for - this noise is capable of providing the guarantees that we promised while calling M to be ε-differentially private. Here’s how the Double Exponential Distribution looks like:\nLet’s work it out! Without any noise, the output O of M = 500. Let’s say we sample some noise X as told above and it comes out to be X = -1, and hence the the output O of ε-differentially private M that we also publish becomes 500 - 1 = 499. Now ofcourse, using 499 as the released statistic on the number of left handed individuals, the attacker cannot conclude that I am right handed. Same goes for any value of noise X.\nLet’s see if our privacy guarantees are satisfied with X=-1. Before showing it mathematically, it might help to make sense of the graph below and realising that the X-axis shows the several values of O that can be published under DP, and that at each such X, the probabilities of D1 and D2 being the truth (shown by the blue and red graphs) are similar.\nClearly, mathbb{P}\\left[M(D_1)=O\\right] = mathbb{P}\\left[X=-1\\right] (If I am left handed then the true O is 500 and I need to add a noise of -1 to be able to publish 499).\nSimilarly, mathbb{P}\\left[M(D_2)=O\\right] = mathbb{P}\\left[X=0\\right] (If I am left handed then the true O is 499 and I need to add a noise of 0 to be able topublish 499).\nAnd there we go: // ratio = mathbb{P}\\left[X=-1\\right] / mathbb{P}\\left[X=0\\right] = e^{-\\varepsilon} (This simply follows from the pdf of Double Exponential Distribution with a scale parameter 1/ε so I will not show that math here).\nThis clearly shows that using the noise that we did, our mechanism M is clearly ε-differentially private.\nCheers! If you were able to follow this far, you’ve already designed your first own ε-differentially private algorithm. :)\n","wordCount":"1777","inLanguage":"en","datePublished":"2022-08-05T00:00:00Z","dateModified":"2022-08-05T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/blog/dp/differential-privacy/"},"publisher":{"@type":"Organization","name":"Srishti Gureja","logo":{"@type":"ImageObject","url":"http://localhost:1313/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Srishti Gureja (Alt + H)">Srishti Gureja</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/about/ title=about><span>about</span></a></li><li><a href=http://localhost:1313/projects/ title=projects><span>projects</span></a></li><li><a href=http://localhost:1313/blog/ title=blog><span>blog</span></a></li><li><a href=http://localhost:1313/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/blog/>Blogs</a></div><h1 class="post-title entry-hint-parent">An Introduction to Differential Privacy</h1><div class=post-description>A detailed mathematical and intuitive introduction to differential privacy.</div><div class=post-meta><span title='2022-08-05 00:00:00 +0000 UTC'>August 5, 2022</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#differential-privacy-a-high-level-intuition>Differential Privacy: A High Level Intuition</a></li><li><a href=#the-math>The Math</a><ul><li><a href=#definition>Definition</a></li><li><a href=#connect-everything>Connect everything</a></li></ul></li></ul></nav></div></details></div><div class=post-content><p>I think differential privacy is beautiful!</p><p><strong>Why are we here?</strong>: Protecting the privacy of data is important and not trivial. To help make sense of things here, the Fundamental Law of Information Recovery becomes useful which states: Overly accurate estimates of too many statistics can completely destroy (data) privacy. Another example that provides a good incentive for why privacy is important is the ability of <a href=https://blog.research.google/2020/12/privacy-considerations-in-large.html>LLMs to memorize data</a> which is an undesirable outcome as it risks the leak of PII.</p><p>Before discussing DP, I&rsquo;d like to spend a few words on giving some intuition behind The Privacy vs Utility Tradeoff: Whenever we make some use of the data at hand and hence, learn something useful from it, we lose out on some privacy. Elaboratively, let&rsquo;s say we begin by learning only one statistic from a collection of datapoints at hand, even then we lose out on some privacy as far as the individual data points are concerned. This privacy loss keeps amplifying as we keep on learning more useful information from the data, and importantly, this is inevitable.</p><p>Conversely, to be able to maintain full privacy of the data, we will have to give up on learning &ldquo;anything&rdquo; useful from it.
Total privacy = No learning.</p><h2 id=differential-privacy-a-high-level-intuition>Differential Privacy: A High Level Intuition<a hidden class=anchor aria-hidden=true href=#differential-privacy-a-high-level-intuition>#</a></h2><p>First of all, what even is privacy in a &ldquo;non-subjective sense&rdquo;?</p><p><em>[Why non-subjective?: To me, getting the data on handedness leaked is not a breach of my privacy. I don&rsquo;t mind it but someone else might consider it as a privacy violation if their handedness is to get leaked.]</em></p><p>DP answers this question in a concrete mathematical equation, but let&rsquo;s first get the intuition right.
Consider an attacker who is interested in knowing my handedness and somehow gets access to the following information:</p><ul><li>I, along with 999 other individuals are participating in a survey whose end result shall be percentage statistics of people belonging to both kinds of handedness.</li><li>The attacker is very strong and somehow was also able to get their hands on the handedness of the other 999 people.</li></ul><p>Voila! Let the survey results get published and the attacker will be able to learn my handedness with 100% accuracy (I am ambidexterous btw and I don&rsquo;t mind sharing it but you get the gist :)).</p><p><strong>How can we prevent this?</strong>: Keeping the above example in mind, consider a Mechanism M (the survey) and a dataset D. The Mechanism M processes D and produces some output O. By virtue of M, it&rsquo;s possible for the attacker to look at O and recover my info using it. What differential privacy does is that it adds some form of noise (or randomness) to M as a result of which the attacker is no more able to make such deductions using the output O that&rsquo;s a result of the Differentially Private Mechanism M. I mean they are free to make such deductions ofcourse, but those will not be as accurate as we discussed above and it is all by virtue of the noise we add to M.</p><p><em>[What this noise is, where and how it&rsquo;s added and how it works to provide mathematical privacy guarantees is something I&rsquo;ll circle back to in a minute.]</em></p><p><strong>Elaborating on it</strong>: If M is differentially private, then the output we will get out of M in case I am right handed and the output we get out of M in case I am left, are &ldquo;similar&rdquo;.</p><p>I want to define here concretely what &ldquo;similar&rdquo; actually means: It means that we can get the <strong>exact same output</strong> out of M for both these cases with a &ldquo;similar probability&rdquo;. And hence, the attacker can never be 100% sure of my handedness and so the Mechanism M protects my privacy.
A key thing to note here is that it is the Mechanism M, and not the output O, that is differentially private.</p><p>There are two very intuitive guarantees that need some clarification here:</p><ol><li>Goal of the attacker - DP is capable of protecting all kinds of info. Say, if there&rsquo;s a different setup wherein there are two datasets D1 and D2 of which I am present in only one, and the goal of the attacker is to identify the dataset which I am a part of. DP protects my privacy in this and many other kinds of potential attacks, too.</li><li>Strength of the Attacker - The attacker that we considered above was quite strong (so much so that the two cases we considered differ only in one data point which is mine), and even then we were able to protect my data using DP. The point here is that DP protects privacy no matter how strong the attacker is, what they know and whatever their capabilities are.</li></ol><p>I hope this intuition was helpful in building some notion of what DP is and how it defines privacy.</p><h2 id=the-math>The Math<a hidden class=anchor aria-hidden=true href=#the-math>#</a></h2><p>I will now write in an equation what we discussed above in words. This equation is actually the standard definition of differential privacy and the goal of the next few paragraphs is to <strong>decode the formal definition of DP and understand how that relates to the intuitive understanding of privacy that we got above</strong>. If the equation does not make 100% sense at the first read, please hold on and read and re-read more.</p><h3 id=definition>Definition<a hidden class=anchor aria-hidden=true href=#definition>#</a></h3><p>Consider a mechanism M, and two datasets D1 and D2 that differ only in one single datapoints. The mechanism M is ε-differentially private if for all such datasets D1 and D2, the following holds:</p><p>$$ \mathbb{P}\left[M(D_1)=O\right] \le e^\varepsilon\cdot\mathbb{P}\left[M(D_2)=O\right] , ε>0 $$</p><p>and, it holds for all possible output values of M.</p><p>It should go without saying that if I swap the places of D1 and D2, the equation will still hold in which case it becomes:
\mathbb{P}\left[M(D_2)=O\right] \le e^\varepsilon\cdot\mathbb{P}\left[M(D_1)=O\right]</p><p>With some easy math on these two equations, here&rsquo;s what we get:</p><p>Now, let&rsquo;s reiterate:
The mechanism M is ε-differentially private if for all such datasets D1 and D2 that differ in only one data point, the following holds for all possible outputs O:</p><p>This equation is exactly what we dicussed above: If M is differentially private, then the (ratio of the) output we get out of M in case I am right handed and the output we get out of M in case I am left are &ldquo;similar&rdquo;, and hence we are protecting the privacy here. In essence, the probability of obtaining O as the output of M(D1) does not differ too much from the probability of obtaining O as the output of M(D2) (that&rsquo;s what the bounds above mean) thus making it hard for the attacker to make deductions.</p><p><em>[BTW, if it isn&rsquo;t clear why we&rsquo;re talking terms of probability here: Remember we add some random noise to M rather than returning its true output. This makes it a probabilistic mechanism rather than a deterministic one.]</em></p><p>And ofcourse, it follows by basic math that higher the value of ε, lesser is the privacy guarantee that we get and vice versa. What&rsquo;s also nice to note here is that now that we have formalized DP by means of the parameter ε, we can easily compare the privacy offered by two mechanisms M1 and M2 and say things like M1 is more private than M2 etc.</p><h3 id=connect-everything>Connect everything<a hidden class=anchor aria-hidden=true href=#connect-everything>#</a></h3><p>We have discussed the mathematical formalization and the intuitive understanding of DP but it is all still very abstract. To get a crystal clear picture, let us circle back to my handedness example.</p><p>In numbers, say,</p><ul><li>Total no. of individuals in the survey = 999 + 1 (I) = 1000 individuals
The attacker knows the following:</li><li>500 are right handed and 499 are left handed</li><li>The statistic to be released is the number of left handed individuals (for simplicity and without any loss of generality, I am taking the number rather than the proportion here).</li></ul><p>Also, let D1 = I am left handed and D2 = I am right handed and say, the ground truth is D1 which the attacker obviously does not know and which is what they intend to deduce/know by means of the survey statistic.</p><p><strong>DP in the picture:</strong>
Now the mechanism M here simply counts the number of left handed and right handed individuals with a goal to release the said statistic. And to be able to protect my privacy, we also want to make M ε-differentially private.</p><p>How do we do that?: As part of M, we need to add some noise to the counts before releasing them. The mathematical selection of this noise is the key and it should be such that the mechanism M becomes ε-differentially private. Without any hints and long talk, I&rsquo;ll tell you that we sample this noise from the Double Exponential Distribution with a scale parameter 1/ε and it does what we are looking for - this noise is capable of providing the guarantees that we promised while calling M to be ε-differentially private. Here&rsquo;s how the Double Exponential Distribution looks like:</p><p><img loading=lazy src=../images/laplace.png alt="Laplace/Double Exponential Distribution distribution centered at 0 with a scale of 1"></p><p>Let&rsquo;s work it out!
Without any noise, the output O of M = 500.
Let&rsquo;s say we sample some noise X as told above and it comes out to be X = -1, and hence the the output O of ε-differentially private M that we also publish becomes 500 - 1 = 499. Now ofcourse, using 499 as the released statistic on the number of left handed individuals, the attacker cannot conclude that I am right handed.
Same goes for any value of noise X.</p><p>Let&rsquo;s see if our privacy guarantees are satisfied with X=-1.
Before showing it mathematically, it might help to make sense of the graph below and realising that the X-axis shows the several values of O that can be published under DP, and that at each such X, the probabilities of D1 and D2 being the truth (shown by the blue and red graphs) are similar.</p><p><img loading=lazy src=../images/lap_compare.png alt="the probabilities of D1 and D2 being the truth (shown by the blue and red graphs) are similar."></p><p>Clearly, mathbb{P}\left[M(D_1)=O\right] = mathbb{P}\left[X=-1\right] (If I am left handed then the true O is 500 and I need to add a noise of -1 to be able to publish 499).</p><p>Similarly, mathbb{P}\left[M(D_2)=O\right] = mathbb{P}\left[X=0\right] (If I am left handed then the true O is 499 and I need to add a noise of 0 to be able topublish 499).</p><p>And there we go:
// ratio = mathbb{P}\left[X=-1\right] / mathbb{P}\left[X=0\right] = e^{-\varepsilon}
(This simply follows from the pdf of Double Exponential Distribution with a scale parameter 1/ε so I will not show that math here).</p><p>This clearly shows that using the noise that we did, our mechanism M is clearly ε-differentially private.</p><p>Cheers! If you were able to follow this far, you&rsquo;ve already designed your first own ε-differentially private algorithm. :)</p></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/differential-privacy-pytorch-privacy-preserving-ml/>Differential Privacy, PyTorch, Privacy Preserving ML</a></li></ul><nav class=paginav><a class=next href=http://localhost:1313/blog/2023-01-05-pytorch-ds-dl/><span class=title>Next »</span><br><span>PyTorch Datasets and Dataloaders</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2024 <a href=http://localhost:1313/>Srishti Gureja</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>