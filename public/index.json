[{"content":"In this short post, I am going to talk about the Switch Transformer paper.\nBackground and Architecture To begin: In Dense Models, each parameter/neuron of the network gets activated during the forward pass.\nHence, to be able to derive more performance by means of increasing the model size, more computation is required. Implicitly, more model parameters =\u0026gt; more computation performed by a single token. And tangential to this, work by Kaplan et al. (2020) becomes super relevant as they discuss training larger models on comparatively smaller amounts of data as the computationally optimal approach.\nSo, the point is that while increasing the number of model parameters yields performance, it comes at the cost of increased total computation/FLOPs per token (There\u0026rsquo;s nuance to the former part of this statement but it communicates the gist).\nSuch is not the case with Routed Models as we\u0026rsquo;ll see. Let\u0026rsquo;s first look at what a \u0026ldquo;Routed Layer\u0026rdquo; looks like. The above figure might seem intimidating but we\u0026rsquo;ll understand all of it. The single feed forward layer is replaced by several different FF layers (FF1 through FF4 in the figure) each of which is called an Expert. Each token is routed to only one of the experts during a single forward pass. The Router (in green) is yet another FFNN that decides which expert should each token be routed to. And to make it clear, the same router is used for all tokens.\nLet\u0026rsquo;s look at some math Consider,\n\\( \\mathcal{B} \\): the current batch \\( T \\): number of tokens in the current batch \\( N \\): the number of experts in the current routed layer \\( E_i \\): the expert \\( i \\) \\( p_i(x) \\): the probability that the token \\( x \\) is routed to \\( E_i \\) The Router, as discussed, is a FFNN that takes \\( x \\) as an input and first produces logits given by: $$ h(x) = W_r . x $$\nwhere,\n\\( W_r \\): The router weights\nTalk of probability in FFNNs and you have softmax; hence the router probability for a particular expert \\( E_i \\) is calculated by normalizing the logits as:\n$$ p_i(x) = \\frac{e^{h(x)_i}}{\\sum_{j=1}^{N} e^{h(x)_j}} $$\nFrom here, as can be seen in the figure above, the Switch paper routes \\( x \\) to the expert with the highest \\( p_i(x) \\).\nAnd thereupon the output corresponding to token \\( x \\) is calculated as:\n$$ y = p_i(x)E_i(x); i \\in {1, 2, \u0026hellip;, N} $$\nThis is what the dotted line in the figure shows.\nTo revise: we went from a layer with a single feed forward network to what the paper calls a \u0026ldquo;Switch layer\u0026rdquo;.\nWhat happened here? The architectural change was straight forward to understand: you replace the single FF network with a bunch of different FF networks, of which one token uses only one at a time. What this means is that while the total parameters in the layer increase this way by \\( N \\) times, the total computation performed by each token remained constant.\nAnd so here we go: We are able to scale on the number-of-parameters axis while keeping the FLOPs per token constant. All of this by virtue of the routing mechanism that introduces \u0026ldquo;sparsity\u0026rdquo; in the model \u0026ndash; not all parameters of the model are activated during a token\u0026rsquo;s forward pass.\nRevising the Switch Layer: MoE vs Switch Transformer As we saw above, one token is routed to only one expert at a time in the Switch Transformer paper.\nThe Outrageously Large Neural Networks paper (Shazeer et al. 2017) does this differently. Instead of routing a token to only one expert, they select the top-k experts for a token, in which case the output of the MoE layer (they call it that) for that token becomes:\n$$ y = \\sum_{i \\in \\mathcal{T}} p_i(x)E_i(x) $$\nwhere,\n\\( \\mathcal{T} \\): set of top-k Experts\nWhy top-k? I read the OLNN paper carefully but did not see any elaborate mentions of why \\( k\u0026gt;1 \\) experts are required. But what the paper nicely elaborates on is \u0026ldquo;load-balancing\u0026rdquo;. We\u0026rsquo;ll circle back to load-balancing in context of the Switch paper but the term essentially means to ensure that all experts are sufficiently utilized and hence trained during the training process. In other words, the router shouldn\u0026rsquo;t always favour a certain (set of) expert(s).\nAnd from here, one can intuition about why the authors chose \\( k\u0026gt;1 \\) (precisely they use \\( k=4 \\)) \u0026ndash; to make sure that not just one but more than one expert is involved during any single backprop for the router to be able learn the routing process without favouring a single (set of) expert(s).\nThat said, if \\( k\u0026gt;1 \\) is one aspect of load-balancing along with playing a part in making sure all experts are sufficiently trained, how were the Switch authors able to do well without it?\nAuxiliary loss This section is the answer to that. The aux loss is just a term that\u0026rsquo;s added to the total model loss. The purpose of it is to penalize the router for favouring a certain expert too much, i.e. for giving too large a probability to any one expert. Hence, this loss encourages a \u0026ldquo;uniform routing\u0026rdquo;. The word \u0026ldquo;uniform\u0026rdquo; here is more than an english word; it\u0026rsquo;s used in the statistical sense of the Uniform Distribution and logically so. Because what we want is for the tokens to be distributed uniformly among the experts.\nAnd we will see how exactly does this loss encourages a Uniform Distribution. But let\u0026rsquo;s see the math once.\nKeeping the previous notations same, consider:\n\\( f_i \\): the fraction of tokens routed to \\( E_i \\) Hence,\n\\( f_i \\) = Number of tokens from \\( \\mathcal{B} \\) router to \\( E_i \\) / Total tokens in \\( \\mathcal{B} \\)\n$$ f_i = \\frac{1}{T} \\sum_{x \\in \\mathcal{B}} \\mathbb{1} \\{argmax \\ p(x) = i \\} $$\n\\( P_i \\): fraction of the router probability assigned to \\( E_i \\) across all tokens in \\( \\mathcal{B} \\) Hence,\n\\( P_i \\) = Sum of \\( p_i(x) \\) across \\( \\mathcal{B} \\) / Total router probability across \\( \\mathcal{B} \\)\n$$ P_i = \\frac{1}{T} \\sum_{x \\in \\mathcal{B}} p_i(x) $$\nThe aux loss is now given by:\n$$ loss = \\alpha . N . \\sum_{i=1}^{N} f_i . P_i $$\nwhere,\n\\( \\alpha \\): coefficient to control the effect of aux losses It needs to be large enough for the aux losses to have a sufficient load-balancing effect, and small enough to not overwhelm the primary modelling loss.\nNow, under Uniform routing (which is what we want), each expert should get \\( \\frac{T}{N} \\) tokens routed to it. And for every particular \\( x \\), \\( p_i(x) \\) should be \\( \\frac{1}{N} \\) for each value of \\( i \\) i.e. for each expert.\nHence, under uniform routing:\n$$ f_i = \\frac{1}{T} x \\frac{T}{N} = \\frac{1}{N} $$ $$ P_i = \\frac{1}{T} x \\sum_{x \\in \\mathcal{B}} \\frac{1}{N} = \\frac{1}{T} x \\frac{T}{N} = \\frac{1}{N} $$\nAnd these are the exact values at which the aux loss as described above is minimized, and hence it encourages uniform routing!\n[Notice the term \\( N \\) multiplied in the aux loss equation? Plug in these uniform values and try to reason why it\u0026rsquo;s necessary.]\nObviously, there\u0026rsquo;s more than a single switch layer in the full architecture, and we want each one of it to route uniformly. So, aux losses corresponding to each switch layer are calculated and added to the language model training loss.\nBenefits of \\( k=1 \\) (The Switch Layer) Choosing \\( k=1 \\) over \\( k\u0026gt;1 \\) results in some obvious benefits:\nLess computation needs to be performed per token. Cross device communication is reduced owing to the needlessness to perform an addition operation to be able to get the token output. Efficient routing: Expert Capacity The Expert Capacity of each expert which is basically the max batch size allocated to each expert. It goes:\n$$ \\text{expert capacity} = \\left( \\frac{\\text{tokens per batch}}{\\text{number of experts}} \\right) \\times \\text{capacity factor} $$\nAs is clear, the capacity factor is introduced in the above equation to create an additional buffer for cases when there\u0026rsquo;s an unequal allocation of tokens among the experts by the router. If the router allocated count exceeds the Expert Capacity, the extra tokens are just dropped from the expert computation (and make their way to the next layer via the residual connection).\nNow a natural question is that why is this additional constraint even required and why can\u0026rsquo;t we just have the router route however it wants with the flexibility to increase the Expert Capacity indefinitely? At once, load balancing might come up as one reason but if thought carefully, Expert Capacity doesn\u0026rsquo;t really do anything in that sense. As in it\u0026rsquo;s not encouraging balancing in the sense of causing other experts to be equally utilized simply because the extra tokens are just dropped and not like re-routed to some other expert(s).\nThe reason why Expert Capacity is needed is to make sure that resources (computation and memory) are not wasted while some expert operates at its full capacity and others don\u0026rsquo;t; and obviously this is also the reason why we don\u0026rsquo;t want to have too large of an expert capacity. The goal is to strike a good balance so that not many tokens are dropped (the authors empirically prove that a lower drop rate is important to scale these models) and not a lot of resources are wasted.\n[I realise that this reasoning might still remain a bit unclear or atleast too abstract to grasp to those that don\u0026rsquo;t have a clear understanding of the GPU arch, resource management, memory allocation on the devices etc. The insight here is that if the expert capacity of each expert is set to very high, it will be a waste for those experts that do not process as many tokens. GPUs are expensive and we don\u0026rsquo;t want resources to be idle.]\nScaling Laws There are a couple interesting plots in the paper showing scaling on the number-of-experts axis. Note that increasing the number of experts does not increase the FLOPs per token and hence the models remain FLOPs matched \u0026ndash; the computation budget per token remains constant.\nMore experts = Better test loss The plot below clearly shows that as the number of experts increase the test loss decreases. Ofcourse, everything else (#tokens, #training steps etc.) must be kept constant. More experts = Fast training Firstly, fast here does not mean more examples processed per second. Fast implies what can be clearly seen in the plot below: Having more experts achieves the same level of negative log perplexity faster than a model with lesser experts \u0026ndash; faster in terms of the number of training steps performed. Additionally, the authors also find for a fixed number of tokens seen, larger models learn faster.\nWhat needs to be carefully noted here is that while the total computation performed per token (FLOPs per token) is constant across models with varying number of experts, a relatively very small computation difference is there by virtue of the router having to compute as many probailities as there are the number of experts; but this is minor. Consequently, we also note that there\u0026rsquo;s no router calculations at all in dense models. And additionally, Switch transformer also needs to do some extra communication across devices by virtue of the different experts being on different devices.\nThis leads us to an important point: Being faster on the numnber-of-training steps axis does not necessarily mean being faster on the time-taken-by-the-wall-clock axis. Again, the premise here is that the Switch Transformer, owing to the extra computation and communication that it needs to do, might complete a certain number of training steps slower than its dense counterpart. This means that while we expect the Switch Transformer to be better performing after completing the same number of training steps as the dense model, it can still take more time for the former to actually reach that point.\nSo the next obvious question is: Given a fixed amount of wall clock time (and ofcourse the computation budget), how do these models compare in performance with each other?\nFixed training time The plot below answers the question posed just above - For a fixed training duration, Switch transformer outperforms the dense model. Same story for the number of experts as they grow, but here the difference isn\u0026rsquo;t as significant and the intuition is pretty easy to reach at if the past few paras were clear enough. Moving forward to FLOPs un-matched If we go back to the motivation behind using a MoE or a sparse routed model like the Switch Transformer, it\u0026rsquo;s simply stated as: MoEs allow us to increase the parameter count while keeping the computational budget constant. While the benefit of training an MoE/Switch model over its FLOPs matched baseline is clear by now, how does a Switch model compare with a more-FLOPs-demanding dense baseline? The plot below shows that Switch-Base (124B FLOPs/seq) is still more sample efficient than dense T5-Large (425B FLOPs/seq). This is cool!\nLet\u0026rsquo;s \u0026ldquo;infer\u0026rdquo; the tradeoff! Now, let\u0026rsquo;s \u0026ldquo;infer\u0026rdquo; a tradeoff from whatever we\u0026rsquo;ve studied till now. I am using those quotes because the tradeoff is regarding the Inference stage. :-)\nThe obvious reason being that all the experts need to be loaded As we\u0026rsquo;ve seen, with the same amount of computational budget, MoE/Switch performs better than the FLOPs matched dense baseline owing to ofcourse, the greater number of parameters that it has. What this also means for the inference stage is that the former requires more memory than the latter in exchange of the performance gains that it offers. The obvious reason being that all the experts need to be loaded in the memory.\nHence, it\u0026rsquo;s a memory vs performance tradeoff.\nReferences [1] Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity\n[2] Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer\n","permalink":"http://localhost:1313/blog/moes/","summary":"In this short post, I am going to talk about the Switch Transformer paper.\nBackground and Architecture To begin: In Dense Models, each parameter/neuron of the network gets activated during the forward pass.\nHence, to be able to derive more performance by means of increasing the model size, more computation is required. Implicitly, more model parameters =\u0026gt; more computation performed by a single token. And tangential to this, work by Kaplan et al.","title":"Switch Transformer - Sparse Routed Networks/MoEs"},{"content":"I think differential privacy is beautiful!\nWhy are we here? Protecting the privacy of data is important and not trivial. To help make sense of things here, the Fundamental Law of Information Recovery becomes useful which states: Overly accurate estimates of too many statistics can completely destroy (data) privacy. Another example that provides a good incentive for why privacy is important is the ability of LLMs to memorize data which is an undesirable outcome as it risks the leak of PII.\nBefore discussing DP, I\u0026rsquo;d like to spend a few words on giving some intuition behind The Privacy vs Utility Tradeoff:\nWhenever we make some use of the data at hand and hence, learn something useful from it, we lose out on some privacy. Elaboratively, let\u0026rsquo;s say we begin by learning only one statistic from a collection of datapoints at hand, even then we lose out on some privacy as far as the individual data points are concerned. This privacy loss keeps amplifying as we keep on learning more useful information from the data, and importantly, this is inevitable.\nConversely, to be able to maintain full privacy of the data, we will have to give up on learning \u0026ldquo;anything\u0026rdquo; useful from it. Total privacy = No learning.\nDifferential Privacy: A High Level Intuition First of all, what even is privacy in a \u0026ldquo;non-subjective sense\u0026rdquo;?\n[Why non-subjective?: To me, getting the data on my handedness leaked is not a breach of my privacy. I don\u0026rsquo;t mind it but someone else might consider it as a privacy violation if their handedness is to get leaked.]\nDP answers this question in a concrete mathematical equation, but let\u0026rsquo;s first get the intuition right. Consider an attacker who is interested in knowing my handedness and somehow gets access to the following information:\nI, along with 999 other individuals are participating in a survey whose end result shall be the percentage statistics of people belonging to the left-handed category. The attacker is very strong and somehow was also able to get their hands on the handedness of the other 999 people. Voila! Let the survey results get published and the attacker will be able to learn my handedness with 100% accuracy (I am ambidexterous btw and I don\u0026rsquo;t mind sharing it but you get the gist :)).\nHow can we prevent this?:\nKeeping the above example in mind, consider a Mechanism \\( M \\) (the survey) and a dataset \\( D \\). The Mechanism \\( M \\) processes \\( D \\) and produces some output \\( O \\). By virtue of \\( M \\), it\u0026rsquo;s possible for the attacker to look at \\( O \\) and recover my info using it. What differential privacy does is that it adds some form of noise (or randomness) to \\( M \\) as a result of which the attacker is no more able to make such deductions using the output \\( O \\) that\u0026rsquo;s a result of the \u0026ldquo;Differentially Private Mechanism \\( M \\)\u0026rdquo;. I mean they are free to make such deductions ofcourse, but those will not be as accurate as we discussed above and it is all by virtue of the noise we add to \\( M \\).\n[What this noise is, where and how it\u0026rsquo;s added and how it works to provide mathematical privacy guarantees is something I\u0026rsquo;ll circle back to in a minute.]\nElaborating on it:\nIf \\( M \\) is differentially private, then the output we will get out of \\( M \\) in case I am right handed and the output we get out of \\( M \\) in case I am left, are \u0026ldquo;similar\u0026rdquo;.\nI want to define here concretely what \u0026ldquo;similar\u0026rdquo; actually means: It means that we can get the exact same output out of \\( M \\) in both these cases with a \u0026ldquo;similar probability\u0026rdquo;. And hence, the attacker can never be 100% sure of my handedness and so the Mechanism \\( M \\) protects my privacy. A key thing to note here is that it is the Mechanism \\( M \\), and not the output \\( O \\), that is differentially private.\nDP is strong! Before diving in the Math, there are two very interesting properties that need some clarification here:\nGoal of the attacker - DP is capable of protecting all kinds of info. Say, if there\u0026rsquo;s a different setup wherein there are two datasets \\( D1 \\) and \\( D2 \\) between which I am present in only one, and the goal of the attacker is to identify the dataset which I am a part of. DP protects my privacy against this and many other kinds of potential attacks.\nStrength of the Attacker - The attacker that we considered above was quite strong (so much so that the two cases we considered differ only in one data point which is mine), and even then we were able to protect my data using DP. The point here is that DP protects privacy no matter how strong the attacker is, what they know and whatever their capabilities are.\nI hope this section was helpful in building some notion of what DP is and how it defines and protects privacy.\nThe Math I will now write in an equation what we discussed above in words. This equation is actually the standard definition of differential privacy and the goal of the next few paragraphs is to decode the formal definition of DP and understand how that relates to the intuitive understanding of privacy that we got above. If the equation does not make 100% sense at the first read, please hold on and read and re-read more.\nDefinition Consider a mechanism M, and two datasets \\( D1 \\) and \\( D2 \\) that differ only in one single datapoint. The mechanism \\( M \\) is ε-differentially private if, for all such datasets \\( D1 \\) and \\( D2 \\), the following holds:\n$$ \\mathbb{P}\\left[M(D_1)=O\\right] \\le e^\\varepsilon\\cdot\\mathbb{P}\\left[M(D_2)=O\\right] , ε\u0026gt;0 $$\nand, it holds for all possible output values of \\( M \\).\nIt should go without saying that if I swap the places of \\( D1 \\) and \\( D2 \\), the equation must still hold in which case it becomes: $$ \\mathbb{P}\\left[M(D_2)=O\\right] \\le e^\\varepsilon\\cdot\\mathbb{P}\\left[M(D_1)=O\\right] $$\nWith some easy math on these two equations, here\u0026rsquo;s what we get:\n$$ e^{-\\varepsilon} \\le \\frac{\\mathbb{P}\\left[M(D_1)=O\\right]}{\\mathbb{P}\\left[M(D_2)=O\\right]} \\le e^\\varepsilon $$\n[Please stare at this equation for a while :) It\u0026rsquo;s what the essence is and if you\u0026rsquo;re able to understand the intuitive meaning of what the equation is trying to say, my purpose of writing this blog is solved.]\nNow, let\u0026rsquo;s reiterate: The mechanism \\( M \\) is ε-differentially private if, for all such datasets \\( D1 \\) and \\( D2 \\) that differ in only one data point, the following holds for all possible outputs O:\n$$ e^{-\\varepsilon} \\le \\frac{\\mathbb{P}\\left[M(D_1)=O\\right]}{\\mathbb{P}\\left[M(D_2)=O\\right]} \\le e^\\varepsilon $$\nThis equation is exactly what we dicussed above: If \\( M \\) is differentially private, then the (ratio of the) output we get out of \\( M \\) in case I am right handed and the output we get out of \\( M \\) in case I am left are \u0026ldquo;similar\u0026rdquo;, and hence we the ratio of them is bounded in the equation above. In essence, the probability of obtaining O as the output of \\( M(D1) \\) does not differ too much from the probability of obtaining O as the output of \\( M(D2) \\) (that\u0026rsquo;s what the bounds above guarantee) thus making it hard for the attacker to make deductions. Another way to write it is: For an ε-differentially private, the above holds. Hence, the two probabilities cannot differ too much from each other for the bound to hold!\n[BTW, if it isn\u0026rsquo;t clear why we\u0026rsquo;re talking terms of probability here: Remember we add some random noise to \\( M \\) rather than returning its true output. This makes it a probabilistic mechanism rather than a deterministic one.]\nAnd ofcourse, it follows by basic math that higher the value of \\( ε \\), lesser is the privacy guarantee that we get and vice versa. What\u0026rsquo;s also nice to note here is that now that we have formalized DP by means of the parameter \\( ε \\), we can \u0026ldquo;quantitatively\u0026rdquo; compare the privacies offered by two mechanisms \\( M1 \\) and \\( M2 \\) and say things like \\( M1 \\) is more private than \\( M2 \\) etc.\nConnect everything We have discussed the mathematical formalization and the intuitive understanding of DP but it is all still very abstract. To get a crystal clear picture, let us circle back to my handedness example.\nIn numbers, say,\nTotal no. of individuals in the survey = 999 + 1 (I) = 1000 individuals The attacker knows the following: 500 are right-handed and 499 are-left handed. The statistic to be released is the number of left handed individuals (for simplicity and without any loss of generality, I am taking the number rather than the proportion here). Also, let \\( D1 \\) = I am left handed and \\( D2 \\) = I am right handed and say, the ground truth is \\( D1 \\) which the attacker obviously does not know and which is what they intend to deduce/know by means of the survey statistic. And since, \\( D1 \\) is the truth, the non-differentially private output of \\( M \\) = 500.\nDP in the picture:\nNow the mechanism \\( M \\) here simply counts the number of left-handed and right-handed individuals with a goal to release the said statistic. And to be able to protect my privacy, we also want to make \\( M \\) ε-differentially private.\nHow do we do that?:\nOfcourse, we cannot release the true statistic and so, as part of \\( M \\), we need to add some noise (say \\( X \\)) to the counts before releasing them. The mathematical selection of this noise is the key and it should be such that \\( M \\) becomes ε-differentially private (which means that after adding the noise, \\( M \\) should satisfy the equation discussed above).\nWithout any long talk, I\u0026rsquo;ll tell you that we sample this noise from the Double Exponential Distribution with a scale parameter \\( 1/ε \\) and it does what we are looking for - this noise is capable of providing the guarantees that we promised while calling \\( M \\) to be ε-differentially private.\nBTW, here\u0026rsquo;s how the Double Exponential Distribution looks like:\nLet\u0026rsquo;s work out the Math! Without any noise, the output \\( O \\) of \\( M \\) = 500. Let\u0026rsquo;s say we sample some noise \\( X \\) as told above and it comes out to be \\( X = -1 \\), and hence the the output \\( O \\) of ε-differentially private \\( M \\) that we publish becomes 500 - 1 = 499. Now ofcourse, using 499 as the released statistic on the number of left handed individuals, the attacker cannot conclude that I am right handed. Same goes for any value of noise \\( X \\). We are left to see if our privacy guarantees are satisfied with \\( X = -1 \\).\nBefore showing it mathematically, it might help to make sense of the graph below and realising that the X-axis shows the several values of \\( O \\) that can be published under DP, and that under both \\( D1 \\) and \\( D2 \\) ((shown by the blue and red graphs) the probability of each such \\( O \\) getting published are similar.\nThe Privacy Guarantees for \\( X = -1 \\) or \\( O = 499 \\):\nClearly,\n$$ \\mathbb{P}\\left[M(D_1)=O\\right] = \\mathbb{P}\\left[X=-1\\right] $$\n(If I am left handed then the true \\( O \\) is 500 and I need to add a noise of -1 to be able to publish 499).\nSimilarly, $$ \\mathbb{P}\\left[M(D_2)=O\\right] = \\mathbb{P}\\left[X=0\\right] $$\n(If I am right handed then the true \\( O \\) is 499 and I need to add a noise of 0 to be able to publish 499).\nAnd there we go: $$ \\frac{\\mathbb{P}\\left[M(D_1)=O\\right]}{\\mathbb{P}\\left[M(D_2)=O\\right]} = \\frac{\\mathbb{P}\\left[X=-1\\right]}{\\mathbb{P}\\left[X=0\\right]} = e^{-\\varepsilon} $$ (This simply follows from the pdf of Double Exponential Distribution with a scale parameter \\( \\frac{1}{ε} \\) so I will not show that math here).\nThis clearly shows that using the noise that we used, our mechanism \\( M \\) is ε-differentially private. Cheers! If you were able to follow this far, you\u0026rsquo;ve already designed your first own ε-differentially private algorithm. :)\n","permalink":"http://localhost:1313/blog/dp/","summary":"I think differential privacy is beautiful!\nWhy are we here? Protecting the privacy of data is important and not trivial. To help make sense of things here, the Fundamental Law of Information Recovery becomes useful which states: Overly accurate estimates of too many statistics can completely destroy (data) privacy. Another example that provides a good incentive for why privacy is important is the ability of LLMs to memorize data which is an undesirable outcome as it risks the leak of PII.","title":"An Introduction to Differential Privacy"},{"content":"This piece of mine got featured in the Weights and Biases fully connected blog. It can be found here.\n","permalink":"http://localhost:1313/blog/2023-01-05-pytorch-ds-dl/","summary":"This piece of mine got featured in the Weights and Biases fully connected blog. It can be found here.","title":"PyTorch Datasets and Dataloaders"},{"content":"Visit my github for more. Following are some selected samples.\nPaper Implementations Switch Transformers.\nEfficient PyTorch implementation of the Switch Transformer with (optional) aux loss for each layer and configurable number of experts and expert capacity..\n. ","permalink":"http://localhost:1313/code/","summary":"Visit my github for more. Following are some selected samples.\nPaper Implementations Switch Transformers.\nEfficient PyTorch implementation of the Switch Transformer with (optional) aux loss for each layer and configurable number of experts and expert capacity..\n. ","title":"Papers"}]