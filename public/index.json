[{"content":"In this short post, I am going to talk about the Switch Transformer paper.\nBackground and Architecture To begin: In Dense Models, each parameter/neuron of the network gets activated during the forward pass.\nHence, to be able to derive more performance by means of increasing the model size, more computation is required. Implicitly, more model parameters =\u0026gt; more computation performed by a single token. And tangential to this, work by Kaplan et al. (2020) becomes super relevant as they discuss training larger models on comparatively smaller amounts of data as the computationally optimal approach.\nSo, the point is that while increasing the number of model parameters yields performance, it comes at the cost of increased total computation/FLOPs per token (There\u0026rsquo;s nuance to the former part of this statement but it communicates the gist).\nSuch is not the case with Routed Models as we\u0026rsquo;ll see. Let\u0026rsquo;s first look at what a \u0026ldquo;Routed Layer\u0026rdquo; looks like. The above figure might seem intimidating but we\u0026rsquo;ll understand all of it. The single feed forward layer is replaced by several different FF layers (FF1 through FF4 in the figure) each of which is called an Expert. Each token is routed to only one of the experts during a single forward pass. The Router (in green) is yet another FFNN that decides which expert should each token be routed to. And to make it clear, the same router is used for all tokens.\nLet\u0026rsquo;s look at some math Consider,\n\\( \\mathcal{B} \\): the current batch \\( T \\): number of tokens in the current batch \\( N \\): the number of experts in the current routed layer \\( E_i \\): the expert \\( i \\) \\( p_i(x) \\): the probability that the token \\( x \\) is routed to \\( E_i \\) The Router, as discussed, is a FFNN that takes \\( x \\) as an input and first produces logits given by: $$ h(x) = W_r . x $$\nwhere,\n\\( W_r \\): The router weights\nTalk of probability in FFNNs and you have softmax; hence the router probability for a particular expert \\( E_i \\) is calculated by normalizing the logits as:\n$$ p_i(x) = \\frac{e^{h(x)_i}}{\\sum_{j=1}^{N} e^{h(x)_j}} $$\nFrom here, as can be seen in the figure above, the Switch paper routes \\( x \\) to the expert with the highest \\( p_i(x) \\).\nAnd thereupon the output corresponding to token \\( x \\) is calculated as:\n$$ y = p_i(x)E_i(x); i \\in {1, 2, \u0026hellip;, N} $$\nThis is what the dotted line in the figure shows.\nTo revise: we went from a layer with a single feed forward network to what the paper calls a \u0026ldquo;Switch layer\u0026rdquo;.\nWhat happened here? The architectural change was straight forward to understand: you replace the single FF network with a bunch of different FF networks, of which one token uses only one at a time. What this means is that while the total parameters in the layer increase this way by \\( N \\) times, the total computation performed by each token remained constant.\nAnd so here we go: We are able to scale on the number-of-parameters axis while keeping the FLOPs per token constant. All of this by virtue of the routing mechanism that introduces \u0026ldquo;sparsity\u0026rdquo; in the model \u0026ndash; not all parameters of the model are activated during a token\u0026rsquo;s forward pass.\nRevising the Switch Layer: MoE vs Switch Transformer As we saw above, one token is routed to only one expert at a time in the Switch Transformer paper.\nThe Outrageously Large Neural Networks paper (Shazeer et al. 2017) does this differently. Instead of routing a token to only one expert, they select the top-k experts for a token, in which case the output of the MoE layer (they call it that) for that token becomes:\n$$ y = \\sum_{i \\in \\mathcal{T}} p_i(x)E_i(x) $$\nwhere,\n\\( \\mathcal{T} \\): set of top-k Experts\nWhy top-k? I read the OLNN paper carefully but did not see any elaborate mentions of why \\( k\u0026gt;1 \\) experts are required. But what the paper nicely elaborates on is \u0026ldquo;load-balancing\u0026rdquo;. We\u0026rsquo;ll circle back to load-balancing in context of the Switch paper but the term essentially means to ensure that all experts are sufficiently utilized and hence trained during the training process. In other words, the router shouldn\u0026rsquo;t always favour a certain (set of) expert(s).\nAnd from here, one can intuition about why the authors chose \\( k\u0026gt;1 \\) (precisely they use \\( k=4 \\)) \u0026ndash; to make sure that not just one but more than one expert is involved during any single backprop for the router to be able learn the routing process without favouring a single (set of) expert(s).\nThat said, if \\( k\u0026gt;1 \\) is one aspect of load-balancing along with playing a part in making sure all experts are sufficiently trained, how were the Switch authors able to do well without it?\nAuxiliary loss This section is the answer to that. The aux loss is just a term that\u0026rsquo;s added to the total model loss. The purpose of it is to penalize the router for favouring a certain expert too much, i.e. for giving too large a probability to any one expert. Hence, this loss encourages a \u0026ldquo;uniform routing\u0026rdquo;. The word \u0026ldquo;uniform\u0026rdquo; here is more than an english word; it\u0026rsquo;s used in the statistical sense of the Uniform Distribution and logically so. Because what we want is for the tokens to be distributed uniformly among the experts.\nAnd we will see how exactly does this loss encourages a Uniform Distribution. But let\u0026rsquo;s see the math once.\nKeeping the previous notations same, consider:\n\\( f_i \\): the fraction of tokens routed to \\( E_i \\) Hence,\n\\( f_i \\) = Number of tokens from \\( \\mathcal{B} \\) router to \\( E_i \\) / Total tokens in \\( \\mathcal{B} \\)\n$$ f_i = \\frac{1}{T} \\sum_{x \\in \\mathcal{B}} \\mathbb{1} \\{argmax \\ p(x) = i \\} $$\n\\( P_i \\): fraction of the router probability assigned to \\( E_i \\) across all tokens in \\( \\mathcal{B} \\) Hence,\n\\( P_i \\) = Sum of \\( p_i(x) \\) across \\( \\mathcal{B} \\) / Total router probability across \\( \\mathcal{B} \\)\n$$ P_i = \\frac{1}{T} \\sum_{x \\in \\mathcal{B}} p_i(x) $$\nThe aux loss is now given by:\n$$ loss = \\alpha . N . \\sum_{i=1}^{N} f_i . P_i $$\nwhere,\n\\( \\alpha \\): coefficient to control the effect of aux losses It needs to be large enough for the aux losses to have a sufficient load-balancing effect, and small enough to not overwhelm the primary modelling loss.\nNow, under Uniform routing (which is what we want), each expert should get \\( \\frac{T}{N} \\) tokens routed to it. And for every particular \\( x \\), \\( p_i(x) \\) should be \\( \\frac{1}{N} \\) for each value of \\( i \\) i.e. for each expert.\nHence, under uniform routing:\n$$ f_i = \\frac{1}{T} . \\frac{T}{N} = \\frac{1}{N} $$ $$ P_i = \\frac{1}{T} . \\sum_{x \\in \\mathcal{B}} \\frac{1}{N} = \\frac{1}{T} . \\frac{T}{N} = \\frac{1}{N} $$\nAnd these are the exact values at which the aux loss as described above is minimized, and hence it encourages uniform routing!\n[Notice the term \\( N \\) multiplied in the aux loss equation? Plug in these uniform values and try to reason why it\u0026rsquo;s necessary.]\nObviously, there\u0026rsquo;s more than a single switch layer in the full architecture, and we want each one of it to route uniformly. So, aux losses corresponding to each switch layer are calculated and added to the language model training loss.\nBenefits of \\( k=1 \\) (The Switch Layer) Choosing \\( k=1 \\) over \\( k\u0026gt;1 \\) results in some obvious benefits:\nLess computation needs to be performed per token. Cross device communication is reduced owing to the needlessness to perform an addition operation to be able to get the token output. Efficient routing: Expert Capacity The Expert Capacity of each expert which is basically the max batch size allocated to each expert. It goes:\n$$ \\text{expert capacity} = \\left( \\frac{\\text{tokens per batch}}{\\text{number of experts}} \\right) \\times \\text{capacity factor} $$\nAs is clear, the capacity factor is introduced in the above equation to create an additional buffer for cases when there\u0026rsquo;s an unequal allocation of tokens among the experts by the router. If the router allocated count exceeds the Expert Capacity, the extra tokens are just dropped from the expert computation (and make their way to the next layer via the residual connection).\nNow a natural question is that why is this additional constraint even required and why can\u0026rsquo;t we just have the router route however it wants with the flexibility to increase the Expert Capacity indefinitely? At once, load balancing might come up as one reason but if thought carefully, Expert Capacity doesn\u0026rsquo;t really do anything in that sense. As in it\u0026rsquo;s not encouraging balancing in the sense of causing other experts to be equally utilized simply because the extra tokens are just dropped and not like re-routed to some other expert(s).\nThe reason why Expert Capacity is needed is to make sure that resources (computation and memory) are not wasted while some expert operates at its full capacity and others don\u0026rsquo;t; and obviously this is also the reason why we don\u0026rsquo;t want to have too large of an expert capacity. The goal is to strike a good balance so that not many tokens are dropped (the authors empirically prove that a lower drop rate is important to scale these models) and not a lot of resources are wasted.\n[I realise that this reasoning might still remain a bit unclear or atleast too abstract to grasp to those that don\u0026rsquo;t have a clear understanding of the GPU arch, resource management, memory allocation on the devices etc. The insight here is that if the expert capacity of each expert is set to very high, it will be a waste for those experts that do not process as many tokens. GPUs are expensive and we don\u0026rsquo;t want resources to be idle.]\nScaling Laws There are a couple interesting plots in the paper showing scaling on the number-of-experts axis. Note that increasing the number of experts does not increase the FLOPs per token and hence the models remain FLOPs matched \u0026ndash; the computation budget per token remains constant.\nMore experts = Better test loss The plot below clearly shows that as the number of experts increase the test loss decreases. Ofcourse, everything else (#tokens, #training steps etc.) must be kept constant. More experts = Fast training Firstly, fast here does not mean more examples processed per second. Fast implies what can be clearly seen in the plot below: Having more experts achieves the same level of negative log perplexity faster than a model with lesser experts \u0026ndash; faster in terms of the number of training steps performed. Additionally, the authors also find for a fixed number of tokens seen, larger models learn faster.\nWhat needs to be carefully noted here is that while the total computation performed per token (FLOPs per token) is constant across models with varying number of experts, a relatively very small computation difference is there by virtue of the router having to compute as many probailities as there are the number of experts; but this is minor. Consequently, we also note that there\u0026rsquo;s no router calculations at all in dense models. And additionally, Switch transformer also needs to do some extra communication across devices by virtue of the different experts being on different devices.\nThis leads us to an important point: Being faster on the numnber-of-training steps axis does not necessarily mean being faster on the time-taken-by-the-wall-clock axis. Again, the premise here is that the Switch Transformer, owing to the extra computation and communication that it needs to do, might complete a certain number of training steps slower than its dense counterpart. This means that while we expect the Switch Transformer to be better performing after completing the same number of training steps as the dense model, it can still take more time for the former to actually reach that point.\nSo the next obvious question is: Given a fixed amount of wall clock time (and ofcourse the computation budget), how do these models compare in performance with each other?\nFixed training time The plot below answers the question posed just above - For a fixed training duration, Switch transformer outperforms the dense model. Same story for the number of experts as they grow, but here the difference isn\u0026rsquo;t as significant and the intuition is pretty easy to reach at if the past few paras were clear enough. Moving forward to FLOPs un-matched If we go back to the motivation behind using a MoE or a sparse routed model like the Switch Transformer, it\u0026rsquo;s simply stated as: MoEs allow us to increase the parameter count while keeping the computational budget constant. While the benefit of training an MoE/Switch model over its FLOPs matched baseline is clear by now, how does a Switch model compare with a more-FLOPs-demanding dense baseline? The plot below shows that Switch-Base (124B FLOPs/seq) is still more sample efficient than dense T5-Large (425B FLOPs/seq). This is cool!\nLet\u0026rsquo;s \u0026ldquo;infer\u0026rdquo; the tradeoff! Now, let\u0026rsquo;s \u0026ldquo;infer\u0026rdquo; a tradeoff from whatever we\u0026rsquo;ve studied till now. I am using those quotes because the tradeoff is regarding the Inference stage. :-)\nAs we\u0026rsquo;ve seen, with the same amount of computational budget, MoE/Switch performs better than the FLOPs matched dense baseline owing to ofcourse, the greater number of parameters that it has. What this also means for the inference stage is that the former requires more memory than the latter in exchange of the performance gains that it offers. The obvious reason being that all the experts need to be loaded in the memory.\nHence, it\u0026rsquo;s a memory vs performance tradeoff.\nReferences [1] Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity\n[2] Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer\n","permalink":"http://localhost:1313/blog/moes/","summary":"\u003cp\u003eIn this short post, I am going to talk about the Switch Transformer paper.\u003c/p\u003e\n\u003ch2 id=\"background-and-architecture\"\u003eBackground and Architecture\u003c/h2\u003e\n\u003cp\u003eTo begin: In Dense Models, each parameter/neuron of the network gets activated during the forward pass.\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"ffnn.png#center\" alt=\"fully activated dense model\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003eHence, to be able to derive more performance by means of increasing the model size, more computation is required. Implicitly, more model parameters =\u0026gt; more computation performed by a single token.\nAnd tangential to this, work by \u003ca href=\"https://arxiv.org/abs/2001.08361\"\u003eKaplan et al. (2020)\u003c/a\u003e becomes super relevant as they discuss training larger models on comparatively smaller amounts of data as the computationally optimal approach.\u003c/p\u003e","title":"Switch Transformer - Sparse Routed Networks/MoEs"},{"content":"This post covers the very basic foundation needed to learn GPU programming and/or Parallel programming on CPUs only.\nI will cover the architectural details of two of the several processors that empower the modern day computer - the CPUs and the GPUs. By the end of this post, one should have a good understanding of the following terms - (in no particular order) chips, processors, microprocessors, cores, latency device, throughput device, clock speed, threads, processes, instructions, memory bandwidth, memory system.\nCore, Microprocessor/Processor, Chip \u0026ldquo;A chip\u0026rdquo; is the physical semiconductor chip; it\u0026rsquo;s \u0026ldquo;a physical integrated circuit\u0026rdquo; comprised of transistors, resistors, and capacitors.\nA processor (here, think of CPU, the central \u0026ldquo;processing\u0026rdquo; unit) is a digital circuit that\u0026rsquo;s implemented on a single or a few chips. Now, the term micro is appended to the beginning of processors to refer to the fact that it takes a single or a very few chips to implement a microprocessor. But this is more of a definition. In the context/scope of this post, consider 1 microprocessor = 1 chip.\nThe modern day computer is powered by processors of different types - CPUs, GPUs etc. I have also read the term processor chip, the meaning of which should be clear now.\nNow, here\u0026rsquo;s the thing: In older days, 1 processor used to mean 1 processing unit (single CPU based microprocessor) which changed around the year 2000 when microprocessors with more than one processing unit/CPU were introduced. Those are what are called as multi-core processors. Hence, a CPU \u0026ldquo;core\u0026rdquo; is basically a single processing unit within a processor chip that is capable of running instructions independently; and hence the modern day microprocessor with several cores is essentially a parallel processor benefitting from software that leverages parallel programming paradigms. Read that again until the terms Core, Microprocessor/Processor, Chip and the distinctions and synonymities between them are clear.\n👉 If you google intel core i9 processor, the table there has a column # of \u0026ldquo;cores\u0026rdquo;.\nClock rate Types of Random Access Memory (RAM) Before proceeding to study the processor architectures, it\u0026rsquo;s worth discussing in brief two types of RAM - Static RAM (SRAM) and Dynamic RAM (DRAM).\nSRAM - DRAM - Processor Architectures (Hardware Design) Let us now look into the architectures of the CPU and the GPU, and try to make sense of why the CPU is called a latency device and the GPU a throughput device.\nCPU Let us first look at what a chip with 4 cores looks like:\nImage source A slight but important correction to note here is that while according to the figure above, the DRAM (sometimes simply referred to as RAM or system memory) appears to be located on the chip, it\u0026rsquo;s not acutally the case. The DRAM is a separate hardware entity that\u0026rsquo;s mounted on the motherboard.\nNext, pay attention to how the chip area is divided among the different components. Note also the multiple levels of cache memories present on the chip (purple and blue) \u0026ndash; they help to reduce the latency by decreasing the required amounts of high latency memory (DRAM) accesses. Ah, I went too fast here! To further clarify, since\nNow let\u0026rsquo;s zoom into a single core:\n// figure\nA few main components are shown in the core above:\nA few very powerful ALUs (Arithmetic Logic Units): A few of them are present on each core and it\u0026rsquo;s where the actual computation happens. Each ALU in itself is very powerful and capable of completing a computation in a very few clock cycles; and hence are geared towards the low latency paradigm.\nA Control Unit (CU): A major area is occupied by this component as its two main functions help greatly in reducing the latency - branch prediction and data forwarding. I won\u0026rsquo;t elaborate on these two but the takeaway is that each CPU core features a sophisticated CU which again serves the low latency design.\nA big L1 cache: Ofcourse, much smaller than the DRAM, a significant portion on each is dedicated to the L1 cache again to reduce the latency.\nGPU From the same source, here\u0026rsquo;s what a GPU chip looks like:\nAs can be seen, the major chip area is now occupied by the green boxes which are the components where the computation takes place (I am refraining from giving a name to the green boxes just yet but yes they are the equivalent of the green ALUs we saw in the CPU). But what\u0026rsquo;s also worth noting is that each green box is now much more smaller than 1 single ALU in the CPU core \u0026ndash; this actually reflects the real scenario that a single of these units on the GPU is much much less powerful than a single ALU and hence has a much longer latency.\nThe L1 caches and the control occupy much lesser chip area.\nInstructions, Threads, Processes Memory Bandwidth Why can\u0026rsquo;t CPUs just have a higher memory bandwidth? ","permalink":"http://localhost:1313/blog/cuda-primer/","summary":"\u003cp\u003eThis post covers the very basic foundation needed to learn GPU programming and/or Parallel programming on CPUs only.\u003c/p\u003e\n\u003cp\u003eI will cover the architectural details of two of the several processors that empower the modern day computer - the CPUs and the GPUs.\nBy the end of this post, one should have a good understanding of the following terms - (in no particular order) chips, processors, microprocessors, cores, latency device, throughput device, clock speed, threads, processes, instructions, memory bandwidth, memory system.\u003c/p\u003e","title":"[WIP] Learning CUDA Programming: A Primer"},{"content":"I think differential privacy is beautiful!\nWhy are we here? Protecting the privacy of data is important and not trivial. To help make sense of things here, the Fundamental Law of Information Recovery becomes useful which states: Overly accurate estimates of too many statistics can completely destroy (data) privacy. Another example that provides a good incentive for why privacy is important is the ability of LLMs to memorize data which is an undesirable outcome as it risks the leak of PII.\nBefore discussing DP, I\u0026rsquo;d like to spend a few words on giving some intuition behind The Privacy vs Utility Tradeoff:\nWhenever we make some use of the data at hand and hence, learn something useful from it, we lose out on some privacy. Elaboratively, let\u0026rsquo;s say we begin by learning only one statistic from a collection of datapoints at hand, even then we lose out on some privacy as far as the individual data points are concerned. This privacy loss keeps amplifying as we keep on learning more useful information from the data, and importantly, this is inevitable.\nConversely, to be able to maintain full privacy of the data, we will have to give up on learning \u0026ldquo;anything\u0026rdquo; useful from it. Total privacy = No learning.\nDifferential Privacy: A High Level Intuition First of all, what even is privacy in a \u0026ldquo;non-subjective sense\u0026rdquo;?\n[Why non-subjective?: To me, getting the data on my handedness leaked is not a breach of my privacy. I don\u0026rsquo;t mind it but someone else might consider it as a privacy violation if their handedness is to get leaked.]\nDP answers this question in a concrete mathematical equation, but let\u0026rsquo;s first get the intuition right. Consider an attacker who is interested in knowing my handedness and somehow gets access to the following information:\nI, along with 999 other individuals are participating in a survey whose end result shall be the percentage statistics of people belonging to the left-handed category. The attacker is very strong and somehow was also able to get their hands on the handedness of the other 999 people. Voila! Let the survey results get published and the attacker will be able to learn my handedness with 100% accuracy (I am ambidexterous btw and I don\u0026rsquo;t mind sharing it but you get the gist :)).\nHow can we prevent this?:\nKeeping the above example in mind, consider a Mechanism \\( M \\) (the survey) and a dataset \\( D \\). The Mechanism \\( M \\) processes \\( D \\) and produces some output \\( O \\). By virtue of \\( M \\), it\u0026rsquo;s possible for the attacker to look at \\( O \\) and recover my info using it. What differential privacy does is that it adds some form of noise (or randomness) to \\( M \\) as a result of which the attacker is no more able to make such deductions using the output \\( O \\) that\u0026rsquo;s a result of the \u0026ldquo;Differentially Private Mechanism \\( M \\)\u0026rdquo;. I mean they are free to make such deductions ofcourse, but those will not be as accurate as we discussed above and it is all by virtue of the noise we add to \\( M \\).\n[What this noise is, where and how it\u0026rsquo;s added and how it works to provide mathematical privacy guarantees is something I\u0026rsquo;ll circle back to in a minute.]\nElaborating on it:\nIf \\( M \\) is differentially private, then the output we will get out of \\( M \\) in case I am right handed and the output we get out of \\( M \\) in case I am left, are \u0026ldquo;similar\u0026rdquo;.\nI want to define here concretely what \u0026ldquo;similar\u0026rdquo; actually means: It means that we can get the exact same output out of \\( M \\) in both these cases with a \u0026ldquo;similar probability\u0026rdquo;. And hence, the attacker can never be 100% sure of my handedness and so the Mechanism \\( M \\) protects my privacy. A key thing to note here is that it is the Mechanism \\( M \\), and not the output \\( O \\), that is differentially private.\nDP is strong! Before diving in the Math, there are two very interesting properties that need some clarification here:\nGoal of the attacker - DP is capable of protecting all kinds of info. Say, if there\u0026rsquo;s a different setup wherein there are two datasets \\( D1 \\) and \\( D2 \\) between which I am present in only one, and the goal of the attacker is to identify the dataset which I am a part of. DP protects my privacy against this and many other kinds of potential attacks.\nStrength of the Attacker - The attacker that we considered above was quite strong (so much so that the two cases we considered differ only in one data point which is mine), and even then we were able to protect my data using DP. The point here is that DP protects privacy no matter how strong the attacker is, what they know and whatever their capabilities are.\nI hope this section was helpful in building some notion of what DP is and how it defines and protects privacy.\nThe Math I will now write in an equation what we discussed above in words. This equation is actually the standard definition of differential privacy and the goal of the next few paragraphs is to decode the formal definition of DP and understand how that relates to the intuitive understanding of privacy that we got above. If the equation does not make 100% sense at the first read, please hold on and read and re-read more.\nDefinition Consider a mechanism M, and two datasets \\( D1 \\) and \\( D2 \\) that differ only in one single datapoint. The mechanism \\( M \\) is ε-differentially private if, for all such datasets \\( D1 \\) and \\( D2 \\), the following holds:\n$$ \\mathbb{P}\\left[M(D_1)=O\\right] \\le e^\\varepsilon\\cdot\\mathbb{P}\\left[M(D_2)=O\\right] , ε\u0026gt;0 $$\nand, it holds for all possible output values of \\( M \\).\nIt should go without saying that if I swap the places of \\( D1 \\) and \\( D2 \\), the equation must still hold in which case it becomes: $$ \\mathbb{P}\\left[M(D_2)=O\\right] \\le e^\\varepsilon\\cdot\\mathbb{P}\\left[M(D_1)=O\\right] $$\nWith some easy math on these two equations, here\u0026rsquo;s what we get:\n$$ e^{-\\varepsilon} \\le \\frac{\\mathbb{P}\\left[M(D_1)=O\\right]}{\\mathbb{P}\\left[M(D_2)=O\\right]} \\le e^\\varepsilon $$\n[Please stare at this equation for a while :) It\u0026rsquo;s what the essence is and if you\u0026rsquo;re able to understand the intuitive meaning of what the equation is trying to say, my purpose of writing this blog is solved.]\nNow, let\u0026rsquo;s reiterate: The mechanism \\( M \\) is ε-differentially private if, for all such datasets \\( D1 \\) and \\( D2 \\) that differ in only one data point, the following holds for all possible outputs O:\n$$ e^{-\\varepsilon} \\le \\frac{\\mathbb{P}\\left[M(D_1)=O\\right]}{\\mathbb{P}\\left[M(D_2)=O\\right]} \\le e^\\varepsilon $$\nThis equation is exactly what we dicussed above: If \\( M \\) is differentially private, then the (ratio of the) output we get out of \\( M \\) in case I am right handed and the output we get out of \\( M \\) in case I am left are \u0026ldquo;similar\u0026rdquo;, and hence we the ratio of them is bounded in the equation above. In essence, the probability of obtaining O as the output of \\( M(D1) \\) does not differ too much from the probability of obtaining O as the output of \\( M(D2) \\) (that\u0026rsquo;s what the bounds above guarantee) thus making it hard for the attacker to make deductions. Another way to write it is: For an ε-differentially private, the above holds. Hence, the two probabilities cannot differ too much from each other for the bound to hold!\n[BTW, if it isn\u0026rsquo;t clear why we\u0026rsquo;re talking terms of probability here: Remember we add some random noise to \\( M \\) rather than returning its true output. This makes it a probabilistic mechanism rather than a deterministic one.]\nAnd ofcourse, it follows by basic math that higher the value of \\( ε \\), lesser is the privacy guarantee that we get and vice versa. What\u0026rsquo;s also nice to note here is that now that we have formalized DP by means of the parameter \\( ε \\), we can \u0026ldquo;quantitatively\u0026rdquo; compare the privacies offered by two mechanisms \\( M1 \\) and \\( M2 \\) and say things like \\( M1 \\) is more private than \\( M2 \\) etc.\nConnect everything We have discussed the mathematical formalization and the intuitive understanding of DP but it is all still very abstract. To get a crystal clear picture, let us circle back to my handedness example.\nIn numbers, say,\nTotal no. of individuals in the survey = 999 + 1 (I) = 1000 individuals The attacker knows the following: 500 are right-handed and 499 are-left handed. The statistic to be released is the number of left handed individuals (for simplicity and without any loss of generality, I am taking the number rather than the proportion here). Also, let \\( D1 \\) = I am left handed and \\( D2 \\) = I am right handed and say, the ground truth is \\( D1 \\) which the attacker obviously does not know and which is what they intend to deduce/know by means of the survey statistic. And since, \\( D1 \\) is the truth, the non-differentially private output of \\( M \\) = 500.\nDP in the picture:\nNow the mechanism \\( M \\) here simply counts the number of left-handed and right-handed individuals with a goal to release the said statistic. And to be able to protect my privacy, we also want to make \\( M \\) ε-differentially private.\nHow do we do that?:\nOfcourse, we cannot release the true statistic and so, as part of \\( M \\), we need to add some noise (say \\( X \\)) to the counts before releasing them. The mathematical selection of this noise is the key and it should be such that \\( M \\) becomes ε-differentially private (which means that after adding the noise, \\( M \\) should satisfy the equation discussed above).\nWithout any long talk, I\u0026rsquo;ll tell you that we sample this noise from the Double Exponential Distribution with a scale parameter \\( 1/ε \\) and it does what we are looking for - this noise is capable of providing the guarantees that we promised while calling \\( M \\) to be ε-differentially private.\nBTW, here\u0026rsquo;s how the Double Exponential Distribution looks like:\nLet\u0026rsquo;s work out the Math! Without any noise, the output \\( O \\) of \\( M \\) = 500. Let\u0026rsquo;s say we sample some noise \\( X \\) as told above and it comes out to be \\( X = -1 \\), and hence the the output \\( O \\) of ε-differentially private \\( M \\) that we publish becomes 500 - 1 = 499. Now ofcourse, using 499 as the released statistic on the number of left handed individuals, the attacker cannot conclude that I am right handed. Same goes for any value of noise \\( X \\). We are left to see if our privacy guarantees are satisfied with \\( X = -1 \\).\nBefore showing it mathematically, it might help to make sense of the graph below and realising that the X-axis shows the several values of \\( O \\) that can be published under DP, and that under both \\( D1 \\) and \\( D2 \\) ((shown by the blue and red graphs) the probability of each such \\( O \\) getting published are similar.\nThe Privacy Guarantees for \\( X = -1 \\) or \\( O = 499 \\):\nClearly,\n$$ \\mathbb{P}\\left[M(D_1)=O\\right] = \\mathbb{P}\\left[X=-1\\right] $$\n(If I am left handed then the true \\( O \\) is 500 and I need to add a noise of -1 to be able to publish 499).\nSimilarly, $$ \\mathbb{P}\\left[M(D_2)=O\\right] = \\mathbb{P}\\left[X=0\\right] $$\n(If I am right handed then the true \\( O \\) is 499 and I need to add a noise of 0 to be able to publish 499).\nAnd there we go: $$ \\frac{\\mathbb{P}\\left[M(D_1)=O\\right]}{\\mathbb{P}\\left[M(D_2)=O\\right]} = \\frac{\\mathbb{P}\\left[X=-1\\right]}{\\mathbb{P}\\left[X=0\\right]} = e^{-\\varepsilon} $$ (This simply follows from the pdf of Double Exponential Distribution with a scale parameter \\( \\frac{1}{ε} \\) so I will not show that math here).\nThis clearly shows that using the noise that we used, our mechanism \\( M \\) is ε-differentially private. Cheers! If you were able to follow this far, you\u0026rsquo;ve already designed your first own ε-differentially private algorithm. :)\n","permalink":"http://localhost:1313/blog/dp/","summary":"\u003cp\u003eI think differential privacy is beautiful!\u003c/p\u003e\n\u003ch2 id=\"why-are-we-here\"\u003eWhy are we here?\u003c/h2\u003e\n\u003cp\u003eProtecting the privacy of data is important and not trivial. To help make sense of things here, the Fundamental Law of Information Recovery becomes useful which states: Overly accurate estimates of too many statistics can completely destroy (data) privacy. Another example that provides a good incentive for why privacy is important is the ability of \u003ca href=\"https://blog.research.google/2020/12/privacy-considerations-in-large.html\"\u003eLLMs to memorize data\u003c/a\u003e which is an undesirable outcome as it risks the leak of PII.\u003c/p\u003e","title":"An Introduction to Differential Privacy"},{"content":"This piece of mine got featured in the Weights and Biases fully connected blog. It can be found here.\n","permalink":"http://localhost:1313/blog/2023-01-05-pytorch-ds-dl/","summary":"\u003cp\u003eThis piece of mine got featured in the Weights and Biases fully connected blog. It can be found \u003ca href=\"https://wandb.ai/srishti-gureja-wandb/posts/reports/How-To-Eliminate-the-Data-Processing-Bottleneck-With-PyTorch--VmlldzoyNDMxNzM1\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","title":"PyTorch Datasets and Dataloaders"},{"content":"Visit my github for more. Following are some selected samples.\nPaper Implementations Switch Transformers.\nEfficient PyTorch implementation of the Switch Transformer with (optional) aux loss for each layer and configurable number of experts and expert capacity..\n. ","permalink":"http://localhost:1313/code/","summary":"\u003cp\u003eVisit my \u003ca href=\"https://github.com/srishti-git1110\"\u003egithub\u003c/a\u003e for more. Following are some selected samples.\u003c/p\u003e\n\u003ch3 id=\"paper-implementations\"\u003ePaper Implementations\u003c/h3\u003e\n\u003cdiv style=\"margin-bottom: 1em;\"\u003e\n    \u003ca href=\"https://github.com/srishti-git1110/torch-switch-transformers\"\u003eSwitch Transformers\u003c/a\u003e.\u003cbr\u003e\n    \u003cdiv style=\"line-height: 1.4; font-size: smaller; color: var(--secondary);\"\u003e\n    \n    \u003cspan\u003eEfficient PyTorch implementation of the Switch Transformer with (optional) aux loss for each layer and configurable number of experts and expert capacity.\u003c/span\u003e.\u003cbr\u003e\n    \n    \u003cspan\u003e\u003cem\u003e\u003c/em\u003e.\u003c/span\u003e\n    \n    \u003cspan class=\"resource pub-icon\"\u003e\u003ca href=\"https://github.com/srishti-git1110/torch-switch-transformers\"\u003e\u003csvg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\"\n    stroke-linecap=\"round\" stroke-linejoin=\"round\"\u003e\n    \u003cpath\n        d=\"M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22\"\u003e\n    \u003c/path\u003e\n\u003c/svg\u003e\u003c/a\u003e\u003c/span\u003e\n    \n    \n    \n    \n    \n    \u003c/div\u003e\n\u003c/div\u003e","title":"Papers"},{"content":"Below are some of my talks that I\u0026rsquo;ve delivered on Cohere\u0026rsquo;s Discord.\nML Efficiency GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection.\n. Talk Slides SpQR: A Sparse-Quantized Representation for Near-lossless LLM Weight Compression.\n. Talk Slides LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale.\n. Talk Slides NLP GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints.\n. Talk Slides Universal and Transferable Adversarial Attacks on Aligned Language Models.\n. Talk Slides Extending Context Window of Large Language Models via Positional Interpolation.\n. Talk Slides ","permalink":"http://localhost:1313/talks/","summary":"\u003cp\u003eBelow are some of my talks that I\u0026rsquo;ve delivered on Cohere\u0026rsquo;s Discord.\u003c/p\u003e\n\u003ch3 id=\"ml-efficiency\"\u003eML Efficiency\u003c/h3\u003e\n\u003cdiv style=\"margin-bottom: 1em;\"\u003e\n    \u003ca href=\"\"\u003e\u003ca href=\"https://arxiv.org/abs/2403.03507\"\u003eGaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection\u003c/a\u003e\u003c/a\u003e.\u003cbr\u003e\n    \u003cdiv style=\"line-height: 1.4; font-size: smaller; color: var(--secondary);\"\u003e\n    \n    \u003cspan\u003e\u003cem\u003e\u003c/em\u003e.\u003c/span\u003e\n    \n    \n    \n    \n    \u003cspan class=\"resource pub-icon\"\u003e\u003ca href=\"https://drive.google.com/file/d/1I_4SW4ArncOgrGB5gVRfVkSo_3osZ9cC/view\"\u003eTalk\u003c/a\u003e\u003c/span\u003e\n    \n    \n    \u003cspan class=\"resource pub-icon\"\u003e\u003ca href=\"https://docs.google.com/presentation/d/1cLEVYjd7W3hc9Pz9vNth6L5Rw-Z4nzN5PXyNGN8ky7s/edit?usp=sharing\"\u003eSlides\u003c/a\u003e\u003c/span\u003e\n    \n    \u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv style=\"margin-bottom: 1em;\"\u003e\n    \u003ca href=\"\"\u003e\u003ca href=\"https://arxiv.org/abs/2306.03078\"\u003eSpQR: A Sparse-Quantized Representation for Near-lossless LLM Weight Compression\u003c/a\u003e\u003c/a\u003e.\u003cbr\u003e\n    \u003cdiv style=\"line-height: 1.4; font-size: smaller; color: var(--secondary);\"\u003e\n    \n    \u003cspan\u003e\u003cem\u003e\u003c/em\u003e.\u003c/span\u003e\n    \n    \n    \n    \n    \u003cspan class=\"resource pub-icon\"\u003e\u003ca href=\"https://drive.google.com/file/d/1Bny_KBOnuDDmK1pdt_1QpBEYU8aEkXuK/view\"\u003eTalk\u003c/a\u003e\u003c/span\u003e\n    \n    \n    \u003cspan class=\"resource pub-icon\"\u003e\u003ca href=\"https://docs.google.com/presentation/d/1yj-_pnTZEBXhHlBczFA9q0EqE8zjAaNyI2XG8zpef8M/edit?usp=sharing\"\u003eSlides\u003c/a\u003e\u003c/span\u003e\n    \n    \u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv style=\"margin-bottom: 1em;\"\u003e\n    \u003ca href=\"\"\u003e\u003ca href=\"https://arxiv.org/abs/2208.07339\"\u003eLLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale\u003c/a\u003e\u003c/a\u003e.\u003cbr\u003e\n    \u003cdiv style=\"line-height: 1.4; font-size: smaller; color: var(--secondary);\"\u003e\n    \n    \u003cspan\u003e\u003cem\u003e\u003c/em\u003e.\u003c/span\u003e\n    \n    \n    \n    \n    \u003cspan class=\"resource pub-icon\"\u003e\u003ca href=\"https://drive.google.com/file/d/12BaLLOeYU0tVH25PKfBsebF972xcir2t/view\"\u003eTalk\u003c/a\u003e\u003c/span\u003e\n    \n    \n    \u003cspan class=\"resource pub-icon\"\u003e\u003ca href=\"https://docs.google.com/presentation/d/1hAgm3VqR4e6dromxyurGAFThWb4uHAw-JE-i1Ag-55k/edit?usp=sharing\"\u003eSlides\u003c/a\u003e\u003c/span\u003e\n    \n    \u003c/div\u003e\n\u003c/div\u003e\n\u003ch3 id=\"nlp\"\u003eNLP\u003c/h3\u003e\n\u003cdiv style=\"margin-bottom: 1em;\"\u003e\n    \u003ca href=\"\"\u003e\u003ca href=\"https://arxiv.org/abs/2305.13245\"\u003eGQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints\u003c/a\u003e\u003c/a\u003e.\u003cbr\u003e\n    \u003cdiv style=\"line-height: 1.4; font-size: smaller; color: var(--secondary);\"\u003e\n    \n    \u003cspan\u003e\u003cem\u003e\u003c/em\u003e.\u003c/span\u003e\n    \n    \n    \n    \n    \u003cspan class=\"resource pub-icon\"\u003e\u003ca href=\"https://drive.google.com/file/d/1YlEXIzbwPifWy5Byib4on5zHkJoQeQfk/view\"\u003eTalk\u003c/a\u003e\u003c/span\u003e\n    \n    \n    \u003cspan class=\"resource pub-icon\"\u003e\u003ca href=\"https://docs.google.com/presentation/d/10VXjiUtca3gcgR6PiEQHFhh7J1cvtOGZQZRX9eJPlTk/edit?usp=sharing\"\u003eSlides\u003c/a\u003e\u003c/span\u003e\n    \n    \u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv style=\"margin-bottom: 1em;\"\u003e\n    \u003ca href=\"\"\u003e\u003ca href=\"https://arxiv.org/abs/2307.15043\"\u003eUniversal and Transferable Adversarial Attacks on Aligned Language Models\u003c/a\u003e\u003c/a\u003e.\u003cbr\u003e\n    \u003cdiv style=\"line-height: 1.4; font-size: smaller; color: var(--secondary);\"\u003e\n    \n    \u003cspan\u003e\u003cem\u003e\u003c/em\u003e.\u003c/span\u003e\n    \n    \n    \n    \n    \u003cspan class=\"resource pub-icon\"\u003e\u003ca href=\"https://drive.google.com/file/d/1mKhfhOwHuQH96CNEI5ZP3RjNCGZZ62pu/view\"\u003eTalk\u003c/a\u003e\u003c/span\u003e\n    \n    \n    \u003cspan class=\"resource pub-icon\"\u003e\u003ca href=\"https://docs.google.com/presentation/d/1YK1-wyiVPaPe6Rk7tFWABwKhLfIK399MT2MMIiGw4uY/edit?usp=sharing\"\u003eSlides\u003c/a\u003e\u003c/span\u003e\n    \n    \u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv style=\"margin-bottom: 1em;\"\u003e\n    \u003ca href=\"\"\u003e\u003ca href=\"https://arxiv.org/abs/2306.15595\"\u003eExtending Context Window of Large Language Models via Positional Interpolation\u003c/a\u003e\u003c/a\u003e.\u003cbr\u003e\n    \u003cdiv style=\"line-height: 1.4; font-size: smaller; color: var(--secondary);\"\u003e\n    \n    \u003cspan\u003e\u003cem\u003e\u003c/em\u003e.\u003c/span\u003e\n    \n    \n    \n    \n    \u003cspan class=\"resource pub-icon\"\u003e\u003ca href=\"https://drive.google.com/file/d/13x1SgBadD2zHLn_YtjHm3syVjgMXtN1M/view?t=11\"\u003eTalk\u003c/a\u003e\u003c/span\u003e\n    \n    \n    \u003cspan class=\"resource pub-icon\"\u003e\u003ca href=\"https://docs.google.com/presentation/d/1D7jeVFS1v6syjRzVF6WgZrn6bBCaDnGPrWMQfzmQfiY/edit?usp=sharing\"\u003eSlides\u003c/a\u003e\u003c/span\u003e\n    \n    \u003c/div\u003e\n\u003c/div\u003e","title":"Talks"}]