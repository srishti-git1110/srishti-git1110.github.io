<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Home on Srishti Gureja</title>
    <link>http://localhost:1313/</link>
    <description>Recent content in Home on Srishti Gureja</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 20 Dec 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>[WIP] Optimizing matmul on CPU</title>
      <link>http://localhost:1313/blog/matmul-cpu/</link>
      <pubDate>Sat, 20 Dec 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/blog/matmul-cpu/</guid>
      <description>&lt;h2 id=&#34;introduction-and-setup&#34;&gt;Introduction and Setup&lt;/h2&gt;
&lt;p&gt;This blog starts with a naive implementation of matmul in C and optimizes it one step at a time.&lt;/p&gt;
&lt;p&gt;I am using &lt;a href=&#34;https://www.apple.com/in/shop/buy-mac/macbook-pro/14-inch-space-black-standard-display-apple-m5-chip-with-10-core-cpu-and-10-core-gpu-16gb-memory-512gb&#34;&gt;the following machine&lt;/a&gt; with a 10 core CPU (4 Performance cores+6 Efficiency cores).&lt;/p&gt;
&lt;p&gt;Food for thought: Is matrix multiplication on CPU compute bound or memory bound? Think about it&amp;hellip;&lt;/p&gt;
&lt;h2 id=&#34;algorithmic-complexity-of-matrix-multiplication-calculating-the-flops-required&#34;&gt;Algorithmic complexity of Matrix Multiplication: Calculating the FLOPs required&lt;/h2&gt;
&lt;p&gt;Matrix multiplication is ubiquitous in many areas of computer sciene. As the matrices grow in sizes, the amount of FLOPs required to calculate the matmul grow cubically&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. Let&amp;rsquo;s see how.&lt;/p&gt;</description>
    </item>
    <item>
      <title>[WIP] CPU, GPU and stuff!</title>
      <link>http://localhost:1313/blog/hardware-primer/</link>
      <pubDate>Sun, 30 Nov 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/blog/hardware-primer/</guid>
      <description>&lt;p&gt;Some beginner stuff!&lt;/p&gt;
&lt;h2 id=&#34;core-microprocessorprocessor-chip&#34;&gt;Core, Microprocessor/Processor, Chip&lt;/h2&gt;
&lt;p&gt;&amp;ldquo;A chip&amp;rdquo; is the physical semiconductor chip; it&amp;rsquo;s &amp;ldquo;a physical integrated circuit&amp;rdquo; comprised of transistors, resistors, and capacitors.&lt;/p&gt;
&lt;p&gt;A processor (here, think of CPU, the central &amp;ldquo;processing&amp;rdquo; unit) is a digital circuit that&amp;rsquo;s implemented on a single or a few chips. Now, the term micro is appended to the beginning of processors to refer to the fact that it takes a single or a very few chips to implement a microprocessor. But this is more of a definition. In the context/scope of this post, consider 1 microprocessor = 1 chip.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Switch Transformer - Sparse Routed Networks/MoEs</title>
      <link>http://localhost:1313/blog/moes/</link>
      <pubDate>Mon, 17 Jun 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/blog/moes/</guid>
      <description>&lt;p&gt;In this short post, I am going to talk about the Switch Transformer paper.&lt;/p&gt;
&lt;h2 id=&#34;background-and-architecture&#34;&gt;Background and Architecture&lt;/h2&gt;
&lt;p&gt;To begin: In Dense Models, each parameter/neuron of the network gets activated during the forward pass.&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;ffnn.png#center&#34; alt=&#34;fully activated dense model&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;Hence, to be able to derive more performance by means of increasing the model size, more computation is required. Implicitly, more model parameters =&amp;gt; more computation performed by a single token.
And tangential to this, work by &lt;a href=&#34;https://arxiv.org/abs/2001.08361&#34;&gt;Kaplan et al. (2020)&lt;/a&gt; becomes super relevant as they discuss training larger models on comparatively smaller amounts of data as the computationally optimal approach.&lt;/p&gt;</description>
    </item>
    <item>
      <title>An Introduction to Differential Privacy</title>
      <link>http://localhost:1313/blog/dp/</link>
      <pubDate>Mon, 25 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/blog/dp/</guid>
      <description>A detailed mathematical and intuitive introduction to differential privacy.</description>
    </item>
    <item>
      <title>PyTorch Datasets and Dataloaders</title>
      <link>http://localhost:1313/blog/2023-01-05-pytorch-ds-dl/</link>
      <pubDate>Fri, 05 Aug 2022 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/blog/2023-01-05-pytorch-ds-dl/</guid>
      <description>A beginner&amp;#39;s guide to understanding two important PyTorch classes - Dataset and Dataloader. The new DataPipe functionality is also explored.</description>
    </item>
    <item>
      <title>Papers</title>
      <link>http://localhost:1313/code/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/code/</guid>
      <description>&lt;p&gt;Visit my &lt;a href=&#34;https://github.com/srishti-git1110&#34;&gt;github&lt;/a&gt; for more. Following are some selected samples.&lt;/p&gt;
&lt;h3 id=&#34;paper-implementations&#34;&gt;Paper Implementations&lt;/h3&gt;
&lt;div style=&#34;margin-bottom: 1em;&#34;&gt;
    &lt;a href=&#34;https://github.com/srishti-git1110/torch-switch-transformers&#34;&gt;Switch Transformers&lt;/a&gt;.&lt;br&gt;
    &lt;div style=&#34;line-height: 1.4; font-size: smaller; color: var(--secondary);&#34;&gt;
    
    &lt;span&gt;Efficient PyTorch implementation of the Switch Transformer with (optional) aux loss for each layer and configurable number of experts and expert capacity.&lt;/span&gt;.&lt;br&gt;
    
    &lt;span&gt;&lt;em&gt;&lt;/em&gt;.&lt;/span&gt;
    
    &lt;span class=&#34;resource pub-icon&#34;&gt;&lt;a href=&#34;https://github.com/srishti-git1110/torch-switch-transformers&#34;&gt;&lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34; fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-width=&#34;2&#34;
    stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34;&gt;
    &lt;path
        d=&#34;M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22&#34;&gt;
    &lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;&lt;/span&gt;
    
    
    
    
    
    &lt;/div&gt;
&lt;/div&gt;</description>
    </item>
    <item>
      <title>Talks</title>
      <link>http://localhost:1313/talks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/talks/</guid>
      <description>&lt;p&gt;Below are some of my talks that I&amp;rsquo;ve delivered on Cohere&amp;rsquo;s Discord.&lt;/p&gt;
&lt;h3 id=&#34;ml-efficiency&#34;&gt;ML Efficiency&lt;/h3&gt;
&lt;div style=&#34;margin-bottom: 1em;&#34;&gt;
    &lt;a href=&#34;&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.03507&#34;&gt;GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection&lt;/a&gt;&lt;/a&gt;.&lt;br&gt;
    &lt;div style=&#34;line-height: 1.4; font-size: smaller; color: var(--secondary);&#34;&gt;
    
    &lt;span&gt;&lt;em&gt;&lt;/em&gt;.&lt;/span&gt;
    
    
    
    
    &lt;span class=&#34;resource pub-icon&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1I_4SW4ArncOgrGB5gVRfVkSo_3osZ9cC/view&#34;&gt;Talk&lt;/a&gt;&lt;/span&gt;
    
    
    &lt;span class=&#34;resource pub-icon&#34;&gt;&lt;a href=&#34;https://docs.google.com/presentation/d/1cLEVYjd7W3hc9Pz9vNth6L5Rw-Z4nzN5PXyNGN8ky7s/edit?usp=sharing&#34;&gt;Slides&lt;/a&gt;&lt;/span&gt;
    
    &lt;/div&gt;
&lt;/div&gt;
&lt;div style=&#34;margin-bottom: 1em;&#34;&gt;
    &lt;a href=&#34;&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2306.03078&#34;&gt;SpQR: A Sparse-Quantized Representation for Near-lossless LLM Weight Compression&lt;/a&gt;&lt;/a&gt;.&lt;br&gt;
    &lt;div style=&#34;line-height: 1.4; font-size: smaller; color: var(--secondary);&#34;&gt;
    
    &lt;span&gt;&lt;em&gt;&lt;/em&gt;.&lt;/span&gt;
    
    
    
    
    &lt;span class=&#34;resource pub-icon&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1Bny_KBOnuDDmK1pdt_1QpBEYU8aEkXuK/view&#34;&gt;Talk&lt;/a&gt;&lt;/span&gt;
    
    
    &lt;span class=&#34;resource pub-icon&#34;&gt;&lt;a href=&#34;https://docs.google.com/presentation/d/1yj-_pnTZEBXhHlBczFA9q0EqE8zjAaNyI2XG8zpef8M/edit?usp=sharing&#34;&gt;Slides&lt;/a&gt;&lt;/span&gt;
    
    &lt;/div&gt;
&lt;/div&gt;
&lt;div style=&#34;margin-bottom: 1em;&#34;&gt;
    &lt;a href=&#34;&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2208.07339&#34;&gt;LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale&lt;/a&gt;&lt;/a&gt;.&lt;br&gt;
    &lt;div style=&#34;line-height: 1.4; font-size: smaller; color: var(--secondary);&#34;&gt;
    
    &lt;span&gt;&lt;em&gt;&lt;/em&gt;.&lt;/span&gt;
    
    
    
    
    &lt;span class=&#34;resource pub-icon&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/12BaLLOeYU0tVH25PKfBsebF972xcir2t/view&#34;&gt;Talk&lt;/a&gt;&lt;/span&gt;
    
    
    &lt;span class=&#34;resource pub-icon&#34;&gt;&lt;a href=&#34;https://docs.google.com/presentation/d/1hAgm3VqR4e6dromxyurGAFThWb4uHAw-JE-i1Ag-55k/edit?usp=sharing&#34;&gt;Slides&lt;/a&gt;&lt;/span&gt;
    
    &lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&#34;nlp&#34;&gt;NLP&lt;/h3&gt;
&lt;div style=&#34;margin-bottom: 1em;&#34;&gt;
    &lt;a href=&#34;&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2305.13245&#34;&gt;GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints&lt;/a&gt;&lt;/a&gt;.&lt;br&gt;
    &lt;div style=&#34;line-height: 1.4; font-size: smaller; color: var(--secondary);&#34;&gt;
    
    &lt;span&gt;&lt;em&gt;&lt;/em&gt;.&lt;/span&gt;
    
    
    
    
    &lt;span class=&#34;resource pub-icon&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1YlEXIzbwPifWy5Byib4on5zHkJoQeQfk/view&#34;&gt;Talk&lt;/a&gt;&lt;/span&gt;
    
    
    &lt;span class=&#34;resource pub-icon&#34;&gt;&lt;a href=&#34;https://docs.google.com/presentation/d/10VXjiUtca3gcgR6PiEQHFhh7J1cvtOGZQZRX9eJPlTk/edit?usp=sharing&#34;&gt;Slides&lt;/a&gt;&lt;/span&gt;
    
    &lt;/div&gt;
&lt;/div&gt;
&lt;div style=&#34;margin-bottom: 1em;&#34;&gt;
    &lt;a href=&#34;&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2307.15043&#34;&gt;Universal and Transferable Adversarial Attacks on Aligned Language Models&lt;/a&gt;&lt;/a&gt;.&lt;br&gt;
    &lt;div style=&#34;line-height: 1.4; font-size: smaller; color: var(--secondary);&#34;&gt;
    
    &lt;span&gt;&lt;em&gt;&lt;/em&gt;.&lt;/span&gt;
    
    
    
    
    &lt;span class=&#34;resource pub-icon&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1mKhfhOwHuQH96CNEI5ZP3RjNCGZZ62pu/view&#34;&gt;Talk&lt;/a&gt;&lt;/span&gt;
    
    
    &lt;span class=&#34;resource pub-icon&#34;&gt;&lt;a href=&#34;https://docs.google.com/presentation/d/1YK1-wyiVPaPe6Rk7tFWABwKhLfIK399MT2MMIiGw4uY/edit?usp=sharing&#34;&gt;Slides&lt;/a&gt;&lt;/span&gt;
    
    &lt;/div&gt;
&lt;/div&gt;
&lt;div style=&#34;margin-bottom: 1em;&#34;&gt;
    &lt;a href=&#34;&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2306.15595&#34;&gt;Extending Context Window of Large Language Models via Positional Interpolation&lt;/a&gt;&lt;/a&gt;.&lt;br&gt;
    &lt;div style=&#34;line-height: 1.4; font-size: smaller; color: var(--secondary);&#34;&gt;
    
    &lt;span&gt;&lt;em&gt;&lt;/em&gt;.&lt;/span&gt;
    
    
    
    
    &lt;span class=&#34;resource pub-icon&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/13x1SgBadD2zHLn_YtjHm3syVjgMXtN1M/view?t=11&#34;&gt;Talk&lt;/a&gt;&lt;/span&gt;
    
    
    &lt;span class=&#34;resource pub-icon&#34;&gt;&lt;a href=&#34;https://docs.google.com/presentation/d/1D7jeVFS1v6syjRzVF6WgZrn6bBCaDnGPrWMQfzmQfiY/edit?usp=sharing&#34;&gt;Slides&lt;/a&gt;&lt;/span&gt;
    
    &lt;/div&gt;
&lt;/div&gt;</description>
    </item>
  </channel>
</rss>
