<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Srishti Gureja</title>
    <link>http://localhost:1313/</link>
    <description>Recent content on Srishti Gureja</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 31 Aug 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>[Coming Soon] Scaling Laws: What We Know So Far</title>
      <link>http://localhost:1313/blog/sclaing-laws/</link>
      <pubDate>Sat, 31 Aug 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/blog/sclaing-laws/</guid>
      <description>On what we know so far about Scaling Laws in LLMs.</description>
    </item>
    <item>
      <title>Switch Transformer - Sparse Routed Networks/MoEs</title>
      <link>http://localhost:1313/blog/moes/</link>
      <pubDate>Mon, 17 Jun 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/blog/moes/</guid>
      <description>In this short post, I am going to talk about the Switch Transformer paper.
Background and Architecture To begin: In Dense Models, each parameter/neuron of the network gets activated during the forward pass.
Hence, to be able to derive more performance by means of increasing the model size, more computation is required. Implicitly, more model parameters =&amp;gt; more computation performed by a single token. And tangential to this, work by Kaplan et al.</description>
    </item>
    <item>
      <title>[WIP] Learning CUDA Programming: A Primer</title>
      <link>http://localhost:1313/blog/cuda-primer/</link>
      <pubDate>Sun, 31 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/blog/cuda-primer/</guid>
      <description>A friendly primer to learning CUDA programming</description>
    </item>
    <item>
      <title>An Introduction to Differential Privacy</title>
      <link>http://localhost:1313/blog/dp/</link>
      <pubDate>Mon, 25 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/blog/dp/</guid>
      <description>A detailed mathematical and intuitive introduction to differential privacy.</description>
    </item>
    <item>
      <title>PyTorch Datasets and Dataloaders</title>
      <link>http://localhost:1313/blog/2023-01-05-pytorch-ds-dl/</link>
      <pubDate>Fri, 05 Aug 2022 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/blog/2023-01-05-pytorch-ds-dl/</guid>
      <description>A beginner&amp;#39;s guide to understanding two important PyTorch classes - Dataset and Dataloader. The new DataPipe functionality is also explored.</description>
    </item>
    <item>
      <title>About</title>
      <link>http://localhost:1313/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/about/</guid>
      <description>Hi there! My name is Srishti. I am a Machine Learning Engineer and Researcher, and I care the most about the following areas of study:
Alignment (For example: While, in theory, SFT teaches the models to lie and RLHF helps get around this by virtue of Reward Models, how well do RMs work specifically for this purpose?) Techniques for efficient training of and inference from Language Models (Here, I am particularly interested in writing software that makes the most efficient use of the accelerators, and techniques that can help reduce the compute and memory requirements without drastically affecting the quality.</description>
    </item>
    <item>
      <title>Papers</title>
      <link>http://localhost:1313/code/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/code/</guid>
      <description>Visit my github for more. Following are some selected samples.
Paper Implementations Switch Transformers.
Efficient PyTorch implementation of the Switch Transformer with (optional) aux loss for each layer and configurable number of experts and expert capacity..
. </description>
    </item>
    <item>
      <title>Talks</title>
      <link>http://localhost:1313/talks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/talks/</guid>
      <description>Below are some of the talks that I&amp;rsquo;ve delivered on Cohere&amp;rsquo;s Discord.
ML Efficiency GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection.
. Talk Slides SpQR: A Sparse-Quantized Representation for Near-lossless LLM Weight Compression.
. Talk Slides LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale.
. Talk Slides NLP GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints.
. Talk Slides Universal and Transferable Adversarial Attacks on Aligned Language Models.</description>
    </item>
  </channel>
</rss>
