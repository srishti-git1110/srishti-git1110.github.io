<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>NLP, PyTorch on Srishti Gureja</title>
    <link>https://srishti-git1110.github.io/tags/nlp-pytorch/</link>
    <description>Recent content in NLP, PyTorch on Srishti Gureja</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 17 Jun 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://srishti-git1110.github.io/tags/nlp-pytorch/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Switch Transformer - Sparse Routed Networks/MoEs</title>
      <link>https://srishti-git1110.github.io/blog/moes/</link>
      <pubDate>Mon, 17 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://srishti-git1110.github.io/blog/moes/</guid>
      <description>In this short post, I am going to talk about the Switch Transformer paper.
Background and Architecture To begin: In Dense Models, each parameter/neuron of the network gets activated during the forward pass.
Hence, to be able to derive more performance by means of increasing the model size, more computation is required. Implicitly, more model parameters =&amp;gt; more computation performed by a single token. And tangential to this, work by Kaplan et al.</description>
    </item>
    <item>
      <title>PyTorch Datasets and Dataloaders</title>
      <link>https://srishti-git1110.github.io/blog/2023-01-05-pytorch-ds-dl/</link>
      <pubDate>Fri, 05 Aug 2022 00:00:00 +0000</pubDate>
      <guid>https://srishti-git1110.github.io/blog/2023-01-05-pytorch-ds-dl/</guid>
      <description>A beginner&amp;#39;s guide to understanding two important PyTorch classes - Dataset and Dataloader. The new DataPipe functionality is also explored.</description>
    </item>
  </channel>
</rss>
