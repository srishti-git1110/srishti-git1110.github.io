<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Reinforcement Learning | Srishti Gureja</title><meta name=keywords content="Reinforcement Learning,Decision Making"><meta name=description content="Prep: Formalizing the RL problem
Reinforcement learning is the science of decision making. What does it mean to make decisions? Making decisions requires one to take into account the current circumstances and then take an action(s) with a goal to optimize the long term effects of the decision. RL is no different.
I think it&rsquo;s already nice to note that we&rsquo;re talking about sequential decision making where the decisions taken today are relevant to the long term outcomes."><meta name=author content><link rel=canonical href=http://localhost:1313/blog/rl/><link crossorigin=anonymous href=/assets/css/stylesheet.b26ba54a60d00ea06fda6b711f3da6382e5fe3a6fae7b4cc58bdc38f1f26bddc.css integrity="sha256-smulSmDQDqBv2mtxHz2mOC5f46b657TMWL3Djx8mvdw=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/blog/rl/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Reinforcement Learning"><meta property="og:description" content="Prep: Formalizing the RL problem
Reinforcement learning is the science of decision making. What does it mean to make decisions? Making decisions requires one to take into account the current circumstances and then take an action(s) with a goal to optimize the long term effects of the decision. RL is no different.
I think it&rsquo;s already nice to note that we&rsquo;re talking about sequential decision making where the decisions taken today are relevant to the long term outcomes."><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/blog/rl/"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-06-10T00:00:00+00:00"><meta property="article:modified_time" content="2025-06-10T00:00:00+00:00"><meta property="og:site_name" content="Srishti Gureja"><meta name=twitter:card content="summary"><meta name=twitter:title content="Reinforcement Learning"><meta name=twitter:description content="Prep: Formalizing the RL problem
Reinforcement learning is the science of decision making. What does it mean to make decisions? Making decisions requires one to take into account the current circumstances and then take an action(s) with a goal to optimize the long term effects of the decision. RL is no different.
I think it&rsquo;s already nice to note that we&rsquo;re talking about sequential decision making where the decisions taken today are relevant to the long term outcomes."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blogs","item":"http://localhost:1313/blog/"},{"@type":"ListItem","position":2,"name":"Reinforcement Learning","item":"http://localhost:1313/blog/rl/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Reinforcement Learning","name":"Reinforcement Learning","description":"Prep: Formalizing the RL problem Reinforcement learning is the science of decision making. What does it mean to make decisions? Making decisions requires one to take into account the current circumstances and then take an action(s) with a goal to optimize the long term effects of the decision. RL is no different.\nI think it\u0026rsquo;s already nice to note that we\u0026rsquo;re talking about sequential decision making where the decisions taken today are relevant to the long term outcomes.\n","keywords":["Reinforcement Learning, Decision Making"],"articleBody":"Prep: Formalizing the RL problem Reinforcement learning is the science of decision making. What does it mean to make decisions? Making decisions requires one to take into account the current circumstances and then take an action(s) with a goal to optimize the long term effects of the decision. RL is no different.\nI think it‚Äôs already nice to note that we‚Äôre talking about sequential decision making where the decisions taken today are relevant to the long term outcomes.\nThe way humans ‚Äúlearn‚Äù to make decisions using their life experiences is analogous to the ‚Äúlearning‚Äù part in Reinforcement Learning.\nBear with me for this analogy before we jump right into the foundational jargon needed to formally define the RL setup.\nAnalogy As humans, we live in an environment/world and we ought to make decisions at every point in our lives. If I state the obvious more explicity, I‚Äôd go: As humans, we need to make decisions all the time keeping in mind a few factors‚Äîthe past until the present point in time, the present state of our lives, the immediate state our decision will land us in, both the short term gain received in response to our decision and also our long term goals, and so on. All of this is difficult because of one single fact‚ÄîUncertainty. Of what? Of the future. The future is uncertain and hence it‚Äôs hard to know in advance the actual outcome or the response from the world received by our actions.\nThis is really important. If this uncertainty wasn‚Äôt there, the science of decision making would‚Äôve been much more simpler. At this point, it can also benefit to concretely define the two sources of uncertainty‚Äîthe exact immediate/next state our current action/decision would land us in, and the gain that our current decision would yield immediately.\nFor example, consider an individual who currently needs to decide between accepting an admit to a graduate program and accepting a high paying job at a newly found startup. Let‚Äôs suppose their long term goal is to also build a startup of their own. Given that, their current decision needs to be the one that helps fulfill that goal in the most optimized manner. Hence, the two reasons why it‚Äôs hard to make this decision are:\nThe individual is uncertain of the benefit yielded by both decisions. Maybe the MS program yields more benefit via providing a strong knowledge base that could be leveraged to build a startup of maybe the job yields better benefit by providing strong hands-on exp!\nThe individual is also uncertain of the state both decisions land them in. Maybe the job environment as a fast paced startup quickly puts them in more senior or managerial positions that can be better for their own startup or maybe the MS programme would allow them to meet like-minded peers that could be their co-founders.\nAnd of course, this was my way of characterizing the problem. One could obviously define the benefit received and the next state differently.\nThe Jargon As noted, RL is the science of sequential decision making under uncertainty with a goal to optimize (of course, optimize here = maximize) the long term rewards. Mathematically, the RL problem can be defined using the following terms:\nAn agent. The state of the agent at any time step t st ùúñ S where S is the state space defining all possible states the agent can be in. The action(s) taken by the agent at any time step t at ùúñ A where A is the action space defining all possible actions that can be taken by the agent. The Policy followed by the agent according to which it acts. The environment the agent is interacting with. The setting goes: An agent is interacting with the environment. At any point in time t, the agent is in a state St and takes an action At. As a result of this, it receives some reward Rt from the environment and transitions to a new state St+1 both of which are defined by the environment.\nPolicy. Now, how does the agent know which action to take in which state? The Policy defines that. Mathematically, the policy is just the conditional probability distribution of s ùúñ S given a ùúñ A.\nThe policy is what‚Äôs learned by the agent.\n‚ñ∂ The policy can either be stochastic or deterministic.\nStochastic Policy. Given a state s, a probability distribution can be defined over the action space A. enter expression here. i.e. In any state, the agent can take one or more actions depending on the probability of taking each as determined by the policy. Deterministic Policy. Given a state s, the agent will take only one action defined by the policy. enter exp Environment.\nReturn.\nValue Function.\nQ Function.\nGoal of the problem.\nHorizon.\nMarkov Reward Process (MRP) Markov Decision Process (MDP) Policy Evaluation and Policy Search (Iteration) Value Iteration ","wordCount":"816","inLanguage":"en","datePublished":"2025-06-10T00:00:00Z","dateModified":"2025-06-10T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/blog/rl/"},"publisher":{"@type":"Organization","name":"Srishti Gureja","logo":{"@type":"ImageObject","url":"http://localhost:1313/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Srishti Gureja (Alt + H)">Srishti Gureja</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/code/ title=code><span>code</span></a></li><li><a href=http://localhost:1313/blog/ title=blog><span>blog</span></a></li><li><a href=http://localhost:1313/talks/ title=talks><span>talks</span></a></li><li><a href=http://localhost:1313/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;¬ª&nbsp;<a href=http://localhost:1313/blog/>Blogs</a></div><h1 class="post-title entry-hint-parent">Reinforcement Learning</h1><div class=post-meta><span title='2025-06-10 00:00:00 +0000 UTC'>June 10, 2025</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><ul><li><a href=#analogy>Analogy</a></li><li><a href=#the-jargon>The Jargon</a></li></ul></li></ul></nav></div></details></div><div class=post-content><h1 id=prep-formalizing-the-rl-problem>Prep: Formalizing the RL problem<a hidden class=anchor aria-hidden=true href=#prep-formalizing-the-rl-problem>#</a></h1><p>Reinforcement learning is the science of decision making. What does it mean to make decisions? Making decisions requires one to take into account the current circumstances and then take an action(s) with a goal to optimize the long term effects of the decision. RL is no different.</p><p>I think it&rsquo;s already nice to note that we&rsquo;re talking about sequential decision making where the decisions taken today are relevant to the long term outcomes.</p><p>The way humans &ldquo;learn&rdquo; to make decisions using their life experiences is analogous to the &ldquo;learning&rdquo; part in Reinforcement Learning.</p><p>Bear with me for this analogy before we jump right into the foundational jargon needed to formally define the RL setup.</p><h3 id=analogy>Analogy<a hidden class=anchor aria-hidden=true href=#analogy>#</a></h3><p>As humans, we live in an environment/world and we ought to make decisions at every point in our lives. If I state the obvious more explicity, I&rsquo;d go: As humans, we need to make decisions all the time keeping in mind a few factors‚Äîthe past until the present point in time, the present state of our lives, the immediate state our decision will land us in, both the short term gain received in response to our decision and also our long term goals, and so on. All of this is difficult because of one single fact‚ÄîUncertainty. Of what? Of the future. The future is uncertain and hence it&rsquo;s hard to know in advance the actual outcome or the response from the world received by our actions.</p><p>This is really important. If this uncertainty wasn&rsquo;t there, the science of decision making would&rsquo;ve been much more simpler. At this point, it can also benefit to concretely define the two sources of uncertainty‚Äîthe exact immediate/next state our current action/decision would land us in, and the gain that our current decision would yield immediately.</p><p>For example, consider an individual who currently needs to decide between accepting an admit to a graduate program and accepting a high paying job at a newly found startup. Let&rsquo;s suppose their long term goal is to also build a startup of their own. Given that, their current decision needs to be the one that helps fulfill that goal in the most optimized manner. Hence, the two reasons why it&rsquo;s hard to make this decision are:</p><ul><li><p>The individual is uncertain of the benefit yielded by both decisions. Maybe the MS program yields more benefit via providing a strong knowledge base that could be leveraged to build a startup of maybe the job yields better benefit by providing strong hands-on exp!</p></li><li><p>The individual is also uncertain of the state both decisions land them in. Maybe the job environment as a fast paced startup quickly puts them in more senior or managerial positions that can be better for their own startup or maybe the MS programme would allow them to meet like-minded peers that could be their co-founders.</p></li></ul><p><em>And of course, this was my way of characterizing the problem. One could obviously define the benefit received and the next state differently.</em></p><h3 id=the-jargon>The Jargon<a hidden class=anchor aria-hidden=true href=#the-jargon>#</a></h3><p>As noted, RL is the science of sequential decision making under uncertainty with a goal to optimize (of course, optimize here = maximize) the long term rewards. Mathematically, the RL problem can be defined using the following terms:</p><ul><li>An agent.</li><li>The state of the agent at any time step t st ùúñ S where S is the state space defining all possible states the agent can be in.</li><li>The action(s) taken by the agent at any time step t at ùúñ A where A is the action space defining all possible actions that can be taken by the agent.</li><li>The Policy followed by the agent according to which it acts.</li><li>The environment the agent is interacting with.</li></ul><p><img loading=lazy src=agent.png#center alt="the rl setting"></p><p>The setting goes: An agent is interacting with the environment. At any point in time t, the agent is in a state St and takes an action At. As a result of this, it receives some reward Rt from the environment and transitions to a new state St+1 both of which are defined by the environment.</p><p><strong>Policy.</strong> Now, how does the agent know which action to take in which state? The Policy defines that. Mathematically, the policy is just the conditional probability distribution of s ùúñ S given a ùúñ A.</p><exp><p>The policy is what&rsquo;s learned by the agent.</p><style>.collapsible{background-color:#090909;cursor:pointer;padding:10px;width:100%;border:none;text-align:left;outline:none;font-size:1em}.active,.collapsible:hover{background-color:#ddd}.content{display:none;padding:10px 0;border-top:1px solid #ccc;text-align:center}</style><p><button class=collapsible>‚ñ∂ The policy can either be stochastic or deterministic.</button></p><div class=content><strong>Stochastic Policy.</strong> Given a state s, a probability distribution can be defined over the action space A.<br>enter expression here.<br>i.e. In any state, the agent can take one or more actions depending on the probability of taking each as determined by the policy.<br><strong>Deterministic Policy.</strong> Given a state s, the agent will take only one action defined by the policy.<br>enter exp<br></div><script>document.querySelectorAll(".collapsible").forEach(function(e){e.addEventListener("click",function(){this.classList.toggle("active");var e=this.nextElementSibling;e.style.display==="block"?(e.style.display="none",this.textContent="‚ñ∂ Click to show explanation"):(e.style.display="block",this.textContent="‚ñº Hide explanation")})})</script><p><strong>Environment.</strong></p><p><strong>Return.</strong></p><p><strong>Value Function.</strong></p><p><strong>Q Function.</strong></p><p><strong>Goal of the problem.</strong></p><p><strong>Horizon.</strong></p><h1 id=markov-reward-process-mrp>Markov Reward Process (MRP)<a hidden class=anchor aria-hidden=true href=#markov-reward-process-mrp>#</a></h1><h1 id=markov-decision-process-mdp>Markov Decision Process (MDP)<a hidden class=anchor aria-hidden=true href=#markov-decision-process-mdp>#</a></h1><h1 id=policy-evaluation-and-policy-search-iteration>Policy Evaluation and Policy Search (Iteration)<a hidden class=anchor aria-hidden=true href=#policy-evaluation-and-policy-search-iteration>#</a></h1><h1 id=value-iteration>Value Iteration<a hidden class=anchor aria-hidden=true href=#value-iteration>#</a></h1></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/reinforcement-learning-decision-making/>Reinforcement Learning, Decision Making</a></li></ul><nav class=paginav><a class=next href=http://localhost:1313/blog/moes/><span class=title>Next ¬ª</span><br><span>Switch Transformer - Sparse Routed Networks/MoEs</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=http://localhost:1313/>Srishti Gureja</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}}</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js defer></script></body></html>