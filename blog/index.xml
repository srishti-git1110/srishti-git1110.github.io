<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Blogs on Srishti Gureja</title>
    <link>https://srishti-git1110.github.io/blog/</link>
    <description>Recent content in Blogs on Srishti Gureja</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 20 Dec 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://srishti-git1110.github.io/blog/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>[WIP] Optimizing matmul on CPU</title>
      <link>https://srishti-git1110.github.io/blog/matmul-cpu/</link>
      <pubDate>Sat, 20 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://srishti-git1110.github.io/blog/matmul-cpu/</guid>
      <description>&lt;h2 id=&#34;introduction-and-setup&#34;&gt;Introduction and Setup&lt;/h2&gt;
&lt;p&gt;This blog starts with a naive implementation of matmul in C and optimizes it one step at a time.&lt;/p&gt;
&lt;p&gt;I am using &lt;a href=&#34;https://www.apple.com/in/shop/buy-mac/macbook-pro/14-inch-space-black-standard-display-apple-m5-chip-with-10-core-cpu-and-10-core-gpu-16gb-memory-512gb&#34;&gt;the following machine&lt;/a&gt; with a 10 core CPU (4 Performance cores+6 Efficiency cores).&lt;/p&gt;
&lt;p&gt;Food for thought: Is matrix multiplication on CPU compute bound or memory bound? Think about it&amp;hellip;&lt;/p&gt;
&lt;h2 id=&#34;algorithmic-complexity-of-matrix-multiplication-calculating-the-flops-required&#34;&gt;Algorithmic complexity of Matrix Multiplication: Calculating the FLOPs required&lt;/h2&gt;
&lt;p&gt;Matrix multiplication is ubiquitous in many areas of computer sciene. As the matrices grow in sizes, the amount of FLOPs required to calculate the matmul grow cubically&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. Let&amp;rsquo;s see how.&lt;/p&gt;</description>
    </item>
    <item>
      <title>[WIP] CPU, GPU and stuff!</title>
      <link>https://srishti-git1110.github.io/blog/hardware-primer/</link>
      <pubDate>Sun, 30 Nov 2025 00:00:00 +0000</pubDate>
      <guid>https://srishti-git1110.github.io/blog/hardware-primer/</guid>
      <description>&lt;p&gt;Some beginner stuff!&lt;/p&gt;
&lt;h2 id=&#34;core-microprocessorprocessor-chip&#34;&gt;Core, Microprocessor/Processor, Chip&lt;/h2&gt;
&lt;p&gt;&amp;ldquo;A chip&amp;rdquo; is the physical semiconductor chip; it&amp;rsquo;s &amp;ldquo;a physical integrated circuit&amp;rdquo; comprised of transistors, resistors, and capacitors.&lt;/p&gt;
&lt;p&gt;A processor (here, think of CPU, the central &amp;ldquo;processing&amp;rdquo; unit) is a digital circuit that&amp;rsquo;s implemented on a single or a few chips. Now, the term micro is appended to the beginning of processors to refer to the fact that it takes a single or a very few chips to implement a microprocessor. But this is more of a definition. In the context/scope of this post, consider 1 microprocessor = 1 chip.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Switch Transformer - Sparse Routed Networks/MoEs</title>
      <link>https://srishti-git1110.github.io/blog/moes/</link>
      <pubDate>Mon, 17 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://srishti-git1110.github.io/blog/moes/</guid>
      <description>&lt;p&gt;In this short post, I am going to talk about the Switch Transformer paper.&lt;/p&gt;
&lt;h2 id=&#34;background-and-architecture&#34;&gt;Background and Architecture&lt;/h2&gt;
&lt;p&gt;To begin: In Dense Models, each parameter/neuron of the network gets activated during the forward pass.&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;ffnn.png#center&#34; alt=&#34;fully activated dense model&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;Hence, to be able to derive more performance by means of increasing the model size, more computation is required. Implicitly, more model parameters =&amp;gt; more computation performed by a single token.
And tangential to this, work by &lt;a href=&#34;https://arxiv.org/abs/2001.08361&#34;&gt;Kaplan et al. (2020)&lt;/a&gt; becomes super relevant as they discuss training larger models on comparatively smaller amounts of data as the computationally optimal approach.&lt;/p&gt;</description>
    </item>
    <item>
      <title>An Introduction to Differential Privacy</title>
      <link>https://srishti-git1110.github.io/blog/dp/</link>
      <pubDate>Mon, 25 Mar 2024 00:00:00 +0000</pubDate>
      <guid>https://srishti-git1110.github.io/blog/dp/</guid>
      <description>A detailed mathematical and intuitive introduction to differential privacy.</description>
    </item>
    <item>
      <title>PyTorch Datasets and Dataloaders</title>
      <link>https://srishti-git1110.github.io/blog/2023-01-05-pytorch-ds-dl/</link>
      <pubDate>Fri, 05 Aug 2022 00:00:00 +0000</pubDate>
      <guid>https://srishti-git1110.github.io/blog/2023-01-05-pytorch-ds-dl/</guid>
      <description>A beginner&amp;#39;s guide to understanding two important PyTorch classes - Dataset and Dataloader. The new DataPipe functionality is also explored.</description>
    </item>
  </channel>
</rss>
