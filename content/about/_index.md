---
title: About
draft: false
---
Hi there! My name is Srishti.
I am a Machine Learning Engineer and Researcher broadly interested in two research areas:
- Alignment of Language Models (For example: While, in theory, SFT teaches the models to lie and RLHF helps get around this by virtue of Reward Models, how well do RMs work for this purpose?)
- Techniques for efficient training of and inference from Language Models (Towards this, I am learning GPU Programming and design these days)

## Work
- Currently, I am working as an ML Engineer at an Indian startup where I work on many things ML + software engineering including fine-tuning language models for focussed use cases, building evaluation and training pipelines etc.

- I am also working with [Eleuther AI](https://www.eleuther.ai/) towards a project aimed at understanding Conditional Pre-training of LMs at scale as a way to achieve better alignment. The rationale here is that rather than first learning and then unlearning unwanted behaviour via techniques like RLHF, how much it benefits at scale to learn aligned behaviour right from the pre-training stage.

- In the past, I have worked at [Translated](https://translated.com/welcome) where I had a lot of fun working on the intersection of LLMs, DP, FL, PEFT for the [EU Data Tools for Heart Project](https://www.datatools4heart.eu/).

- I was also one of the 10 fellows that were selected to be a part of [Pi School](https://picampus-school.com/) where I worked on using many traditional and neural NLP techniques to automate an end to end document processing and analysis pipeline for a Berlin-based startup, Briink.

- I started teaching myself Machine learning in late 2021 using online courses. In the Jan of 2022, I worked on evaluating the robutness of BERT based models for the task of biomedical entity linking - we got this published as a workshop paper @ NeurIPS '22.

This is also the time when I got deeply interested in the internal workings of PyTorch which led me to studying and exploring more of the library an outcome of which is something I cherish - I was awarded by The Linux Foundation and The PyTorch Foundation with one of the 12 [PyTorch Contributor Awards 2023](https://pytorch.org/ecosystem/contributor-awards-2023).



## More
I am a math lover! From LA to calculus, I get immense joy in thinking about things in the world from both a mathematical and an intuitive perspective. My love for numbers let to me study Statistics during my undergrad years. 

Well, the way these programs are taught in India didn't turn out very well for me. Although I scored high throughout, most of my learning happened outside classes in a way that I liked - thinking deeply about things where one thing leads to another and not caring much about the curriculum.

This way of learning has benefitted me immensely - I failed to get into IITB for an MS programme as during prep, I started focussing too much on depth rather than picking up tricks to solve and pass the exam quickly. This was a blessing in disguise as only after this, I decided to follow a learning path that appeals to me and that somehow led me to learning ML.

These days I don't do just ML - I self-study Operating systems, GPU programming, compilers etc. and it is all so much fun!