<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Srishti Gureja</title>
    <link>https://srishti-git1110.github.io/</link>
    <description>Recent content on Srishti Gureja</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 31 Aug 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://srishti-git1110.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>[Coming Soon] Scaling Laws: What We Know So Far</title>
      <link>https://srishti-git1110.github.io/blog/sclaing-laws/</link>
      <pubDate>Sat, 31 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://srishti-git1110.github.io/blog/sclaing-laws/</guid>
      <description>On what we know so far about Scaling Laws in LLMs.</description>
    </item>
    <item>
      <title>Switch Transformer - Sparse Routed Networks/MoEs</title>
      <link>https://srishti-git1110.github.io/blog/moes/</link>
      <pubDate>Mon, 17 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://srishti-git1110.github.io/blog/moes/</guid>
      <description>In this short post, I am going to talk about the Switch Transformer paper.
Background and Architecture To begin: In Dense Models, each parameter/neuron of the network gets activated during the forward pass.
Hence, to be able to derive more performance by means of increasing the model size, more computation is required. Implicitly, more model parameters =&amp;gt; more computation performed by a single token. And tangential to this, work by Kaplan et al.</description>
    </item>
    <item>
      <title>[WIP] Learning CUDA Programming: A Primer</title>
      <link>https://srishti-git1110.github.io/blog/cuda-primer/</link>
      <pubDate>Sun, 31 Mar 2024 00:00:00 +0000</pubDate>
      <guid>https://srishti-git1110.github.io/blog/cuda-primer/</guid>
      <description>A friendly primer to learning CUDA programming</description>
    </item>
    <item>
      <title>An Introduction to Differential Privacy</title>
      <link>https://srishti-git1110.github.io/blog/dp/</link>
      <pubDate>Mon, 25 Mar 2024 00:00:00 +0000</pubDate>
      <guid>https://srishti-git1110.github.io/blog/dp/</guid>
      <description>A detailed mathematical and intuitive introduction to differential privacy.</description>
    </item>
    <item>
      <title>PyTorch Datasets and Dataloaders</title>
      <link>https://srishti-git1110.github.io/blog/2023-01-05-pytorch-ds-dl/</link>
      <pubDate>Fri, 05 Aug 2022 00:00:00 +0000</pubDate>
      <guid>https://srishti-git1110.github.io/blog/2023-01-05-pytorch-ds-dl/</guid>
      <description>A beginner&amp;#39;s guide to understanding two important PyTorch classes - Dataset and Dataloader. The new DataPipe functionality is also explored.</description>
    </item>
    <item>
      <title>Papers</title>
      <link>https://srishti-git1110.github.io/code/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://srishti-git1110.github.io/code/</guid>
      <description>Visit my github for more. Following are some selected samples.
Paper Implementations Switch Transformers.
Efficient PyTorch implementation of the Switch Transformer with (optional) aux loss for each layer and configurable number of experts and expert capacity..
. </description>
    </item>
  </channel>
</rss>
